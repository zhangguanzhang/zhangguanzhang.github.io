<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhangguanzhang</title>
  
  <subtitle>站在巨人的肩膀上</subtitle>
  <link href="http://zhangguanzhang.github.io/atom.xml" rel="self"/>
  
  <link href="http://zhangguanzhang.github.io/"/>
  <updated>2023-11-03T12:37:30.000Z</updated>
  <id>http://zhangguanzhang.github.io/</id>
  
  <author>
    <name>Zhangguanzhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[持续更新] - 容器非 root 启动改造的经验</title>
    <link href="http://zhangguanzhang.github.io/2023/11/03/non-root-containers/"/>
    <id>http://zhangguanzhang.github.io/2023/11/03/non-root-containers/</id>
    <published>2023-11-03T12:37:30.000Z</published>
    <updated>2023-11-03T12:37:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近业务容器的非 root 启动改造实战案例经验，后续有新的也更新进来</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>客户安全要求业务容器改为非 root 启动，很多容器需要操作 ipset iptables 之类的，并不是纯粹 rootless docker 就可以解决的。是尽可能的把（非 k8s 管理容器之类以外）业务容器改为非 root 启动（是容器内业务的所有进程）。</p><h2 id="改造"><a href="#改造" class="headerlink" title="改造"></a>改造</h2><h3 id="前提须知"><a href="#前提须知" class="headerlink" title="前提须知"></a>前提须知</h3><p>这里列举些基础知识</p><h4 id="使用-root-不安全的举例"><a href="#使用-root-不安全的举例" class="headerlink" title="使用 root 不安全的举例"></a>使用 root 不安全的举例</h4><p>虽然 linux 有 user namespace 隔离技术，但是 docker 不支持类似 podman 那样的给每个容器设置范围性的 uidmap 映射（当然 k8s 现在也不支持），并且容器默认配置下的权限虽然去掉了一些。但是容器内还是能对挂载进去的进行修改的，比如帖子 <a href="https://www.v2ex.com/t/976554">rm -rf * 前一定一定要看清当前目录</a> 老哥的操作：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -v /mnt/sda1:/mnt/sda1 -it alpine</span><br><span class="line">cp /mnt/sda1/somefile.tar.gz .</span><br><span class="line">tar xzvf somefile.tar.gz</span><br><span class="line">cd somefile-v1.0</span><br><span class="line">ls</span><br><span class="line"># 看了看内容觉得不是自己想要的，回上一级目录准备删掉：</span><br><span class="line">cd ..</span><br><span class="line">rm -rf *</span><br></pre></td></tr></table></figure><p>嗯，alpine 默认的 workdir 是 <code>/</code> ，所以删除 <code>rm -rf /*</code>。当然还有其他不安全的，所以在业务角度上，我们需要给容器内进程设置在非 root 下最小的运行权限。</p><h4 id="设置-USER-还是使用-docker-entrypoint-sh-入口"><a href="#设置-USER-还是使用-docker-entrypoint-sh-入口" class="headerlink" title="设置 USER 还是使用 docker-entrypoint.sh 入口"></a>设置 USER 还是使用 docker-entrypoint.sh 入口</h4><p>Dockerfile 里设置 <code>USER</code> 或者 run 的时候设置 <code>-u user:group</code> 只能针对于一些简单的进程，例如大部分 exporter 和一些只是用 http API 的进程，这几天我测试后也提交了一些 pr：</p><ul><li><a href="https://github.com/danielqsj/kafka_exporter/pull/410">danielqsj&#x2F;kafka_exporter</a></li><li><a href="https://github.com/ClickHouse/clickhouse_exporter/pull/83">ClickHouse&#x2F;clickhouse_exporter</a></li><li><a href="https://github.com/kubernetes/autoscaler/pull/6242/files">kubernetes addonresizer</a></li></ul><p>对于很多挂载目录持久化数据的，例如各种中间件，例如 mysql，redis ，单纯设置 USER 的话，需要在容器启动之前设置目录的权限。other 权限为 7 的话，很不安全，所以只能是 owner、group 权限，但是容器内的用户名和宿主机用户名是不一致的，只能设置 uid、gid。使用这些需要数据持久化的容器，会存在：</p><ul><li>直接 -v 和在 </li><li>k8s 上使用 hostPath 以及 </li><li>固定 pv </li><li>sc 下使用 pvc</li><li>别人的 k8s 集群或者实例上去部署</li></ul><p>如果你提前修改目录权限，上面最后俩场景根本无法自动化，而且说不定某天新版本的官方 Dockerfile 里换基础镜像切忘记添加用户时候设置 uid 和 gid 就变了，只能是加启动脚本里处理。</p><p>对此，<a href="https://github.com/docker-library/mysql/blob/master/5.7/docker-entrypoint.sh">mysql docker 镜像的官方启动脚本</a> 给了很好的参考，Dockerfile 制作镜像就创建了指定 uid、gid 的 mysql 用户，然后启动容器的时候都是 <code>ENTRYPOINT CMD</code> （k8s 里对应 command、args） 的形式启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-entrypoint.sh mysqld</span><br></pre></td></tr></table></figure><p>或者可以通过 cmdline 设置 mysql 启动端口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run xxx mysql:5.7 --port 4306</span><br></pre></td></tr></table></figure><p>mysql 脚本里包含对于权限以外的信息比较多，不方便举例，这里使用 redis 举例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"><span class="comment"># 脚本某行报错就退出</span></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"><span class="comment"># 脚本的第一个参数为 -开头的字符串，或者是 .conf 结尾的字符串</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$&#123;1#-&#125;</span>&quot;</span> != <span class="string">&quot;<span class="variable">$1</span>&quot;</span> ] || [ <span class="string">&quot;<span class="variable">$&#123;1%.conf&#125;</span>&quot;</span> != <span class="string">&quot;<span class="variable">$1</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="comment"># 重新设置 $@ 为 redis-server &quot;$@&quot;</span></span><br><span class="line"><span class="built_in">set</span> -- redis-server <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># allow the container to be started with `--user`</span></span><br><span class="line"><span class="comment"># 第一个参数为 redis-server 并且执行的用户为 root</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&#x27;redis-server&#x27;</span> -a <span class="string">&quot;<span class="subst">$(id -u)</span>&quot;</span> = <span class="string">&#x27;0&#x27;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="comment"># 更改当前目录下的 owner 为 redis</span></span><br><span class="line">find . \! -user redis -<span class="built_in">exec</span> <span class="built_in">chown</span> redis <span class="string">&#x27;&#123;&#125;&#x27;</span> +</span><br><span class="line">  <span class="comment"># 使用 gosu 切换到 redis 执行本脚本，并带上此刻的 $@参数</span></span><br><span class="line"><span class="built_in">exec</span> gosu redis <span class="string">&quot;<span class="variable">$0</span>&quot;</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set an appropriate umask (if one isn&#x27;t set already)</span></span><br><span class="line"><span class="comment"># - https://github.com/docker-library/redis/issues/305</span></span><br><span class="line"><span class="comment"># - https://github.com/redis/redis/blob/bb875603fb7ff3f9d19aad906bd45d7db98d9a39/utils/systemd-redis_server.service#L37</span></span><br><span class="line">um=<span class="string">&quot;<span class="subst">$(umask)</span>&quot;</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$um</span>&quot;</span> = <span class="string">&#x27;0022&#x27;</span> ]; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">umask</span> 0077</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br></pre></td></tr></table></figure><p>例如下面执行流程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d -name redis7 -v <span class="variable">$PWD</span>/redis-ctr-data:/data --net host redis:7 --port 7777</span><br><span class="line">$ docker top redis7</span><br><span class="line">UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD</span><br><span class="line">systemd+            1041135             1041116             1                   15:47               ?                   00:00:00            redis-server *:7777</span><br><span class="line">$ docker <span class="built_in">exec</span> redis7 <span class="built_in">id</span> redis</span><br><span class="line">uid=999(redis) gid=999(redis) <span class="built_in">groups</span>=999(redis)</span><br><span class="line">$ grep 999 /etc/passwd</span><br><span class="line">systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin</span><br></pre></td></tr></table></figure><p>docker top 显示的用户，是按照宿主机上 uid 显示的，<a href="https://github.com/tianon/gosu">gosu</a> 是 golang 实现 <a href="https://github.com/ncopa/su-exec">su-exec</a>，切换指定用户执行命令，exec 是执行后面的命令，替换当前的 shell 进程，这样在 docker stop 给容器内 pid 为 1 的进程发送信号，业务进程能收到信号进行优雅退出，而没 exec 的话，pid 为 1 的进程是 shell 脚本，它不会转发信号的。</p><p> <code>ENTRYPOINT</code> 使用脚本当作入口的形式，最后业务切用户执行，即使使用 docker exec 还是使用镜像默认的 USER root，排查问题也方便。 也推荐使用镜像之前，先看官方的启动脚本，例如 mongodb 官方镜像是支持类似 redis 这种非 root 启动的，但是我们 k8s 里是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    - name: &#123;&#123; NODE_NAME &#125;&#125;</span><br><span class="line">      image: xxx/mongo:xxx</span><br><span class="line">      command:</span><br><span class="line">        - mongod</span><br><span class="line">        - &quot;--port&quot;</span><br></pre></td></tr></table></figure><p>这样覆盖了 entrypoint，没有使用官方启动脚本执行，就是 root 用户，改为下面的不覆盖就行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- name: &#123;&#123; NODE_NAME &#125;&#125;</span><br><span class="line">  image: xxx/mongo:xxx</span><br><span class="line">  args: # &lt;--- 这里</span><br><span class="line">    - mongod</span><br><span class="line">    - &quot;--port&quot;</span><br></pre></td></tr></table></figure><p>要注意一个点，su-exec 在 alpine 里可以包管理安装，非 alpine 的基础镜像使用 gosu 可以参考 redis 官方镜像</p><h3 id="案例实战"><a href="#案例实战" class="headerlink" title="案例实战"></a>案例实战</h3><p>这列梳理一些我做的案例。先说一些知识点：</p><ul><li>产生 pid 和 sock 文件的，可以放 &#x2F;tmp 下</li><li>业务进程非 root 对 <code>/dev/stdxxx</code> 没权限的，可以脚本里 <code>chmod a+w /dev/std*</code></li><li>如果自己业务镜像产生的数据会被其他容器挂载操作数据，你的业务进程最好创建用户的时候使用固定同样的 <code>uid:gid</code> ，例如我们的 mysql-backup 备份 mysql 数据用到的用户 <code>uid:gid</code> 保持和 mysql 官方镜像一致，这样不需要修改 mysql 数据目录权限和 owner</li><li>不要 <code>chmod -R 777</code> 目录</li></ul><h4 id="机器码处理"><a href="#机器码处理" class="headerlink" title="机器码处理"></a>机器码处理</h4><p>获取机器码一般是使用 <code>dmidecode -s system-uuid</code> ，但是容器内你以 root 执行会报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -ti debian:11</span><br><span class="line">$ apt update &amp;&amp; apt-get install -y dmidecode</span><br><span class="line">$ dmidecode -s system-uuid</span><br><span class="line">/dev/mem: No such file or directory</span><br></pre></td></tr></table></figure><p>所以之前我们都是读取 <code>/sys/devices/virtual/dmi/id/product_uuid</code>，但是非 root 后无法读取，因为该文件权限为 <code>0400</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l /sys/devices/virtual/dmi/id/product_uuid</span><br><span class="line">-r-------- 1 root root 4096 Nov  3 08:48 /sys/devices/virtual/dmi/id/product_uuid</span><br></pre></td></tr></table></figure><p>且该文件是<a href="https://github.com/torvalds/linux/blob/master/drivers/firmware/dmi-id.c#L61">内核设置的权限</a>，无法被更改。</p><p>后面尝试发现一些信息:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ strace dmidecode -s system-uuid</span><br><span class="line">...</span><br><span class="line">openat(AT_FDCWD, &quot;/sys/firmware/dmi/tables/smbios_entry_point&quot;, O_RDONLY)</span><br><span class="line">...</span><br><span class="line">openat(AT_FDCWD, &quot;/sys/firmware/dmi/tables/DMI&quot;, O_RDONLY)</span><br></pre></td></tr></table></figure><p>发现读取了这俩文件，搜索资料发现是 dmi table，例如 root 下可以这样获取机器码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ dmidecode -t 1  &lt; /sys/firmware/dmi/tables/DMI</span><br><span class="line">$ dmidecode -t 1 -u &lt; /sys/firmware/dmi/tables/DMI</span><br></pre></td></tr></table></figure><p>该文件内容按照 DMI 规范字节结构解析可以得到不少信息。然后找到了一个 go 库，在 linux 上尝试成功：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;fmt&quot;</span></span><br><span class="line"><span class="string">&quot;log&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/digitalocean/go-smbios/smbios&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// Find SMBIOS data in operating system-specific location.</span></span><br><span class="line">rc, _, err := smbios.Stream()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;failed to open stream: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Be sure to close the stream!</span></span><br><span class="line"><span class="keyword">defer</span> rc.Close()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Decode SMBIOS structures from the stream.</span></span><br><span class="line">d := smbios.NewDecoder(rc)</span><br><span class="line">ss, err := d.Decode()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;failed to decode structures: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, s := <span class="keyword">range</span> ss &#123;</span><br><span class="line"><span class="keyword">if</span> s.Header.Type == <span class="number">1</span> &#123;</span><br><span class="line">d := s.Formatted</span><br><span class="line">fmt.Printf(<span class="string">&quot;UUID: %X%X%X%X-%X%X-%X%X-%X%X-%X%X%X%X%X%X\n&quot;</span>, </span><br><span class="line">d[<span class="number">7</span>], d[<span class="number">6</span>], d[<span class="number">5</span>], d[<span class="number">4</span>],</span><br><span class="line">d[<span class="number">9</span>], d[<span class="number">8</span>], d[<span class="number">11</span>], d[<span class="number">10</span>], d[<span class="number">12</span>], d[<span class="number">13</span>],</span><br><span class="line">d[<span class="number">14</span>], d[<span class="number">15</span>], d[<span class="number">16</span>], d[<span class="number">17</span>], d[<span class="number">18</span>], d[<span class="number">19</span>],</span><br><span class="line">)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>机器上测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ dmidecode -s system-uuid | <span class="built_in">tr</span> a-z A-Z</span><br><span class="line">66C0F667-71A0-xxxx-xxxx-4AC0A21F5428</span><br><span class="line">$ go build -o /tmp/uuid-go test.go</span><br><span class="line">$ <span class="built_in">chmod</span> a+r /sys/firmware/dmi/tables/DMI</span><br><span class="line">$ su - guanzhang</span><br><span class="line">guanzhang@guan:~$ /tmp/uuid-go </span><br><span class="line">UUID: 66C0F667-71A0-xxxx-xxxx-4AC0A21F5428</span><br></pre></td></tr></table></figure><p>然后把宿主机的 <code>/sys/firmware/dmi/tables</code> 挂载到 <code>/rootfs/sys/firmware/dmi/tables</code> 里，在 gosu 之前 <code>chmod a+r /rootfs/sys/firmware/dmi/tables/DMI</code>，业务使用上面的库 hack 后，从指定路径的 DMI 信息即可获取到机器码。</p><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>没啥说的，加了 gosu 后再加启动脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$&#123;1:0:1&#125;</span>&quot;</span> = <span class="string">&#x27;-&#x27;</span> ]; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">set</span> -- etcd <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># RUN_USER 设置为 nobody 启动</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&#x27;etcd&#x27;</span> ] || [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&#x27;/usr/local/bin/etcd&#x27;</span> ];<span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">&quot;<span class="subst">$(id -u)</span>&quot;</span> = <span class="string">&#x27;0&#x27;</span> -a -n <span class="string">&quot;<span class="variable">$RUN_USER</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    find /var/lib/etcd \! -user <span class="variable">$&#123;RUN_USER&#125;</span> -<span class="built_in">exec</span> <span class="built_in">chown</span> <span class="variable">$&#123;RUN_USER&#125;</span> <span class="string">&#x27;&#123;&#125;&#x27;</span> +</span><br><span class="line">    <span class="built_in">exec</span> gosu <span class="variable">$&#123;RUN_USER&#125;</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br></pre></td></tr></table></figure><p>为了不影响其他分支，这里我用了 env 作为开关，<a href="https://github.com/wurstmeister/kafka-docker">wurstmeister&#x2F;kafka-docker</a> 也是一样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$&#123;1:0:1&#125;</span>&quot;</span> = <span class="string">&#x27;-&#x27;</span> ]; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">set</span> -- start-kafka.sh <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># RUN_USER 设置为 nobody 启动</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&#x27;start-kafka.sh&#x27;</span> ] || [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&#x27;/usr/bin/start-kafka.sh&#x27;</span> ];<span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">&quot;<span class="subst">$(id -u)</span>&quot;</span> = <span class="string">&#x27;0&#x27;</span> -a -n <span class="string">&quot;<span class="variable">$RUN_USER</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">find $(<span class="built_in">readlink</span> -f <span class="variable">$&#123;KAFKA_HOME&#125;</span>) \! -user <span class="variable">$&#123;RUN_USER&#125;</span> -<span class="built_in">exec</span> <span class="built_in">chown</span> <span class="variable">$&#123;RUN_USER&#125;</span> <span class="string">&#x27;&#123;&#125;&#x27;</span> +</span><br><span class="line">find /kafka \! -user <span class="variable">$&#123;RUN_USER&#125;</span> -<span class="built_in">exec</span> <span class="built_in">chown</span> <span class="variable">$&#123;RUN_USER&#125;</span> <span class="string">&#x27;&#123;&#125;&#x27;</span> +</span><br><span class="line">    <span class="built_in">exec</span> gosu <span class="variable">$&#123;RUN_USER&#125;</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br></pre></td></tr></table></figure><p>其他的，例如 promtail 啥的都是一样，不再举例，自行制作</p><h4 id="coredns"><a href="#coredns" class="headerlink" title="coredns"></a>coredns</h4><p>coredns 1.11.0 才开始非 root 启动，我们业务使用的是 1.10.1 的，不升级避免客户现场出现问题，所以重做镜像最稳妥：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ARG</span> DEBIAN_IMAGE=debian:stable-slim</span><br><span class="line"><span class="keyword">ARG</span> BASE=gcr.io/distroless/static-debian12:nonroot</span><br><span class="line"><span class="keyword">FROM</span> coredns/coredns:<span class="number">1.10</span>.<span class="number">1</span> as bin</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span>  $&#123;DEBIAN_IMAGE&#125; AS build</span><br><span class="line"><span class="keyword">SHELL</span><span class="language-bash"> [ <span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-ec&quot;</span> ]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">export</span> DEBCONF_NONINTERACTIVE_SEEN=<span class="literal">true</span> \</span></span><br><span class="line"><span class="language-bash">           DEBIAN_FRONTEND=noninteractive \</span></span><br><span class="line"><span class="language-bash">           DEBIAN_PRIORITY=critical \</span></span><br><span class="line"><span class="language-bash">           TERM=linux ; \</span></span><br><span class="line"><span class="language-bash">    apt-get -qq update ; \</span></span><br><span class="line"><span class="language-bash">    apt-get -yyqq upgrade ; \</span></span><br><span class="line"><span class="language-bash">    apt-get -yyqq install ca-certificates libcap2-bin; \</span></span><br><span class="line"><span class="language-bash">    apt-get clean</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> --from=bin /coredns /coredns</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">setcap</span> cap_net_bind_service=+ep /coredns</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span>  $&#123;BASE&#125;</span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> --from=build /coredns /coredns</span></span><br><span class="line"><span class="keyword">USER</span> nonroot:nonroot</span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">53</span> <span class="number">53</span>/udp</span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;/coredns&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>非 root 用户是无法监听 1024 以下端口的，coredns 监听 53 端口是因为使用了 <code>setcap cap_net_bind_service=+ep /coredns</code>，但是这个属性属于扩展属性，docker 构建多层 COPY 会不支持而丢失，必须使用 buildkit 构建，否则 cap 信息丢失，部署上去无法监听 53 端口：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOCKER_BUILDKIT=1 docker build --platform=amd64  . -t coredns/coredns:1.10.1  --load</span><br></pre></td></tr></table></figure><h4 id="consul"><a href="#consul" class="headerlink" title="consul"></a>consul</h4><p>consul 镜像也支持，但是 chown 的时候没带 -R 选项。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;$(stat -c %u &quot;$CONSUL_DATA_DIR&quot;)&quot; != &quot;$&#123;CONSUL_UID&#125;&quot; ]; then</span><br><span class="line">  chown $&#123;CONSUL_UID&#125;:$&#123;CONSUL_GID&#125; &quot;$CONSUL_DATA_DIR&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>这里会存在一个问题，如果之前是覆盖了 entrypoint 使用 root 启动的，再切正确姿势下，因为 data 目录下子目录没被 chown，consul 在 data 下子目录写入 node-id 会报错没权限，所以我是这样 hack 重做镜像的：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ARG</span> VER=<span class="number">1.8</span>.<span class="number">3</span></span><br><span class="line"><span class="keyword">FROM</span> consul:$&#123;VER&#125;</span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> sed -ri -e <span class="string">&#x27;s/(chown)(\s+consul:)/\1 -R\2/&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">        -e <span class="string">&#x27;1s@/usr/bin/dumb-init\s+@@&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">    /usr/local/bin/docker-entrypoint.sh</span></span><br></pre></td></tr></table></figure><p>去掉 <code>dumb-init</code> 是因为客户要求容器内所有进程都是非 root，不去掉 pid 为 1 的就是 root 用户 dumb-init sh 进程</p><h4 id="docker-sock-文件"><a href="#docker-sock-文件" class="headerlink" title="docker.sock 文件"></a>docker.sock 文件</h4><p>有些进程是需要挂载 <code>/var/run</code> 为了使用宿主机的 <code>/var/run/docker.sock</code> 和宿主机 docker 通信的，这里我们使用 cadvisor 举例：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ARG</span> VER=v0.<span class="number">37.5</span></span><br><span class="line"><span class="keyword">FROM</span> gcr.m.daocloud.io/cadvisor/cadvisor:$&#123;VER&#125;</span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">set</span> -eux; \</span></span><br><span class="line"><span class="language-bash">    sed -i <span class="string">&#x27;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&#x27;</span> /etc/apk/repositories; \</span></span><br><span class="line"><span class="language-bash">    apk update; \</span></span><br><span class="line"><span class="language-bash">    apk add --no-cache \</span></span><br><span class="line"><span class="language-bash">      curl \</span></span><br><span class="line"><span class="language-bash">      su-exec; \</span></span><br><span class="line"><span class="language-bash">    <span class="built_in">rm</span> -rf /var/cache/apk/* /tmp/* </span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> docker-entrypoint.sh /</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;/docker-entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;cadvisor&quot;</span>, <span class="string">&quot;-logtostderr&quot;</span>]</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line">[ -z <span class="string">&quot;<span class="variable">$D_SOCK</span>&quot;</span> ] &amp;&amp; D_SOCK=/var/run/docker.sock</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$&#123;1:0:1&#125;</span>&quot;</span> = <span class="string">&#x27;-&#x27;</span> ]; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">set</span> -- cadvisor <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&#x27;cadvisor&#x27;</span> ] || [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&#x27;/usr/bin/cadvisor&#x27;</span> ];<span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">&quot;<span class="subst">$(id -u)</span>&quot;</span> = <span class="string">&#x27;0&#x27;</span> -a -n <span class="string">&quot;<span class="variable">$RUN_USER</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">if</span> [ -S <span class="string">&quot;<span class="variable">$&#123;D_SOCK&#125;</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">            group_id=`<span class="built_in">stat</span> -c <span class="string">&quot;%g&quot;</span> <span class="string">&quot;<span class="variable">$&#123;D_SOCK&#125;</span>&quot;</span>`</span><br><span class="line">            <span class="keyword">if</span> ! getent group | <span class="built_in">cut</span> -d: -f3 | grep -wq <span class="variable">$group_id</span>; <span class="keyword">then</span></span><br><span class="line">                addgroup -g <span class="variable">$&#123;group_id&#125;</span> docker</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">            group_name=$(<span class="built_in">stat</span> -c <span class="string">&quot;%G&quot;</span> <span class="string">&quot;<span class="variable">$&#123;D_SOCK&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> ! <span class="built_in">id</span> -nG <span class="variable">$&#123;RUN_USER&#125;</span> | grep -w <span class="variable">$&#123;group_name&#125;</span>;<span class="keyword">then</span></span><br><span class="line">                <span class="comment"># ensure user in docker group</span></span><br><span class="line">                adduser <span class="variable">$&#123;RUN_USER&#125;</span> <span class="variable">$&#123;group_name&#125;</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">exec</span> su-exec <span class="variable">$RUN_USER</span> <span class="variable">$@</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> <span class="variable">$@</span></span><br></pre></td></tr></table></figure><ul><li>cadvisor 挂载了宿主机的 rootfs ，改为纯非 root 不行，但是 cadvisor 镜像内有个 <code>operator</code> 用户的 gid 是 0，利用启动脚本和 docker 权限来改造成非 root 启动。</li><li>docker.sock 权限是 <code>0660</code>，利用 shell 把 operator 用户加到 docker 组里即可（必须取 gid）。这里要注意的是，不同版本 alpine 和其他 rootfs 的 adduser&#x2F;addgroup 参数不一样，自行注意 shell 兼容</li></ul><p>设置 “RUN_USER” 为 <code>operator</code> ，然后设置宿主机的 docker 的 data-root 下面权限（可以使用 systemd 的<code>ExecStartPost=</code>）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/var/lib//docker/image：750   ok</span><br><span class="line">/var/lib//docker/image/overlay2：750 ok</span><br><span class="line">/var/lib//docker/image/overlay2/layerdb：750 ok</span><br></pre></td></tr></table></figure><p>cadvisor 参数为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">-docker_only=true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">-housekeeping_interval=20s</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">-disable_metrics=accelerator,cpu_topology,tcp,udp,percpu,sched,process,hugetlb,referenced_memory,resctrl</span></span><br></pre></td></tr></table></figure><h4 id="cron"><a href="#cron" class="headerlink" title="cron"></a>cron</h4><p>非 root 无法使用 cron 启动，使用 <a href="https://github.com/webdevops/go-crond">go-crond</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/kubernetes/enhancements/issues/127">k8s 社区关于支持 user namespace 提议</a></li><li><a href="https://www.dmtf.org/sites/default/files/standards/documents/DSP0134_3.3.0.pdf">dmi 信息规范</a></li><li><a href="https://github.com/mirror/dmidecode/blob/master/dmidecode.c#L448">dmidecode 源码</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近业务容器的非 root 启动改造实战案例经验，后续有新的也更新进来&lt;/p&gt;</summary>
    
    
    
    
    <category term="linux" scheme="http://zhangguanzhang.github.io/tags/linux/"/>
    
    <category term="non-root" scheme="http://zhangguanzhang.github.io/tags/non-root/"/>
    
    <category term="containers" scheme="http://zhangguanzhang.github.io/tags/containers/"/>
    
  </entry>
  
  <entry>
    <title>proxmox 7.4 升级8 后开机卡在 Loading kernel 6.2.16-15</title>
    <link href="http://zhangguanzhang.github.io/2023/10/20/pve7-to-8-hang-on-start/"/>
    <id>http://zhangguanzhang.github.io/2023/10/20/pve7-to-8-hang-on-start/</id>
    <published>2023-10-20T19:07:30.000Z</published>
    <updated>2023-10-20T19:07:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>proxmox 7.4 升级 8 后开机卡住的一次解决过程</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>最开始在 7.2 dist-upgrade 后升级刀 7.4 然后按照官方文档升级 8 的过程中，最后的 apt 阶段报错 proxmox-xxx 和 kernel <code>Sub-process /usr/bin/dpkg returned an error code</code></p><h3 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h3><p>升级完成后重启卡在 <code>Loading kernel 6.2.16-15</code>，然后重启后切到老内核，进去看了下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ grep -P &#x27;menuentry.+pve&#x27; /boot/grub/grub.cfg </span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 6.2.16-15-pve&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-6.2.16-15-pve-advanced-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 6.2.16-15-pve (recovery mode)&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-6.2.16-15-pve-recovery-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 5.15.126-1-pve&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-5.15.126-1-pve-advanced-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 5.15.126-1-pve (recovery mode)&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-5.15.126-1-pve-recovery-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 5.4.203-1-pve&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-5.4.203-1-pve-advanced-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 5.4.203-1-pve (recovery mode)&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-5.4.203-1-pve-recovery-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 5.4.34-1-pve&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-5.4.34-1-pve-advanced-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br><span class="line">menuentry &#x27;Proxmox VE GNU/Linux, with Linux 5.4.34-1-pve (recovery mode)&#x27; --class proxmox --class gnu-linux --class gnu --class os $menuentry_id_option &#x27;gnulinux-5.4.34-1-pve-recovery-bcdbdabd-1b22-41df-a6a7-bef0e7fd8e52&#x27; &#123;</span><br></pre></td></tr></table></figure><p>看着是内核几个 dpkg 包安装阶段有问题，然后查询下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ dpkg -l | grep pve-ker</span><br><span class="line">ii  pve-firmware                            3.8-2                               all          Binary firmware code for the pve-kernel</span><br><span class="line">ii  pve-kernel-5.15                         7.4-7                               all          Latest Proxmox VE Kernel Image</span><br><span class="line">ii  pve-kernel-5.15.126-1-pve               5.15.126-1                          amd64        Proxmox Kernel Image</span><br><span class="line">ii  pve-kernel-5.15.60-1-pve                5.15.60-1                           amd64        Proxmox Kernel Image</span><br><span class="line">ii  pve-kernel-5.4                          6.4-20                              all          Latest Proxmox VE Kernel Image</span><br><span class="line">ii  pve-kernel-5.4.203-1-pve                5.4.203-1                           amd64        The Proxmox PVE Kernel Image</span><br><span class="line">ii  pve-kernel-5.4.34-1-pve                 5.4.34-2                            amd64        The Proxmox PVE Kernel Image</span><br><span class="line">rc  pve-kernel-helper                       7.2-12                              all          Function for various kernel maintenance tasks.</span><br></pre></td></tr></table></figure><p>手动安装下 kernel :</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">$ apt install pve-kernel-6.2.16-15-pve-1</span><br><span class="line">正在读取软件包列表... 完成</span><br><span class="line">正在分析软件包的依赖关系树... 完成</span><br><span class="line">正在读取状态信息... 完成                 </span><br><span class="line">E: 无法定位软件包 pve-kernel-6.2.16-15-pve-1</span><br><span class="line">E: 无法按照 glob ‘pve-kernel-6.2.16-15-pve-1’ 找到任何软件包</span><br><span class="line">$ apt install pve-kernel-6.2.16-15*</span><br><span class="line">正在读取软件包列表... 完成</span><br><span class="line">正在分析软件包的依赖关系树... 完成</span><br><span class="line">正在读取状态信息... 完成                 </span><br><span class="line">注意，根据Glob &#x27;pve-kernel-6.2.16-15*&#x27; 选中了 &#x27;pve-kernel-6.2.16-15-pve&#x27;</span><br><span class="line">注意，选中 &#x27;proxmox-kernel-6.2.16-15-pve&#x27; 而非 &#x27;pve-kernel-6.2.16-15-pve&#x27;</span><br><span class="line">proxmox-kernel-6.2.16-15-pve 已经是最新版 (6.2.16-15)。</span><br><span class="line">proxmox-kernel-6.2.16-15-pve 已设置为手动安装。</span><br><span class="line">下列软件包是自动安装的并且现在不需要了：</span><br><span class="line">  caribou dctrl-tools docker-scan-plugin endeavour endeavour-common g++-10 gedit gedit-common gedit-plugin-bookmarks gedit-plugin-bracket-completion gedit-plugin-character-map gedit-plugin-code-comment gedit-plugin-color-picker</span><br><span class="line">  gedit-plugin-color-schemer gedit-plugin-draw-spaces gedit-plugin-git gedit-plugin-join-lines gedit-plugin-multi-edit gedit-plugin-session-saver gedit-plugin-smart-spaces gedit-plugin-synctex gedit-plugin-terminal</span><br><span class="line">  gedit-plugin-text-size gedit-plugin-word-completion gedit-plugins gedit-plugins-common giblib1 gir1.2-amtk-5 gir1.2-caribou-1.0 gir1.2-champlain-0.12 gir1.2-dazzle-1.0 gir1.2-gdata-0.0 gir1.2-geocodeglib-1.0 gir1.2-gfbgraph-0.2</span><br><span class="line">  gir1.2-ggit-1.0 gir1.2-gnomebluetooth-1.0 gir1.2-gtkchamplain-0.12 gir1.2-gucharmap-2.90 gir1.2-gweather-3.0 gir1.2-handy-0.0 gir1.2-rest-0.7 gir1.2-tepl-6 gir1.2-tracker-2.0 gir1.2-vte-2.91 gir1.2-zpj-0.0 gnome-bluetooth</span><br><span class="line">  gnome-bluetooth-common gnome-getting-started-docs gnome-online-miners gnome-screenshot gnome-todo guile-2.2-libs libamtk-5-0 libamtk-5-common libappstream-glib8 libatk1.0-data libavfilter7 libavformat58 libavresample4 libbpf0</span><br><span class="line">  libcamel-1.2-62 libcaribou-common libcaribou0 libcbor0 libchamplain-0.12-0 libchamplain-gtk-0.12-0 libclang-cpp11 libclang1-11 libcmis-0.5-5v5 libcolord-gtk1 libdazzle-1.0-0 libdazzle-common libdleyna-connector-dbus-1.0-1</span><br><span class="line">  libdleyna-core-1.0-5 libdns-export1110 libebackend-1.2-10 libebook-1.2-20 libebook-contacts-1.2-3 libecal-2.0-1 libedata-book-1.2-26 libedata-cal-2.0-1 libedataserver-1.2-25 libedataserverui-1.2-2 libexporter-tiny-perl</span><br><span class="line">  libextutils-pkgconfig-perl libfluidsynth2 libfwupdplugin1 libgdk-pixbuf-xlib-2.0-0 libgdk-pixbuf2.0-0 libgeocode-glib0 libgfbgraph-0.2-0 libgit2-1.1 libgit2-1.5 libgit2-glib-1.0-0 libgnome-bluetooth13 libgnome-todo libgs9-common</span><br><span class="line">  libgssdp-1.2-0 libgucharmap-2-90-7 libgupnp-1.2-0 libgweather-3-16 libgweather-common libhandy-0.0-0 libhttp-parser2.9 libicu67 libigdgmm11 libilmbase25 libisc-export1105 libjim0.79 libleveldb1d liblibreoffice-java</span><br><span class="line">  liblist-moreutils-perl liblist-moreutils-xs-perl libllvm11 liblttng-ust-ctl4 liblttng-ust0 libmalcontent-ui-0-0 libmbedcrypto3 libmbedtls12 libmbedtls14 libmbedx509-0 libmbedx509-1 libmms0 libmozjs-78-0 libmpdec3 libmusicbrainz5-2</span><br><span class="line">  libmusicbrainz5cc2v5 libnautilus-extension1a libneon27-gnutls libntfs-3g883 libofa0 libopenexr25 libopts25 liborcus-0.16-0 liborcus-parser-0.16-0 libperl5.32 libpkgconf3 libpod-parser-perl libpoppler102 libpostproc55 libprocps8</span><br><span class="line">  libprotobuf23 libpython3.9 libpython3.9-minimal libpython3.9-stdlib libqpdf28 libqrcodegencpp1 libquvi-0.9-0.9.3 libquvi-scripts-0.9 librest-0.7-0 librygel-core-2.6-2 librygel-db-2.6-2 libsgutils2-2 libsrt1.4-gnutls libstdc++-10-dev</span><br><span class="line">  libswscale5 libtepl-5-0 libtepl-6-2 libtepl-common libtiff5 libtracker-control-2.0-0 libtracker-miner-2.0-0 libtracker-sparql-2.0-0 libunoloader-java liburing1 libvncserver1 libxmlb1 libzapojit-0.0-0 linux-compiler-gcc-10-x86</span><br><span class="line">  linux-headers-5.10.0-26-amd64 linux-headers-5.10.0-26-common linux-kbuild-5.10 lua-bitop lua-expat lua-json lua-socket perl-modules-5.32 pkg-config pkgconf pkgconf-bin pve-headers pve-headers-5.15.60-1-pve python3-ldb python3-talloc</span><br><span class="line">  python3.9 python3.9-minimal telnet unattended-upgrades unoconv ure-java</span><br><span class="line">使用&#x27;apt autoremove&#x27;来卸载它(它们)。</span><br><span class="line">升级了 0 个软件包，新安装了 0 个软件包，要卸载 0 个软件包，有 0 个软件包未被升级。</span><br><span class="line">有 6 个软件包没有被完全安装或卸载。</span><br><span class="line">解压缩后会消耗 0 B 的额外空间。</span><br><span class="line">您希望继续执行吗？ [Y/n] y</span><br><span class="line">正在设置 proxmox-kernel-6.2.16-15-pve (6.2.16-15) ...</span><br><span class="line">Examining /etc/kernel/postinst.d.</span><br><span class="line">run-parts: executing /etc/kernel/postinst.d/dkms 6.2.16-15-pve /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">dkms: running auto installation service for kernel 6.2.16-15-pve.</span><br><span class="line">Sign command: /lib/modules/6.2.16-15-pve/build/scripts/sign-file</span><br><span class="line">Signing key: /var/lib/dkms/mok.key</span><br><span class="line">Public certificate (MOK): /var/lib/dkms/mok.pub</span><br><span class="line">Error! The /var/lib/dkms/wireguard/1.0.20210219/6.2.16-15-pve/x86_64/dkms.conf for module wireguard includes a BUILD_EXCLUSIVE directive which does not match this kernel/arch/config.</span><br><span class="line">This indicates that it should not be built.</span><br><span class="line">Error! One or more modules failed to install during autoinstall.</span><br><span class="line">Refer to previous errors for more information.</span><br><span class="line">dkms: autoinstall for kernel: 6.2.16-15-pve failed!</span><br><span class="line">run-parts: /etc/kernel/postinst.d/dkms exited with return code 11</span><br><span class="line">Failed to process /etc/kernel/postinst.d at /var/lib/dpkg/info/proxmox-kernel-6.2.16-15-pve.postinst line 20.</span><br><span class="line">dpkg: 处理软件包 proxmox-kernel-6.2.16-15-pve (--configure)时出错：</span><br><span class="line"> 已安装 proxmox-kernel-6.2.16-15-pve 软件包 post-installation 脚本 子进程返回错误状态 2</span><br><span class="line">正在设置 linux-headers-6.1.0-13-amd64 (6.1.55-1) ...</span><br><span class="line">/etc/kernel/header_postinst.d/dkms:</span><br><span class="line">dkms: running auto installation service for kernel 6.1.0-13-amd64.</span><br><span class="line">Sign command: /usr/lib/linux-kbuild-6.1/scripts/sign-file</span><br><span class="line">Signing key: /var/lib/dkms/mok.key</span><br><span class="line">Public certificate (MOK): /var/lib/dkms/mok.pub</span><br><span class="line">Error! The /var/lib/dkms/wireguard/1.0.20210219/6.1.0-13-amd64/x86_64/dkms.conf for module wireguard includes a BUILD_EXCLUSIVE directive which does not match this kernel/arch/config.</span><br><span class="line">This indicates that it should not be built.</span><br><span class="line">Error! One or more modules failed to install during autoinstall.</span><br><span class="line">Refer to previous errors for more information.</span><br><span class="line">dkms: autoinstall for kernel: 6.1.0-13-amd64 failed!</span><br><span class="line">run-parts: /etc/kernel/header_postinst.d/dkms exited with return code 11</span><br><span class="line">Failed to process /etc/kernel/header_postinst.d at /var/lib/dpkg/info/linux-headers-6.1.0-13-amd64.postinst line 11.</span><br><span class="line">dpkg: 处理软件包 linux-headers-6.1.0-13-amd64 (--configure)时出错：</span><br><span class="line"> 已安装 linux-headers-6.1.0-13-amd64 软件包 post-installation 脚本 子进程返回错误状态 1</span><br><span class="line">dpkg: 依赖关系问题使得 linux-headers-amd64 的配置工作不能继续：</span><br><span class="line"> linux-headers-amd64 依赖于 linux-headers-6.1.0-13-amd64 (= 6.1.55-1)；然而：</span><br><span class="line">  软件包 linux-headers-6.1.0-13-amd64 尚未配置。</span><br><span class="line"></span><br><span class="line">dpkg: 处理软件包 linux-headers-amd64 (--configure)时出错：</span><br><span class="line"> 依赖关系问题 - 仍未被配置</span><br><span class="line">dpkg: 依赖关系问题使得 proxmox-kernel-6.2 的配置工作不能继续：</span><br><span class="line"> proxmox-kernel-6.2 依赖于 proxmox-kernel-6.2.16-15-pve；然而：</span><br><span class="line">  软件包 proxmox-kernel-6.2.16-15-pve 尚未配置。</span><br><span class="line"></span><br><span class="line">dpkg: 处理软件包 proxmox-kernel-6.2 (--configure)时出错：</span><br><span class="line"> 依赖关系问题 - 仍未被配置</span><br><span class="line">dpkg: 依赖关系问题使得 proxmox-default-kernel 的配置工作不能继续：</span><br><span class="line"> proxmox-default-kernel 依赖于 proxmox-kernel-6.2；然而：</span><br><span class="line">  软件包 proxmox-kernel-6.2 尚未配置。</span><br><span class="line"></span><br><span class="line">dpkg: 处理软件包 proxmox-default-kernel (--configure)时出错：</span><br><span class="line"> 依赖关系问题 - 仍未被配置</span><br><span class="line">dpkg: 依赖关系问题使得 proxmox-ve 的配置工作不能继续：</span><br><span class="line"> proxmox-ve 依赖于 proxmox-default-kernel；然而：</span><br><span class="line">  软件包 proxmox-default-kernel 尚未配置。</span><br><span class="line"></span><br><span class="line">dpkg: 处理软件包 proxmox-ve (--configure)时出错：</span><br><span class="line"> 依赖关系问题 - 仍未被配置</span><br><span class="line">在处理时有错误发生：</span><br><span class="line"> proxmox-kernel-6.2.16-15-pve</span><br><span class="line"> linux-headers-6.1.0-13-amd64</span><br><span class="line"> linux-headers-amd64</span><br><span class="line"> proxmox-kernel-6.2</span><br><span class="line"> proxmox-default-kernel</span><br><span class="line"> proxmox-ve</span><br><span class="line">E: Sub-process /usr/bin/dpkg returned an error code (1)</span><br></pre></td></tr></table></figure><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>之前安装了 wireguard ，应该是 dkms 冲突了，处理下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">$ dkms status</span><br><span class="line">wireguard/1.0.20210219, 5.4.203-1-pve, x86_64: installed</span><br><span class="line"></span><br><span class="line">$ dkms remove wireguard/1.0.20210219</span><br><span class="line">$ apt remove dkms</span><br><span class="line">正在读取软件包列表... 完成</span><br><span class="line">正在分析软件包的依赖关系树... 完成</span><br><span class="line">正在读取状态信息... 完成                 </span><br><span class="line">下列软件包是自动安装的并且现在不需要了：</span><br><span class="line">  caribou dctrl-tools docker-scan-plugin endeavour endeavour-common g++-10 gedit gedit-common gedit-plugin-bookmarks gedit-plugin-bracket-completion gedit-plugin-character-map gedit-plugin-code-comment gedit-plugin-color-picker</span><br><span class="line">  gedit-plugin-color-schemer gedit-plugin-draw-spaces gedit-plugin-git gedit-plugin-join-lines gedit-plugin-multi-edit gedit-plugin-session-saver gedit-plugin-smart-spaces gedit-plugin-synctex gedit-plugin-terminal</span><br><span class="line">  gedit-plugin-text-size gedit-plugin-word-completion gedit-plugins gedit-plugins-common giblib1 gir1.2-amtk-5 gir1.2-caribou-1.0 gir1.2-champlain-0.12 gir1.2-dazzle-1.0 gir1.2-gdata-0.0 gir1.2-geocodeglib-1.0 gir1.2-gfbgraph-0.2</span><br><span class="line">  gir1.2-ggit-1.0 gir1.2-gnomebluetooth-1.0 gir1.2-gtkchamplain-0.12 gir1.2-gucharmap-2.90 gir1.2-gweather-3.0 gir1.2-handy-0.0 gir1.2-rest-0.7 gir1.2-tepl-6 gir1.2-tracker-2.0 gir1.2-vte-2.91 gir1.2-zpj-0.0 gnome-bluetooth</span><br><span class="line">  gnome-bluetooth-common gnome-getting-started-docs gnome-online-miners gnome-screenshot gnome-todo guile-2.2-libs libamtk-5-0 libamtk-5-common libappstream-glib8 libatk1.0-data libavfilter7 libavformat58 libavresample4 libbpf0</span><br><span class="line">  libcamel-1.2-62 libcaribou-common libcaribou0 libcbor0 libchamplain-0.12-0 libchamplain-gtk-0.12-0 libclang-cpp11 libclang1-11 libcmis-0.5-5v5 libcolord-gtk1 libdazzle-1.0-0 libdazzle-common libdleyna-connector-dbus-1.0-1</span><br><span class="line">  libdleyna-core-1.0-5 libdns-export1110 libebackend-1.2-10 libebook-1.2-20 libebook-contacts-1.2-3 libecal-2.0-1 libedata-book-1.2-26 libedata-cal-2.0-1 libedataserver-1.2-25 libedataserverui-1.2-2 libexporter-tiny-perl</span><br><span class="line">  libextutils-pkgconfig-perl libfluidsynth2 libfwupdplugin1 libgdk-pixbuf-xlib-2.0-0 libgdk-pixbuf2.0-0 libgeocode-glib0 libgfbgraph-0.2-0 libgit2-1.1 libgit2-1.5 libgit2-glib-1.0-0 libgnome-bluetooth13 libgnome-todo libgs9-common</span><br><span class="line">  libgssdp-1.2-0 libgucharmap-2-90-7 libgupnp-1.2-0 libgweather-3-16 libgweather-common libhandy-0.0-0 libhttp-parser2.9 libicu67 libigdgmm11 libilmbase25 libisc-export1105 libjim0.79 libleveldb1d liblibreoffice-java</span><br><span class="line">  liblist-moreutils-perl liblist-moreutils-xs-perl libllvm11 liblttng-ust-ctl4 liblttng-ust0 libmalcontent-ui-0-0 libmbedcrypto3 libmbedtls12 libmbedtls14 libmbedx509-0 libmbedx509-1 libmms0 libmozjs-78-0 libmpdec3 libmusicbrainz5-2</span><br><span class="line">  libmusicbrainz5cc2v5 libnautilus-extension1a libneon27-gnutls libntfs-3g883 libofa0 libopenexr25 libopts25 liborcus-0.16-0 liborcus-parser-0.16-0 libperl5.32 libpkgconf3 libpod-parser-perl libpoppler102 libpostproc55 libprocps8</span><br><span class="line">  libprotobuf23 libpython3.9 libpython3.9-minimal libpython3.9-stdlib libqpdf28 libqrcodegencpp1 libquvi-0.9-0.9.3 libquvi-scripts-0.9 librest-0.7-0 librygel-core-2.6-2 librygel-db-2.6-2 libsgutils2-2 libsrt1.4-gnutls libstdc++-10-dev</span><br><span class="line">  libswscale5 libtepl-5-0 libtepl-6-2 libtepl-common libtiff5 libtracker-control-2.0-0 libtracker-miner-2.0-0 libtracker-sparql-2.0-0 libunoloader-java liburing1 libvncserver1 libxmlb1 libzapojit-0.0-0 linux-compiler-gcc-10-x86</span><br><span class="line">  linux-compiler-gcc-12-x86 linux-headers-5.10.0-26-amd64 linux-headers-5.10.0-26-common linux-headers-6.1.0-13-amd64 linux-headers-6.1.0-13-common linux-headers-amd64 linux-kbuild-5.10 linux-kbuild-6.1 lua-bitop lua-expat lua-json</span><br><span class="line">  lua-socket perl-modules-5.32 pkg-config pkgconf pkgconf-bin pve-headers pve-headers-5.15 pve-headers-5.15.126-1-pve pve-headers-5.15.60-1-pve pve-headers-5.4.203-1-pve python3-ldb python3-talloc python3.9 python3.9-minimal telnet</span><br><span class="line">  unattended-upgrades unoconv ure-java</span><br><span class="line">使用&#x27;apt autoremove&#x27;来卸载它(它们)。</span><br><span class="line">下列软件包将被【卸载】：</span><br><span class="line">  dkms wireguard-dkms</span><br><span class="line">升级了 0 个软件包，新安装了 0 个软件包，要卸载 2 个软件包，有 0 个软件包未被升级。</span><br><span class="line">有 6 个软件包没有被完全安装或卸载。</span><br><span class="line">解压缩后将会空出 1,956 kB 的空间。</span><br><span class="line">您希望继续执行吗？ [Y/n] y</span><br><span class="line">(正在读取数据库 ... 系统当前共安装有 347124 个文件和目录。)</span><br><span class="line">正在卸载 wireguard-dkms (1.0.20210219-1) ...</span><br><span class="line">Module wireguard-1.0.20210219 for kernel 5.4.203-1-pve (x86_64).</span><br><span class="line">Before uninstall, this module version was ACTIVE on this kernel.</span><br><span class="line"></span><br><span class="line">wireguard.ko:</span><br><span class="line"> - Uninstallation</span><br><span class="line">   - Deleting from: /lib/modules/5.4.203-1-pve/updates/dkms/</span><br><span class="line"> - Original module</span><br><span class="line">   - No original module was found for this module on this kernel.</span><br><span class="line">   - Use the dkms install command to reinstall any previous module version.</span><br><span class="line">depmod....</span><br><span class="line">Deleting module wireguard-1.0.20210219 completely from the DKMS tree.</span><br><span class="line">正在卸载 dkms (3.0.10-8+deb12u1) ...</span><br><span class="line">正在设置 proxmox-kernel-6.2.16-15-pve (6.2.16-15) ...</span><br><span class="line">Examining /etc/kernel/postinst.d.</span><br><span class="line">run-parts: executing /etc/kernel/postinst.d/dkms 6.2.16-15-pve /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">run-parts: executing /etc/kernel/postinst.d/initramfs-tools 6.2.16-15-pve /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">update-initramfs: Generating /boot/initrd.img-6.2.16-15-pve</span><br><span class="line">setupcon: The keyboard model is unknown, assuming &#x27;pc105&#x27;. Keyboard may be configured incorrectly.</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/ip_discovery.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/vega10_cap.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/sienna_cichlid_cap.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/navi12_cap.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/aldebaran_cap.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/gc_11_0_0_toc.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/sienna_cichlid_mes1.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/sienna_cichlid_mes.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/navi10_mes.bin for module amdgpu</span><br><span class="line">W: Possible missing firmware /lib/firmware/amdgpu/gc_11_0_3_mes.bin for module amdgpu</span><br><span class="line">Running hook script &#x27;zz-proxmox-boot&#x27;..</span><br><span class="line">Re-executing &#x27;/etc/kernel/postinst.d/zz-proxmox-boot&#x27; in new private mount namespace..</span><br><span class="line">No /etc/kernel/proxmox-boot-uuids found, skipping ESP sync.</span><br><span class="line">run-parts: executing /etc/kernel/postinst.d/proxmox-auto-removal 6.2.16-15-pve /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">run-parts: executing /etc/kernel/postinst.d/unattended-upgrades 6.2.16-15-pve /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">run-parts: executing /etc/kernel/postinst.d/zz-proxmox-boot 6.2.16-15-pve /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">Re-executing &#x27;/etc/kernel/postinst.d/zz-proxmox-boot&#x27; in new private mount namespace..</span><br><span class="line">No /etc/kernel/proxmox-boot-uuids found, skipping ESP sync.</span><br><span class="line">run-parts: executing /etc/kernel/postinst.d/zz-update-grub 6.2.16-15-pve /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">Generating grub configuration file ...</span><br><span class="line">Found background image: /usr/share/images/desktop-base/desktop-grub.png</span><br><span class="line">Found linux image: /boot/vmlinuz-6.2.16-15-pve</span><br><span class="line">Found initrd image: /boot/initrd.img-6.2.16-15-pve</span><br><span class="line">Found linux image: /boot/vmlinuz-5.15.126-1-pve</span><br><span class="line">Found initrd image: /boot/initrd.img-5.15.126-1-pve</span><br><span class="line">Found linux image: /boot/vmlinuz-5.15.60-1-pve</span><br><span class="line">Found initrd image: /boot/initrd.img-5.15.60-1-pve</span><br><span class="line">Found linux image: /boot/vmlinuz-5.4.203-1-pve</span><br><span class="line">Found initrd image: /boot/initrd.img-5.4.203-1-pve</span><br><span class="line">Found linux image: /boot/vmlinuz-5.4.34-1-pve</span><br><span class="line">Found initrd image: /boot/initrd.img-5.4.34-1-pve</span><br><span class="line">Found memtest86+ 64bit EFI image: /boot/memtest86+x64.efi</span><br><span class="line">Warning: os-prober will not be executed to detect other bootable partitions.</span><br><span class="line">Systems on them will not be added to the GRUB boot configuration.</span><br><span class="line">Check GRUB_DISABLE_OS_PROBER documentation entry.</span><br><span class="line">Adding boot menu entry for UEFI Firmware Settings ...</span><br><span class="line">done</span><br><span class="line">正在设置 linux-headers-6.1.0-13-amd64 (6.1.55-1) ...</span><br><span class="line">正在设置 linux-headers-amd64 (6.1.55-1) ...</span><br><span class="line">正在设置 proxmox-kernel-6.2 (6.2.16-15) ...</span><br><span class="line">正在设置 proxmox-default-kernel (1.0.0) ...</span><br><span class="line">正在设置 proxmox-ve (8.0.2) ...</span><br><span class="line">正在处理用于 man-db (2.11.2-2) 的触发器 ...</span><br></pre></td></tr></table></figure><p>发现卸载 dkms 的时候包管理也处理好内核了，然后重启就好了</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://forums.linuxmint.com/viewtopic.php?t=404757">https://forums.linuxmint.com/viewtopic.php?t=404757</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;proxmox 7.4 升级 8 后开机卡住的一次解决过程&lt;/p&gt;</summary>
    
    
    
    
    <category term="linux" scheme="http://zhangguanzhang.github.io/tags/linux/"/>
    
    <category term="proxmox" scheme="http://zhangguanzhang.github.io/tags/proxmox/"/>
    
  </entry>
  
  <entry>
    <title>k8s 使用 nfs 下 pod 无法创建的解决思路</title>
    <link href="http://zhangguanzhang.github.io/2023/08/18/kubernetes-nfs-waiting-condition/"/>
    <id>http://zhangguanzhang.github.io/2023/08/18/kubernetes-nfs-waiting-condition/</id>
    <published>2023-08-18T19:27:30.000Z</published>
    <updated>2023-08-18T19:27:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>群友付费找我解决 pod 无法创建的过程，写出来给别人参考</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>k8s 群里有群友问 pod 创建调度到某节点后，长期处于 containercreating ，让他看日志他看不出啥来。后面加我还有付费让我看看</p><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>最开始是没挂载的部署一个 nginx 的 pod 出问题，describe 确实看不到啥信息，后面是 nfs pvc 的 pod 无法调度。</p><p>环境信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node -o wide</span><br><span class="line">NAME     STATUS   ROLES    AGE      VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">master   Ready    master   2y129d   v1.18.3   xxx.xx.xx.9   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   docker://19.3.5</span><br><span class="line">work01   Ready    &lt;none&gt;   2y129d   v1.18.3   xxx.xx.xx.1   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   docker://19.3.5</span><br><span class="line">work02   Ready    &lt;none&gt;   2y129d   v1.18.3   xxx.xx.xx.3   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   docker://19.3.5</span><br><span class="line">work03   Ready    &lt;none&gt;   2y129d   v1.18.3   xxx.xx.xx.4   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.24.1.el7.x86_64   docker://19.3.5</span><br></pre></td></tr></table></figure><h3 id="orphaned-pod-xxx-found-but"><a href="#orphaned-pod-xxx-found-but" class="headerlink" title="orphaned pod xxx found, but"></a>orphaned pod xxx found, but</h3><p>kubelet 日志刷下面的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubelet_volumes.go:154] orphaned pod &quot;xxx&quot; found, but volume paths are still present on disk : There were a total of 84 errors similar to this. Turn up verbosity to see them.</span><br></pre></td></tr></table></figure><p>这个是 1.20 还是哪个版本之前，pod 到其他节点或者删掉后，相关的一些目录还遗留在节点上的 <code>--root-dir</code> 下，默认是 <code>/var/lib/kubelet/pods</code> 下的 uuid 字样的目录，可以 find 下它确认里面的内容，以及看 etc-hosts 文件，看 hostname 后利用 kubectl get pod 查看是否存在这个 pod 名，不存在就是遗留目录，可以手动清理下。 这个问题我记得后续有人提交了 pr kubelet 会定期清理这种目录的。</p><h3 id="Faild-to-get-system-container-stats"><a href="#Faild-to-get-system-container-stats" class="headerlink" title="Faild to get system container stats"></a>Faild to get system container stats</h3><p>依次处理掉上面的日志里错误后，看到下面的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_sys_containers.go:47] Failed to get system container stats for &quot;/system.slice/docker.service&quot;: failed to get cgroup stats for &quot;/system.slice/docker.service&quot;: faild to get container info for &quot;system.slice/docker.service&quot;: unknow container &quot;/system.slice/docker.service&quot;</span><br></pre></td></tr></table></figure><p>这个是 kubelet 无法从 docker 获取一些信息，一般是 docker 出问题了，看了下 docker 日志也还正常。询问了下能不能重启 docker ，说上午就重启过了，那就是其他问题了。</p><p>他说 <code>df -h</code> 会卡住，应该要解决掉卡住的问题后再重启 docker，因为 docker 和 kubelet 收集信息会调用一些 fs 操作。安装了个 strace 看了下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">stat(&quot;/var/lib/kubelet/pods/e3b61daa-86a7-4e2d-8c82-bdd96e7a6da2/volumes/kubernetes.io~secret/jenkins-admin-token-6479r&quot;, &#123;st_mode=S_IFDIR|S_ISVTX|0777, st_size=140, ...&#125;) = 0</span><br><span class="line">stat(&quot;/var/lib/kubelet/pods/c527ccff-e623-48e1-90be-11930facc11b/volumes/kubernetes.io~secret/default-token-r9mv9&quot;, &#123;st_mode=S_IFDIR|S_ISVTX|0777, st_size=140, ...&#125;) = 0</span><br><span class="line">stat(&quot;/sys/fs/bpf&quot;, &#123;st_mode=S_IFDIR|S_ISVTX|0777, st_size=0, ...&#125;) = 0</span><br><span class="line">stat(&quot;/var/lib/kubelet/pods/e3b61daa-86a7-4e2d-8c82-bdd96e7a6da2/volumes/kubernetes.io~nfs/jenkins&quot;, &#123;st_mode=S_IFDIR|0777, st_size=3688, ...&#125;) = 0</span><br><span class="line">stat(&quot;/var/lib/kubelet/pods/c527ccff-e623-48e1-90be-11930facc11b/volumes/kubernetes.io~nfs/pv-es-master&quot;, ^C^Z</span><br><span class="line">[1]+  已停止               strace df -h</span><br></pre></td></tr></table></figure><p>可以看到卡住的路径，然后 umount 掉：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">umount -lf /var/lib/kubelet/pods/c527ccff-e623-48e1-90be-11930facc11b/volumes/kubernetes.io~nfs/pv-es-master</span><br></pre></td></tr></table></figure><p>多次处理，直到 <code>df -h</code> 不卡，然后重启 docker 后，nginx 能调度到这个节点上了，有个容器删不掉，最后删掉 <code>/var/lib/docker/containers/xxxxx</code> 后重启才清理掉它的 <code>docker ps -a</code> 显示。</p><h3 id="nfs"><a href="#nfs" class="headerlink" title="nfs"></a>nfs</h3><p>然后 nginx 能调度后，发现带 pvc 的 pod 无法调度到该节点上，等待后 describe 显示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl   -n content-dev  get pod content-754c9964bc-8dbxw -o wide</span><br><span class="line">NAME                       READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">content-754c9964bc-8dbxw   0/1     ContainerCreating   0          63s   &lt;none&gt;   work02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">$ kubectl   -n content-dev  describe  pod content-754c9964bc-8dbxw</span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason       Age   From     Message</span><br><span class="line">  ----     ------       ----  ----     -------</span><br><span class="line">  Warning  FailedMount  8s    kubelet  Unable to attach or mount volumes: unmounted volumes=[nfs], unattached volumes=[nfs default-token-vdjpz]: timed out waiting for the condition</span><br></pre></td></tr></table></figure><p>查看 pod 使用的 pvc 信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deploy content -n content-dev -o yaml</span><br><span class="line">...</span><br><span class="line">      volumes:</span><br><span class="line">      - name: nfs</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: datanfs-pvc</span><br><span class="line">$ kubectl   -n content-dev  get pvc </span><br><span class="line">NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">datanfs-pvc   Bound    pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf   10Gi       RWO            nfs-client     39d</span><br></pre></td></tr></table></figure><p>查看这个 pvc 的 pv 属性：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl   -n content-dev  get pv pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE</span><br><span class="line">pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf   10Gi       RWO            Delete           Bound    content-dev/datanfs-pvc   nfs-client              39d</span><br><span class="line">$ kubectl   -n content-dev  get pv pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    pv.kubernetes.io/provisioned-by: cluster.local/nfs-client-nfs-client-provisioner</span><br><span class="line">  creationTimestamp: &quot;2023-07-12T05:03:19Z&quot;</span><br><span class="line">  finalizers:</span><br><span class="line">  - kubernetes.io/pv-protection</span><br><span class="line">  name: pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br><span class="line">  resourceVersion: &quot;269296403&quot;</span><br><span class="line">  selfLink: /api/v1/persistentvolumes/pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br><span class="line">  uid: add520cb-6506-47b0-b21a-24b1c92e9d56</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  claimRef:</span><br><span class="line">    apiVersion: v1</span><br><span class="line">    kind: PersistentVolumeClaim</span><br><span class="line">    name: datanfs-pvc</span><br><span class="line">    namespace: content-dev</span><br><span class="line">    resourceVersion: &quot;253312376&quot;</span><br><span class="line">    uid: 9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br><span class="line">  nfs:</span><br><span class="line">    path: /volume3/cloudxxx-pre/content-dev-datanfs-pvc-pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br><span class="line">    server: xxx.xx.xx.50</span><br><span class="line">  persistentVolumeReclaimPolicy: Delete</span><br><span class="line">  storageClassName: nfs-client</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">status:</span><br><span class="line">  phase: Bound</span><br></pre></td></tr></table></figure><p>使用的 nfs ， 在 work02 上使用 <code>showmount</code> 查看下本机是否有挂载权限：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ showmount -e xxx.xx.xx.50</span><br><span class="line">Export list for xxx.xx.xx.50:</span><br><span class="line">/volume1/secondary          *</span><br><span class="line">/volume1/primary            *</span><br><span class="line">/volume3/devnfs             *</span><br><span class="line">/volume4/xxxxxxxx-demo      *</span><br><span class="line">/volume4/xxx2               *</span><br><span class="line">/volume4/xxx                *</span><br><span class="line">/volume4/xxxxxxxx           *</span><br><span class="line">/volume4/xxxxxxxx-xxx       *</span><br><span class="line">/volume4/xxxxxxxx-xxxsoft   *</span><br><span class="line">/volume4/k8s-nfs            *</span><br><span class="line">/volume3/cicd               *</span><br><span class="line">/volume3/xxxxxxxx-pre       *</span><br><span class="line">/volume2/web                xxx.xxx.xxx.235,xxx.xxx.xxx.211,xxx.xxx.xxx.51,xxx.xxx.xxx.50,xxx.xxx.xxx.55,xxx.xxx.xxx.238</span><br><span class="line">/volume3/VSPHERE-NFS-LUN1   xxx.xx.xx.167,xxx.xx.xx.166,xxx.xx.xx.165,xxx.xx.xx.164,xxx.xx.xx.163,xxx.xx.xx.162,xxx.xx.xx.161,xxx.xx.xx.160</span><br><span class="line">/volume1/文件共享目录 xxx.xx.xx.11</span><br></pre></td></tr></table></figure><p>有权限，说明 nfs server 的 <code>/etc/exports</code> 配置没问题，然后看下 kubelet 的挂载进程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep pvc-9865b525 </span><br><span class="line">root     163806  0.0  0.0 123632  1056 ?        S    8月18   0:00 /usr/bin/mount -t nfs xxx.xx.xx.50:/volume3/cloudxxx-pre/content-dev-datanfs-pvc-pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf /var/lib/kubelet/pods/cd743cb6-839c-4a15-9203-5321a0ed0666/volumes/kubernetes.io~nfs/pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br></pre></td></tr></table></figure><p>手动尝试挂载，发现卡住</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir test1111</span><br><span class="line">$ mount.nfs xxx.xx.xx.50:/volume3/cloudxxx-pre/ test1111</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>查看下是否有 nfs 内核模块：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ lsmod |grep nfs</span><br><span class="line">nfsv3                  43720  0 </span><br><span class="line">nfsd                  351321  13 </span><br><span class="line">nfs_acl                12837  2 nfsd,nfsv3</span><br><span class="line">auth_rpcgss            59415  2 nfsd,rpcsec_gss_krb5</span><br><span class="line">nfsv4                 584056  3 </span><br><span class="line">dns_resolver           13140  1 nfsv4</span><br><span class="line">nfs                   262045  4 nfsv3,nfsv4</span><br><span class="line">lockd                  98048  3 nfs,nfsd,nfsv3</span><br><span class="line">grace                  13515  2 nfsd,lockd</span><br><span class="line">fscache                64980  2 nfs,nfsv4</span><br><span class="line">sunrpc                358543  33 nfs,nfsd,rpcsec_gss_krb5,auth_rpcgss,lockd,nfsv3,nfsv4,nfs_acl</span><br></pre></td></tr></table></figure><p>确实有，之前接触过 nas ，发现不同内核对于支持的版本不一样，尝试看看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ mount.nfs -o vers=3 xxx.xx.xx.50:/volume3/cloudxxx-pre/ test1111</span><br><span class="line">$ umount test1111</span><br><span class="line">$ mount.nfs -o vers=4 xxx.xx.xx.50:/volume3/cloudxxx-pre/ test1111</span><br><span class="line">^C</span><br><span class="line">$ mount.nfs -o vers=4.0 xxx.xx.xx.50:/volume3/cloudxxx-pre/ test1111</span><br><span class="line">$ umount test1111</span><br><span class="line">$ mount.nfs -o vers=4.1 xxx.xx.xx.50:/volume3/cloudxxx-pre/ test1111</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>从上面看，默认挂载 nfs 时用的是最新 version，目前本机上只有 <code>3</code>、<code>4.0</code> 可以用，需要把挂载版本加到 pv 上。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl   -n content-dev  edit pv pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br><span class="line">  mountOptions:</span><br><span class="line">  - nfsvers=4.0</span><br></pre></td></tr></table></figure><p>然后 pod 就能创建了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl   -n content-dev  describe  pod content-754c9964bc-8dbxw</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason       Age                  From     Message</span><br><span class="line">  ----     ------       ----                 ----     -------</span><br><span class="line">  Warning  FailedMount  4m18s (x4 over 11m)  kubelet  Unable to attach or mount volumes: unmounted volumes=[nfs], unattached volumes=[nfs default-token-vdjpz]: timed out waiting for the condition</span><br><span class="line">  Warning  FailedMount  2m                   kubelet  Unable to attach or mount volumes: unmounted volumes=[nfs], unattached volumes=[default-token-vdjpz nfs]: timed out waiting for the condition</span><br><span class="line">  Warning  FailedMount  11s                  kubelet  MountVolume.SetUp failed for volume &quot;pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf&quot; : mount failed: signal: terminated</span><br><span class="line">Mounting command: systemd-run</span><br><span class="line">Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/c61fd1a0-cb8f-4cc9-84c9-122a7d24cde6/volumes/kubernetes.io~nfs/pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf --scope -- mount -t nfs xxx.xx.xx.50:/volume3/cloudxxx-pre/content-dev-datanfs-pvc-pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf /var/lib/kubelet/pods/c61fd1a0-cb8f-4cc9-84c9-122a7d24cde6/volumes/kubernetes.io~nfs/pvc-9865b525-2cb5-4a2a-b7a7-036ca9f524cf</span><br><span class="line">Output: Running scope as unit run-164051.scope.</span><br><span class="line">  Normal  Pulled   10s  kubelet  Container image &quot;xxx.xx.xx.215/content/content:dev-247&quot; already present on machine</span><br><span class="line">  Normal  Created  10s  kubelet  Created container spring-boot</span><br><span class="line">  Normal  Started  10s  kubelet  Started container spring-boot</span><br></pre></td></tr></table></figure><p>然后处理掉其他的已有的 pv pvc 都加上 <code>mountOptions</code> 。还需要清理掉每个节点上卡住的 mount 进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep -P &#x27;mount.+nf[s]&#x27;</span><br></pre></td></tr></table></figure><p>查找到 pid 后 <code>kill -9</code> 清理下</p><h3 id="处理后续的-pv"><a href="#处理后续的-pv" class="headerlink" title="处理后续的 pv"></a>处理后续的 pv</h3><p>发现使用了 nfs-provisioner，所有的 pvc 都是从 sc 创建出来的，对于已经创建的前面手动处理了，避免后续创建出来的没带 <code>mountOptions</code>，我们需要修改 sc 也加上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get sc nfs-client</span><br><span class="line">NAME         PROVISIONER                                       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">nfs-client   cluster.local/nfs-client-nfs-client-provisioner   Delete          Immediate           true                   499d</span><br><span class="line"></span><br><span class="line">$ kubectl edit sc nfs-client</span><br><span class="line">mountOptions:</span><br><span class="line">- nfsvers=4.0</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;群友付费找我解决 pod 无法创建的过程，写出来给别人参考&lt;/p&gt;</summary>
    
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/tags/kubernetes/"/>
    
    <category term="nfs" scheme="http://zhangguanzhang.github.io/tags/nfs/"/>
    
  </entry>
  
  <entry>
    <title>qemu-user-static 下 cgo go build 卡住的一次解决</title>
    <link href="http://zhangguanzhang.github.io/2023/08/08/qemu-cgo-build-hang/"/>
    <id>http://zhangguanzhang.github.io/2023/08/08/qemu-cgo-build-hang/</id>
    <published>2023-08-08T18:07:30.000Z</published>
    <updated>2023-08-08T18:07:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录 qemu-user-static 下 cgo go build 卡住的一次解决</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>内部构建机器都是 amd64 架构，项目大多数都没用 cgo，用了的几个都是 sqlite 这种长期稳定的，编译 arm64 的是下面的步骤：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM xxx/golang:xxx 带 gcc-aarch64-linux-gnu 的 amd64 golang 镜像</span><br><span class="line"></span><br><span class="line">COPY ...</span><br><span class="line">RUN CC=aarch64-linux-gnu-gcc GOARCH=arm64 CGO_ENABLED=1 \</span><br><span class="line">      go build xxxx</span><br></pre></td></tr></table></figure><p>前不久有个 odbc 的 cgo 项目，这样报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/gcc-cross/aarch64-linux-gnu/7/../../../../aarch64-linux-gnu/bin/ld.gold: error: cannot find -lodbc</span><br></pre></td></tr></table></figure><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>上面报错是因为 amd64 的镜像里安装 unixodbc-dev 的是 amd64 架构的，sqlite 没问题应该是它单元测试覆盖率很高了，而且安卓是 arm64 ，头文件里很多差异性估计用 <code>#define</code> 做 if else 了所以没问题。</p><h3 id="qemu-go-build-hangs"><a href="#qemu-go-build-hangs" class="headerlink" title="qemu go build hangs"></a>qemu go build hangs</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM --platform=linux/arm64 xxx/golang:xxx</span><br><span class="line">RUN apt install -y unixodbc-dev</span><br><span class="line">COPY ...</span><br><span class="line">RUN GOARCH=arm64 CGO_ENABLED=1 \</span><br><span class="line">      go build xxxx</span><br></pre></td></tr></table></figure><p>arm64 机器上编译是没问题的，但是我们构建机器都是 amd64 架构（centos 7，内核 419 版本）使用的 <a href="https://zhangguanzhang.github.io/2023/03/07/qemu-binfmt_misc/">利用 qemu user 模式和 binfmt_misc 构建其他架构的 docker 镜像</a> ，时不时卡在 go build 上</p><p>升级了 qemu 版本和 docker 版本，还是卡住，后面打算升级内核。</p><h3 id="安装-arm64-lib"><a href="#安装-arm64-lib" class="headerlink" title="安装 arm64 lib"></a>安装 arm64 lib</h3><p>但是升级内核之前想了下，能不能在 amd64 里安装 arm64 的 odbc lib，然后传递 <code>CGO_CFLAGS</code>、<code>CGO_LDFLAGS</code> 指定 arm64 的 odbc path，最终搞出来了：</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> --platform=linux/arm64 debian:buster-slim as deb</span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">set</span> -eux; \</span></span><br><span class="line"><span class="language-bash">    <span class="keyword">if</span> [  -e /etc/apt/sources.list ];<span class="keyword">then</span> sed -ri <span class="string">&#x27;s/[a-zA-Z0-9.]+(debian.org|ubuntu.com)/mirrors.aliyun.com/g&#x27;</span> /etc/apt/sources.list; <span class="keyword">fi</span>; \</span></span><br><span class="line"><span class="language-bash">    apt update; \</span></span><br><span class="line"><span class="language-bash">    <span class="built_in">cd</span> /tmp; \</span></span><br><span class="line"><span class="language-bash">    apt download libodbc1 unixodbc-dev; \</span></span><br><span class="line"><span class="language-bash">    <span class="built_in">mkdir</span> -p /test/usr/local/lib_arm64 /test/usr/local/include_arm64; \</span></span><br><span class="line"><span class="language-bash">    <span class="built_in">ls</span> *.deb | <span class="keyword">while</span> <span class="built_in">read</span> deb;<span class="keyword">do</span> dpkg-deb -x <span class="variable">$deb</span> /opt/; <span class="keyword">done</span>; \</span></span><br><span class="line"><span class="language-bash">    find /opt; \</span></span><br><span class="line"><span class="language-bash">    find /opt/usr/lib/ -not -<span class="built_in">type</span> d -<span class="built_in">exec</span> <span class="built_in">cp</span> -a &#123;&#125; /test/usr/local/lib_arm64 \; ; \</span></span><br><span class="line"><span class="language-bash">    find /opt/usr/include/ -not -<span class="built_in">type</span> d -<span class="built_in">exec</span> <span class="built_in">cp</span> -a &#123;&#125; /test/usr/local/include_arm64 \; ; \</span></span><br><span class="line"><span class="language-bash">    <span class="built_in">ls</span> -l /test/usr/local/lib_arm64 /test/usr/local/include_arm64</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> scratch</span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> --from=deb /test /odbc</span></span><br></pre></td></tr></table></figure><p>上面镜像构建成 <code>xxxxx/file/odbc:v1</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">FROM xxxxx/file/odbc:v1 as odbc</span><br><span class="line">FROM xxx/golang:xxx # apt install -y gcc-aarch64-linux-gnu 的 amd64 golang 镜像</span><br><span class="line">COPY --from=odbc /odbc /</span><br><span class="line"></span><br><span class="line">RUN CC=aarch64-linux-gnu-gcc GOARCH=arm64 CGO_ENABLED=1 \</span><br><span class="line">        CGO_CFLAGS=&#x27;-g -O2 -I/usr/local/include_arm64&#x27; \</span><br><span class="line">      CGO_LDFLAGS=&#x27;-g -O2 -L/usr/local/lib_arm64 -lodbc&#x27; \</span><br><span class="line">      go build xxx</span><br></pre></td></tr></table></figure><p><code>-g -O2</code> 是默认的值，主要是后面的参数</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录 qemu-user-static 下 cgo go build 卡住的一次解决&lt;/p&gt;</summary>
    
    
    
    
    <category term="qemu" scheme="http://zhangguanzhang.github.io/tags/qemu/"/>
    
    <category term="cgo" scheme="http://zhangguanzhang.github.io/tags/cgo/"/>
    
  </entry>
  
  <entry>
    <title>为什么ipvs下externalIPs乱设置会崩</title>
    <link href="http://zhangguanzhang.github.io/2023/07/13/k8s-externalIPs/"/>
    <id>http://zhangguanzhang.github.io/2023/07/13/k8s-externalIPs/</id>
    <published>2023-07-13T18:17:30.000Z</published>
    <updated>2023-07-13T18:17:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>总有小白不懂跟着网上文章设置externalIPs后集群崩了</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>今天中午又有小白不懂在 ipvs 模式下，跟着网上文章设置 externalIPs 为 k8s 的机器 IP 后集群崩了，本文讲解下原因和自救手段。</p><h3 id="externalIPs"><a href="#externalIPs" class="headerlink" title="externalIPs"></a>externalIPs</h3><p>下面是 <a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/#external-ips">官方文档 externalIPs</a> 的描述:</p><blockquote><p>如果有外部 IP 能够路由到一个或多个集群节点上，则 Kubernetes 服务可以暴露在这些 externalIPs 上。 当网络流量到达集群时，如果外部 IP（作为目的 IP 地址）和端口都与该 Service 匹配，Kubernetes 配置的规则和路由会确保流量被路由到该 Service 的端点之一。</p></blockquote><p>如果你第一次看不懂没关系，后面看完本文就懂了这部分文字。</p><h3 id="iptables-模式"><a href="#iptables-模式" class="headerlink" title="iptables 模式"></a>iptables 模式</h3><p>先看看 iptables 的模式下原理，iptables 模式集群信息为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NAME            STATUS   ROLES         AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                  KERNEL-VERSION                    CONTAINER-RUNTIME</span><br><span class="line">10.xxx.xx.211   Ready    master,node   14d   v1.20.11   10.xxx.xx.211   &lt;none&gt;        Kylin Linux Advanced Server V10 (Sword)   4.19.90-24.4.v2101.ky10.aarch64   docker://20.10.22</span><br><span class="line">10.xxx.xx.213   Ready    master,node   14d   v1.20.11   10.xxx.xx.213   &lt;none&gt;        Kylin Linux Advanced Server V10 (Sword)   4.19.90-24.4.v2101.ky10.aarch64   docker://20.10.22</span><br><span class="line">10.xxx.xx.214   Ready    master,node   14d   v1.20.11   10.xxx.xx.214   &lt;none&gt;        Kylin Linux Advanced Server V10 (Sword)   4.19.90-24.4.v2101.ky10.aarch64   docker://20.10.22</span><br></pre></td></tr></table></figure><p>部署下下面的服务，</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: zgz</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx:stable</span><br><span class="line">    ports:</span><br><span class="line">      - containerPort: 80</span><br><span class="line">        name: http-web-svc</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-service</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: zgz</span><br><span class="line">  ports:</span><br><span class="line">  - name: name-of-service-port</span><br><span class="line">    protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: http-web-svc</span><br></pre></td></tr></table></figure><p>部署后，选个节点导出下 iptables nat 表规则，然后增加 externalIPs 设置为第一个节点的 IP 后再导出对比下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -S &gt;  before-nat-pod.txt</span><br><span class="line">$ kubectl patch svc nginx-service -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;externalIPs&quot;:[&quot;10.xxx.xx.211&quot;]&#125;&#125;&#x27;</span><br><span class="line">$ iptables -t nat -S &gt;  after-nat-pod.txt</span><br><span class="line"># 对比</span><br><span class="line">$ diff &lt;(sort before-nat-pod.txt) &lt;(sort after-nat-pod.txt)</span><br><span class="line">149a150,152</span><br><span class="line">&gt; -A KUBE-SERVICES -d 10.xxx.xx.211/32 -p tcp -m comment --comment &quot;default/nginx-service:name-of-service-port external IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">&gt; -A KUBE-SERVICES -d 10.xxx.xx.211/32 -p tcp -m comment --comment &quot;default/nginx-service:name-of-service-port external IP&quot; -m tcp --dport 80 -m addrtype --dst-type LOCAL -j KUBE-SVC-IQGXNJVVP26VHMIN</span><br><span class="line">&gt; -A KUBE-SERVICES -d 10.xxx.xx.211/32 -p tcp -m comment --comment &quot;default/nginx-service:name-of-service-port external IP&quot; -m tcp --dport 80 -m physdev ! --physdev-is-in -m addrtype ! --src-type LOCAL -j KUBE-SVC-IQGXNJVVP26VHMIN</span><br><span class="line"># 不 sort 对比就多了因为顺序乱了的重复内容，sort 对比又会导致上面的规则顺序乱了，所以直接正则匹配</span><br><span class="line">$ grep -w &#x27;name-of-service-port external IP&#x27; after-nat-pod.txt</span><br><span class="line">-A KUBE-SERVICES -d 10.xxx.xx.211/32 -p tcp -m comment --comment &quot;default/nginx-service:name-of-service-port external IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 10.xxx.xx.211/32 -p tcp -m comment --comment &quot;default/nginx-service:name-of-service-port external IP&quot; -m tcp --dport 80 -m physdev ! --physdev-is-in -m addrtype ! --src-type LOCAL -j KUBE-SVC-IQGXNJVVP26VHMIN</span><br><span class="line">-A KUBE-SERVICES -d 10.xxx.xx.211/32 -p tcp -m comment --comment &quot;default/nginx-service:name-of-service-port external IP&quot; -m tcp --dport 80 -m addrtype --dst-type LOCAL -j KUBE-SVC-IQGXNJVVP26VHMIN</span><br><span class="line"></span><br><span class="line"># KUBE-SVC-IQGXNJVVP26VHMIN 则是 svc 的 iptables 负载入口链</span><br></pre></td></tr></table></figure><p>diff 发现多了三条 iptables 规则，三条规则的前面属性都是 <code>目标 IP 和端口为 10.xxx.xx.211:80 的</code>：</p><ul><li>进入动态伪装 IP 的处理链</li><li><code>-m physdev ! --physdev-is-in</code> 意思为不是从桥接接口进来，也就是外部的物理网卡进来的，<code>-m addrtype ! --src-type LOCAL</code> 意思是来源地址不是本机上的 IP，最后是到 svc 的 DNAT 到 podIP</li><li>第三个结尾的 <code>-m addrtype --dst-type LOCAL</code> 是目标 IP 在本机，也就是针对 externalIPs 设置成机器上的 网卡 IP 的，此刻拦截发到 svc。</li></ul><p>因为前面都是针对 ip:80 的，所以 iptables 模式针对的都是纯四层，设置为集群机器 IP 这样没啥问题，问题是 ipvs 模式，也就是接下来讲的</p><h3 id="ipvs-模式"><a href="#ipvs-模式" class="headerlink" title="ipvs 模式"></a>ipvs 模式</h3><p>清理掉上面的svc：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod nginx</span><br><span class="line">kubectl delete svc nginx-service</span><br></pre></td></tr></table></figure><p>集群 kube-proxy 切换为 ipvs 模式后。可以先阅读之前的文章 <a href="https://zhangguanzhang.github.io/2021/09/28/ipvs-svc/">在非容器环境上实现散装的 IPVS SVC</a>。</p><h4 id="非-k8s-IP"><a href="#非-k8s-IP" class="headerlink" title="非 k8s IP"></a>非 k8s IP</h4><p>切换模式后，先找个不是 k8s 节点但是和 k8s 节点机器是同一个二层局域网的没使用的 IP 作为 <code>externalIPs</code>，然后部署上面的步骤。</p><p>ipvs 下，kube-proxy 会把 <code>svcIP/32</code> 配置在 <code>kube-ipvs0</code> 的 <code>dummy</code> 接口上，而 <code>externalIPs</code> 也会把它配置在  <code>kube-ipvs0</code> 上。然后我们做一个实验，假设此刻 externalIPs 设置为 <code>10.xxx.xx.250</code> 可以任何一台k8s机器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -d --net host --name test nginx:alpine</span><br><span class="line"></span><br><span class="line">$ curl -I localhost</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">$ curl -I 10.xxx.xx.250</span><br><span class="line">HTTP/1.1 200 OK</span><br></pre></td></tr></table></figure><p>你会发现也能通，这是因为 kube-proxy 把 <code>externalIPs/32</code> 配置在  <code>kube-ipvs0</code> 的 dummy 上了，因为 IP 在本机上，而且是32位掩码，又因为 nginx 进程 bind 的 <code>0.0.0.0</code> ，所以访问 <code>externalIPs:80</code> 也能到 nginx。</p><h4 id="k8s-机器-IP"><a href="#k8s-机器-IP" class="headerlink" title="k8s 机器 IP"></a>k8s 机器 IP</h4><p>你可能在想，我 nginx 设置监听指定的本机节点网卡 IP ，这样 <code>externalIPs:80</code> 不就无法访问了吗，确实，但是 <code>externalIPs:其他端口</code> 也会定向到本机上，由于没端口监听访问最终会超时。假设你把 <code>externalIPs</code> 设置为第一个master 节点：</p><ul><li>第一个 master 节点路由到本机 IP 都会到 kube-ipvs0（因为掩码32最大），也就会和外界断联</li><li>其他 master 访问 master1:6443 和 etcd 的 2379 都被定向到本机的，信息就乱了，包含其他端口都会定向到本机上，有端口监听就信息乱，没端口监听就超时</li><li>然后非第一个 master 上的 kubelet 和 kubectl 只要流量最终负载到第一个 master 上就都会超时</li></ul><p>假设已经发生：</p><ul><li>每台机器从 tty 或者虚拟化 vnc 进去后停止 kube-proxy，<code>ip link delete &lt;externalIPs&gt;/32 dev kube-ipvs0</code></li><li>如果操作后能使用 kubectl 就 edit 或者删除那个 svc</li><li>如果不能就 etcdctl 删除掉这个 svc</li><li>最后再启动停止的进程</li></ul><h4 id="ipvs-模式如何使用-externalIPs"><a href="#ipvs-模式如何使用-externalIPs" class="headerlink" title="ipvs 模式如何使用 externalIPs"></a>ipvs 模式如何使用 externalIPs</h4><p>可以设置外部没使用的 IP，然后网络设备添加路由把这个 IP 导向 k8s 的机器。或者有其他进程&#x2F;轮子宣告 externalIPs 的 arp，让外部机器访问 externalIPs 能到 k8s 机器上</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://ipset.netfilter.org/iptables-extensions.man.html">https://ipset.netfilter.org/iptables-extensions.man.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;总有小白不懂跟着网上文章设置externalIPs后集群崩了&lt;/p&gt;</summary>
    
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
    <category term="externalIP" scheme="http://zhangguanzhang.github.io/tags/externalIP/"/>
    
  </entry>
  
  <entry>
    <title>国产系统上解决docker pull卡住的问题</title>
    <link href="http://zhangguanzhang.github.io/2023/04/12/docker-pull-hang/"/>
    <id>http://zhangguanzhang.github.io/2023/04/12/docker-pull-hang/</id>
    <published>2023-04-12T16:17:30.000Z</published>
    <updated>2023-04-12T16:17:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天客户 arm64 机器上 docker pull 大镜像卡住的一次解决过程</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>同事拉我看一个客户现场 docker 镜像无法拉取的问题，故障如下会一直卡住：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull xxx:5000/xxxx</span><br><span class="line">xxx: Pulling from xxx/xxxxxx</span><br><span class="line">7c0b344a74c2: Extracting [&gt;                                                  ]  294.9kB/26.66MB</span><br><span class="line">7c0b344a74c2: Download complete</span><br><span class="line">e53ed7fd3110: Download complete</span><br><span class="line">d2cae797bc79: Download complete</span><br><span class="line">ec3ddc176f08: Download complete</span><br><span class="line">2969517e196e: Download complete</span><br><span class="line">097fa64722e8: Download complete</span><br><span class="line">1dde4ca01a5a: Download complete</span><br></pre></td></tr></table></figure><p>离线文件 load -i 后，tag 后推送到仓库上，本地删掉这个镜像，然后拉取还是上面这样卡住，部分小镜像拉取是没问题的，所以不可能是 docker data-root 的挂载 option 影响。环境信息如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$ docker info</span><br><span class="line">...</span><br><span class="line"> Server Version: 19.03.15</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: extfs</span><br><span class="line">  Supports d_type: true</span><br><span class="line">  Native Overlay Diff: true</span><br><span class="line"> Logging Driver: json-file</span><br><span class="line"> Cgroup Driver: cgroupfs</span><br><span class="line"> Plugins:</span><br><span class="line">  Volume: local</span><br><span class="line">  Network: bridge host ipvlan macvlan null overlay</span><br><span class="line">  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line"> Swarm: inactive</span><br><span class="line"> Runtimes: runc</span><br><span class="line"> Default Runtime: runc</span><br><span class="line"> Init Binary: docker-init</span><br><span class="line"> containerd version: ea765aba0d05254012b0b9e595e995c09186427f</span><br><span class="line"> runc version: v1.0.0-0-g84113eef</span><br><span class="line"> init version: fec3683</span><br><span class="line"> Security Options:</span><br><span class="line">  seccomp</span><br><span class="line">   Profile: default</span><br><span class="line"> Kernel Version: 4.19.90-2211.5.0.0178.22.uel20.aarch64</span><br><span class="line"> Operating System: UnionTech OS Server 20</span><br><span class="line"> OSType: linux</span><br><span class="line"> Architecture: aarch64</span><br><span class="line"> CPUs: 24</span><br><span class="line"> Total Memory: 94.56GiB</span><br><span class="line"> Name: host-xxxx</span><br><span class="line"> ID: RTQS:5TXE:5T3S:YW7X:OHPK:FZ7D:7EHD:DH5Z:JNBV:FVXS:24FA:EIVS</span><br><span class="line"> Docker Root Dir: /data/kube/docker</span><br><span class="line"> Debug Mode: true</span><br><span class="line">  File Descriptors: 29</span><br><span class="line">  Goroutines: 46</span><br><span class="line">  System Time: 2023-04-12T16:10:25.33362426+08:00</span><br><span class="line">$ uname -a</span><br><span class="line">Linux host-x 4.19.90-2211.5.0.0178.22.uel20.aarch64 #1 SMP Thu Nov 24 10:33:07 CST 2022 aarch64 aarch64 aarch64 GNU/Linux</span><br><span class="line">$ cat /etc/os-release</span><br><span class="line">PRETTY_NAME=&quot;UnionTech OS Server 20&quot;</span><br><span class="line">NAME=&quot;UnionTech OS Server 20&quot;</span><br><span class="line">VERSION_ID=&quot;20&quot;</span><br><span class="line">VERSION=&quot;20&quot;</span><br><span class="line">ID=uos</span><br><span class="line">HOME_URL=&quot;https://www.chinauos.com/&quot;</span><br><span class="line">BUG_REPORT_URL=&quot;https://bbs.chinauos.com/&quot;</span><br><span class="line">VERSION_CODENAME=fuyu</span><br><span class="line">PLATFORM_ID=&quot;platform:uel20&quot;</span><br></pre></td></tr></table></figure><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><p>卡住的过程中在开一个 ssh top 看到了有进程 <code>unpigz</code> 占用比较高，利用它 pid 查看了下一些信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ pstree -sp 1170083</span><br><span class="line">systemd(1)───dockerd(1169795)───unpigz(1170083)─┬─&#123;unpigz&#125;(1170084)</span><br><span class="line">                                                ├─&#123;unpigz&#125;(1170086)</span><br><span class="line">                                                └─&#123;unpigz&#125;(1170087)</span><br></pre></td></tr></table></figure><p>发现是 docker 调用它的，strace 只能看到卡住， kill 了 unpigz 后，卡住的 pull 报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">failed to register layer: Error processing tar file(exit status 1): unexpected EOF</span><br></pre></td></tr></table></figure><p>docker 的镜像每层 layer 实际就是 tar ， pull 的时候都是下载 tar 包后解压，这个看着是解压相关出现了问题，在 docker 源码里搜了下 <code>Error processing tar file</code> 后找到</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/moby/moby/blob/v19.03.15/pkg/chrootarchive/archive_unix.go#L90-L116</span></span><br><span class="line">cmd := reexec.Command(<span class="string">&quot;docker-untar&quot;</span>, dest, root)</span><br><span class="line">  ...</span><br><span class="line"><span class="keyword">if</span> err := cmd.Wait(); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="comment">// when `xz -d -c -q | docker-untar ...` failed on docker-untar side,</span></span><br><span class="line"><span class="comment">// we need to exhaust `xz`&#x27;s output, otherwise the `xz` side will be</span></span><br><span class="line"><span class="comment">// pending on write pipe forever</span></span><br><span class="line">io.Copy(ioutil.Discard, decompressedArchive)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">&quot;Error processing tar file(%v): %s&quot;</span>, err, output)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br></pre></td></tr></table></figure><p>看注释里的 <code>xz -d -c -q | docker-untar ...</code> ，看了下 unpigz 的 cmdline 和确实有一个卡住的 docker-untar 进程</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ xargs -0 &lt; /proc/1170083/cmdline</span><br><span class="line">/usr/bin/unpigz -d -c</span><br><span class="line">$ ps aux | grep docker-unta[r]</span><br><span class="line">root     1164788  0.0  0.0 1491008 39488 pts/2   Sl+  15:21   0:00 docker-untar / /data/kube/docker/overlay2/546b7b992b53b243450807b8150c4a1905e93afae604da69a21bbaaf443f178e/diff</span><br></pre></td></tr></table></figure><p>看着是 exec 调用 unpigz 解压管道给 reexec 注册的 docker-untar，而上面的 unpigz 进程树看到是 docker 默认调用的而非 xz ，搜了下，发现 unpigz 是一个在 gz 格式处理上比 gzip 更快的实现。既然 docker 是 exec 调用的 unpigz ，那就在源码里搜索下它看看：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/moby/moby/blob/v19.03.15/pkg/archive/archive.go#L32-L39</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> path, err := exec.LookPath(<span class="string">&quot;unpigz&quot;</span>); err != <span class="literal">nil</span> &#123;</span><br><span class="line">logrus.Debug(<span class="string">&quot;unpigz binary not found in PATH, falling back to go gzip library&quot;</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">logrus.Debugf(<span class="string">&quot;Using unpigz binary found at path %s&quot;</span>, path)</span><br><span class="line">unpigzPath = path</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>往下翻看，发现 <code>unpigzPath</code> 的 exec 调用地方：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/moby/moby/blob/v19.03.15/pkg/archive/archive.go#L160-L174</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">gzDecompress</span><span class="params">(ctx context.Context, buf io.Reader)</span></span> (io.ReadCloser, <span class="type">error</span>) &#123;</span><br><span class="line"><span class="keyword">if</span> unpigzPath == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line"><span class="keyword">return</span> gzip.NewReader(buf)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">disablePigzEnv := os.Getenv(<span class="string">&quot;MOBY_DISABLE_PIGZ&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> disablePigzEnv != <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line"><span class="keyword">if</span> disablePigz, err := strconv.ParseBool(disablePigzEnv); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> disablePigz &#123;</span><br><span class="line"><span class="keyword">return</span> gzip.NewReader(buf)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> cmdStream(exec.CommandContext(ctx, unpigzPath, <span class="string">&quot;-d&quot;</span>, <span class="string">&quot;-c&quot;</span>), buf)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现场 unpigz 版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ rpm -qf /bin/unpigz</span><br><span class="line">pigz-2.4-7.uel20.01.aarch64</span><br><span class="line">$ rpm -V pigz</span><br><span class="line"># -V 查看包也没被修改</span><br></pre></td></tr></table></figure><p>注意看其中有个 env 设置不使用 PIGZ 而是使用 gzip ，然后启动 docker daemon 的时候设置这个 env 就可以拉取镜像了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop docker</span><br><span class="line"># 临时命令行前台 debug 启动下看看是没问题的</span><br><span class="line">$ MOBY_DISABLE_PIGZ=true dockerd --debug</span><br></pre></td></tr></table></figure><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>uos 这个系统是要授权才能使用 yum 安装升级，去 repo 里的 url 访问报错 401，只有让客户联系 uos 厂商升级 pigz 包先，不能解决再使用 <code>MOBY_DISABLE_PIGZ</code></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天客户 arm64 机器上 docker pull 大镜像卡住的一次解决过程&lt;/p&gt;</summary>
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>利用 qemu user 模式和 binfmt_misc 构建其他架构的 docker 镜像</title>
    <link href="http://zhangguanzhang.github.io/2023/03/07/qemu-binfmt_misc/"/>
    <id>http://zhangguanzhang.github.io/2023/03/07/qemu-binfmt_misc/</id>
    <published>2023-03-07T10:40:30.000Z</published>
    <updated>2023-03-07T10:40:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>docker 的 buildx 需要内核支持，不升级内核的情况下，实际上可以利用 qemu 和 binfmt_misc 在 x86_64 上构建其他架构容器，之前这块也是没大概看组合原理，这次龙芯 loongarch64 适配的时候正好理解</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>借着这次在 x86_64 上构建龙芯的 docker 镜像理解了下 qemu 和 binfmt_misc 的组合大概，相信不少人使用过下面的，运行一个 qemu-user-static 的容器后，就可以在 x86_64 机器上执行其他架构容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ uname -m</span><br><span class="line">x86_64</span><br><span class="line"></span><br><span class="line">$ docker run --rm -t arm64v8/ubuntu uname -m</span><br><span class="line">standard_init_linux.go:211: exec user process caused &quot;exec format error&quot;</span><br><span class="line"></span><br><span class="line">$ docker run --rm --privileged multiarch/qemu-user-static --reset -p yes</span><br><span class="line"></span><br><span class="line">$ docker run --rm -t arm64v8/ubuntu uname -m</span><br><span class="line">aarch64</span><br></pre></td></tr></table></figure><h2 id="原理讲解"><a href="#原理讲解" class="headerlink" title="原理讲解"></a>原理讲解</h2><p>这次文章就是介绍运行这个特权容器到底干了啥和用了啥科技</p><h3 id="qemu"><a href="#qemu" class="headerlink" title="qemu"></a>qemu</h3><p>qemu 是虚拟化技术，可以完全模拟一个虚拟机，如果你安装 RHEL 的 gui 系统或者使用过 proxmox，能看到默认就带 qemu 和 kvm，kvm 是内核模块工作在内核态，它大部分承担硬件翻译执行能力， qemu 和 kvm 是互相弥补不足的，组合起来模拟性能更强和模拟的方面更全面。</p><p>qemu 分为两种模式：</p><ul><li>模拟&#x2F;系统模式（System Mode)：模拟整个计算机系统，包括中央处理器及其他周边设备，它使能为跨平台编写的程序进行测试及排错工作变得容易。其亦能用来在一部主机上虚拟数个不同的虚拟计算机，类似我们平常使用的Vmare、VirtualBox等。运行的二进制是 <code>qemu-system-$arch</code></li><li>用户模式（User Mode)：在 os 上直接启动运行非本机器架构的 Linux 程序，因为 qemu-user 有内置了系统调用翻译，运行的二进制是 <code>qemu-$arch</code> 、<code>qemu-$arch-static</code></li></ul><h4 id="qemu-user-模式"><a href="#qemu-user-模式" class="headerlink" title="qemu user 模式"></a>qemu user 模式</h4><p>看上面使用到的镜像名字里有 <code>qemu-user</code> 字样，说明利用到的是 qemu 的用户模式，下面是一个示例：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ cat arch.<span class="keyword">go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;fmt&quot;</span></span><br><span class="line"><span class="string">&quot;runtime&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">fmt.Println(<span class="string">&quot;Runtime:&quot;</span>, runtime.GOOS)</span><br><span class="line">fmt.Println(<span class="string">&quot;CPU Architecture:&quot;</span>, runtime.GOARCH)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译上面两个文件后，可以看到是不同架构的二进制文件，当然你也可以其他语言，例如 c 语言写个类似的，然后交叉编译工具编译出来：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">CGO_ENABLED=0 go build -o arch_amd64 arch.go</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">CGO_ENABLED=0 GOARCH=arm64 go build -o arch_arm64 arch.go</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">file arch_*</span></span><br><span class="line">arch_arm64:   ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, Go BuildID=yWXdW8xWC_151uR99u6q/2Dejwde0vtiPXdDCC-kL/4vOP0RCfaeaWtMVrus3U/_3s13dNK6GQJhzaszUVM, with debug_info, not stripped</span><br><span class="line">arch_amd64:   ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, Go BuildID=lmGyI5HbkHtM1qoM4Tzh/dX3opcS3RVhsbDro_SJ6/ZjSvKC84nJJnLhTMfSRC/yRfK-axzCadYFm4tDep5, with debug_info, not stripped</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./arch_amd64</span> </span><br><span class="line">Runtime: linux</span><br><span class="line">CPU Architecture: amd64</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./arch_arm64</span></span><br><span class="line">-bash: ./arch_arm64: cannot execute binary file</span><br></pre></td></tr></table></figure><p>直接执行是执行不了其他架构的，报错也可能是 exec format error 之类的，下面就是使用 qemu-user 模式运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 你也可以用包管理安装，有些系统的自带的源里没有静态编译的 qemu-$arch</span><br><span class="line"># 所以这里我直接下载别人编译好的静态二进制</span><br><span class="line">$ wget https://github.com/multiarch/qemu-user-static/releases/download/v7.2.0-1/qemu-aarch64-static</span><br><span class="line">$ chmod a+x qemu-aarch64-static</span><br><span class="line">$ ./qemu-aarch64-static arch_arm64</span><br><span class="line">Runtime: linux</span><br><span class="line">CPU Architecture: arm64</span><br><span class="line">$ ./arch_arm64</span><br><span class="line">-bash: ./arch_arm64: cannot execute binary file</span><br></pre></td></tr></table></figure><p>如果需要运行的程序不是静态链接的，需要宿主机行支持，或者 chroot 进去后，把 qemu-user-static 拷贝进去执行。</p><h3 id="Linux-的-binfmt-misc"><a href="#Linux-的-binfmt-misc" class="headerlink" title="Linux 的 binfmt_misc"></a>Linux 的 binfmt_misc</h3><p>windows 上可以设置不同的后缀文件使用不同软件打开，在 Linux 上，也有类似的功能。Linux的内核从很早开始就引入了一个叫做 Miscellaneous Binary Format（binfmt_misc）的机制，可以通过要打开文件的特性来选择到底使用哪个程序来打开。比 Windows 更加强大的地方是，它不光可以通过文件的扩展名来判断的，还可以通过文件开始位置的特殊的字节（ELF Magic Byte）来判断。</p><h4 id="binfmt-misc-开启"><a href="#binfmt-misc-开启" class="headerlink" title="binfmt_misc 开启"></a>binfmt_misc 开启</h4><p>使用下面命令启用这个功能：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 内核编译的选项 Executable file formats / Emulations  ---&gt; &lt;M&gt; Kernel support for MISC binaries</span><br><span class="line">modprobe binfmt_misc</span><br><span class="line"># 也是 /procfs 注册开启</span><br><span class="line">mount binfmt_misc /proc/sys/fs/binfmt_misc</span><br></pre></td></tr></table></figure><h4 id="binfmt-misc-的注册格式"><a href="#binfmt-misc-的注册格式" class="headerlink" title="binfmt_misc 的注册格式"></a>binfmt_misc 的注册格式</h4><p>然后就可以在 <code>/proc/sys/fs/binfmt_misc</code> 里面看到两个文件</p><ul><li><code>register</code>：该文件只能写入，不可读取，写入注册格式就能注册</li><li><code>status</code>：读取它可以看到当前 binfmt_misc 是否启用</li></ul><p>注册格式很简单：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">:name:type:offset:magic:mask:interpreter:flags</span><br><span class="line"># 字段冒号分隔，某些字段有默认值，默认值情况下也要保留对应位置的冒号分隔符</span><br></pre></td></tr></table></figure><ul><li>name：名字，用来标识这条记录的，理论上可以取任何名字，只要不重名就可以了</li><li>type：<ul><li>M 表示目标文件的内容 magic 来识别的，</li><li>E 则是认文件后缀</li></ul></li><li>offset 在 type 为 M 的时候有用，指定识别的时候的偏移位置，默认是 0 。</li><li>magic 即用来识别的具体 magic 内容</li><li>mask 在 type 为 M 的时候用，默认值全是 0xFF 的 bitmask，二进制下某一位为1，则表示文件的 ELF magic 必须和 magic对应的位匹配。magic 长度一般是 40，具体可以去看 <a href="https://github.com/qemu/qemu/blob/master/scripts/qemu-binfmt-conf.sh">qemu-binfmt-conf.sh</a> 脚本，获取二进制文件的 magic 可以使用例如下面的 shell：<ul><li><code>xxd hello_arm64   | awk &#39;&#123;for(i=2;i&lt;NF;i++)&#123;a=a$i;c++;if(c&gt;9)&#123;print a;exit;&#125;&#125;&#125;&#39;</code> 该示例是 40 个</li></ul></li><li>interpreter 具体用来执行的解释器，必须用绝对路径。不能超过127个字符。</li><li>flags 可选的，用来控制 interpreter 打开文件的行为:<ul><li>P 用于保存用户于命令行中输入的原程序名（通过将程序名添加到argv）；interpreter 必须知悉到此标记才能正确将此额外函数作为其argv[0]传递至解释程序。</li><li>O 用于打开程序文件并将其文档描述符传递至interpreter以读取用户无法读取的文件（对于无读取权限的用户而言）。</li><li>C 用于根据程序文件而非 interpreter 文件决定新进程凭证（参见setuid）；此值默认为O。</li><li>F 最常用的，白话讲就是配置的时候会把 interpreter 文件导入到内存里，后续用户空间和 chroot 里都没这个文件也可以执行。</li></ul></li><li>一些注意事项<ul><li>offset + size(magic) 一定要少于 128 位</li><li>后添加的会先被匹配</li></ul></li></ul><p>取消注册</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 全部</span><br><span class="line">echo -1 &gt; /proc/sys/fs/binfmt_misc/status</span><br><span class="line"># 单个</span><br><span class="line">echo -1 &gt; /proc/sys/fs/binfmt_misc/xxx</span><br><span class="line"># 禁用与开启</span><br><span class="line">echo 0 &gt; /proc/sys/fs/binfmt_misc/status</span><br><span class="line">echo 1 &gt; /proc/sys/fs/binfmt_misc/status</span><br></pre></td></tr></table></figure><h3 id="multiarch-x2F-qemu-user-static-做了啥"><a href="#multiarch-x2F-qemu-user-static-做了啥" class="headerlink" title="multiarch&#x2F;qemu-user-static 做了啥"></a>multiarch&#x2F;qemu-user-static 做了啥</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --privileged multiarch/qemu-user-static --reset -p yes</span><br></pre></td></tr></table></figure><p>实际上运行上面的特权容器，等同于:</p><ol><li>特权容器下，容器内的 &#x2F;proc 和 mount 是和宿主机一致的</li><li>入口 <a href="https://github.com/multiarch/qemu-user-static/blob/master/containers/latest/register.sh">register.sh 脚本</a> 挂载 binfmt_misc </li><li>shift 掉 <code>--reset</code> 后，剩下参数传递执行 qemu 的 <a href="https://github.com/qemu/qemu/blob/master/scripts/qemu-binfmt-conf.sh">qemu-binfmt-conf.sh</a> 脚本，注册各种架构二进制的打开方式为对应的 <code>qemu-$arch-static</code></li><li>因为 <code>multiarch/qemu-user-static</code> 镜像里有 <a href="https://github.com/multiarch/qemu-user-static/blob/master/containers/latest/Dockerfile">COPY qemu-*-static &#x2F;usr&#x2F;bin&#x2F;</a>，然后把 <code>-p yes</code> 传递给最终在脚本里，也就是注册的时候会开 flags 的 <code>F</code> ，会把这些 <code>/usr/bin/qemu-*-static</code> 导入到内存里。这个需要内核支持，<a href="https://github.com/multiarch/qemu-user-static/issues/100">centos 3.10内核就不支持</a></li><li>然后在 x86_64 上执行其他架构的镜像，会被 binfmt_misc 识别，调用内存里的 <code>/usr/bin/qemu-*-static</code> 翻译执行</li></ol><p>还有个 <code>multiarch/qemu-user-static:register</code> 的镜像，运行是:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --privileged multiarch/qemu-user-static:register --reset</span><br></pre></td></tr></table></figure><p>因为这个镜像里没 <code>/usr/bin/qemu-$arch-static</code> ，所以也不会用 <code>-p yes</code> ，这种使用方式就需要你挂载 qemu 到 &#x2F;usr&#x2F;bin&#x2F; 里:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -t -v $PWD/qemu-aarch64-static:/usr/bin/qemu-aarch64-static arm64v8/ubuntu uname -m</span><br><span class="line">aarch64</span><br></pre></td></tr></table></figure><p>或者制作 docker 镜像的时候，内部 <code>/usr/bin/qemu-$arch-static</code> 存在。例如我们业务的 <code>Dockerfile_arm64</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 提前所有 jenkins 上执行 docker run --rm --privileged multiarch/qemu-user-static:register --reset</span><br><span class="line">FROM multiarch/qemu-user-static:x86_64-aarch64 as qemu</span><br><span class="line">FROM arm64v8/ubuntu</span><br><span class="line">COPY --from=qemu /usr/bin/qemu-aarch64-static /usr/bin/</span><br><span class="line">RUN xxxx</span><br></pre></td></tr></table></figure><p>或者 第一阶段是 golang 交叉编译，第二阶段是最终架构：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">FROM go:xxx as build</span><br><span class="line">RUN GOARCH=arm64 go build -o /xxx cmd/main.go</span><br><span class="line"></span><br><span class="line">FROM multiarch/qemu-user-static:x86_64-aarch64 as qemu</span><br><span class="line">FROM arm64v8/ubuntu</span><br><span class="line">COPY --from=build /xxx /xxx</span><br><span class="line">COPY --from=qemu /usr/bin/qemu-aarch64-static /usr/bin/</span><br><span class="line">RUN apt-get ...</span><br></pre></td></tr></table></figure><p>当然，也不是只有 docker，因为是内核拦截的 execute 的 syscall ，所以此刻宿主机上也可以执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ./arch_arm64</span><br><span class="line">Runtime: linux</span><br><span class="line">CPU Architecture: arm64</span><br></pre></td></tr></table></figure><h3 id="一些注意点"><a href="#一些注意点" class="headerlink" title="一些注意点"></a>一些注意点</h3><ul><li><a href="https://github.com/multiarch/qemu-user-static/blob/master/containers/latest/register.sh#L3">register.sh 脚本</a> 里默认是把 <code>QEMU_BIN_DIR</code> 设置为 <code>/usr/bin/</code> ，直接使用 qemu 的脚本默认则是 <code>/usr/local/bin/</code></li><li><a href="https://github.com/multiarch/qemu-user-static/blob/master/containers/latest/register.sh">register.sh 脚本</a> 里默认设置了 <code>--qemu-suffix &quot;-static&quot;</code>，意味着最终的 interpreter 会带有 <code>-static</code> 后缀</li><li>multiarch&#x2F;qemu-user-static 镜像不一定更新及时，可能需要自己替换里面的一些文件，例如我这几天搞的 loongarch64 ，龙芯官方提供的 <code>qemu-loongarch64</code> + 最新的 qemu-binfmt-conf.sh 才行</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM multiarch/qemu-user-static</span><br><span class="line"># 因为设置了 --qemu-suffix &quot;-static&quot; 所以拷贝进去名字要对应</span><br><span class="line">COPY qemu-loongarch64 /usr/bin/qemu-loongarch64-static</span><br><span class="line">ADD https://raw.githubusercontent.com/qemu/qemu/master/scripts/qemu-binfmt-conf.sh /qemu-binfmt-conf.sh</span><br><span class="line">RUN chmod a+x /qemu-binfmt-conf.sh</span><br></pre></td></tr></table></figure><p>构建后测试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t multiarch/qemu-user-static-2 .</span><br><span class="line">$ docker run --rm --privileged multiarch/qemu-user-static-2  --reset -p yes</span><br><span class="line">$ docker run --rm -ti cr.loongnix.cn/library/alpine:3.11.11 uname -m</span><br><span class="line">WARNING: The requested image&#x27;s platform (linux/loong64) does not match the detected host platform (linux/amd64) and no specific platform was requested</span><br><span class="line">loongarch64</span><br><span class="line">$ uname -m</span><br><span class="line">x86_64</span><br><span class="line">$ cat /proc/sys/fs/binfmt_misc/qemu-loongarch64 </span><br><span class="line">enabled</span><br><span class="line">interpreter /usr/bin/qemu-loongarch64-static</span><br><span class="line">flags: F</span><br><span class="line">offset 0</span><br><span class="line">magic 7f454c4602010100000000000000000002000201</span><br><span class="line">mask fffffffffffffffc00fffffffffffffffeffffff</span><br></pre></td></tr></table></figure><p>之前使用 multiarch&#x2F;qemu-user-static 的 <code>qemu-loongarch64-static</code> 会报错 <code>Function not implemented</code>，龙芯官方给我的才可以正常使用，这块后续得等龙芯他们合并到 qemu 去了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -ti cr.loongnix.cn/library/alpine:3.11.11 ls</span><br><span class="line">WARNING: The requested image&#x27;s platform (linux/loong64) does not match the detected host platform (linux/amd64) and no specific platform was requested</span><br><span class="line">ls: .: Function not implemented</span><br></pre></td></tr></table></figure><p><a href="https://github.com/loongson/build-tools/releases/download/2022.09.06/qemu-loongarch64">龙芯的 qemu 下载</a></p><h4 id="env"><a href="#env" class="headerlink" title="env"></a>env</h4><p>发现 dockerfile 里这样会影响 qemu 执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ARG QEMU_VERSION</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.qemu.org/docs/master/user/main.html">https://www.qemu.org/docs/master/user/main.html</a></li><li><a href="https://zh.wikipedia.org/wiki/Binfmt_misc">https://zh.wikipedia.org/wiki/Binfmt_misc</a></li><li><a href="https://www.kernel.org/doc/html/latest/admin-guide/binfmt-misc.html">https://www.kernel.org/doc/html/latest/admin-guide/binfmt-misc.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;docker 的 buildx 需要内核支持，不升级内核的情况下，实际上可以利用 qemu 和 binfmt_misc 在 x86_64 上构建其他架构容器，之前这块也是没大概看组合原理，这次龙芯 loongarch64 适配的时候正好理解&lt;/p&gt;</summary>
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="qemu" scheme="http://zhangguanzhang.github.io/tags/qemu/"/>
    
    <category term="binfmt_misc" scheme="http://zhangguanzhang.github.io/tags/binfmt-misc/"/>
    
    <category term="loongarch64" scheme="http://zhangguanzhang.github.io/tags/loongarch64/"/>
    
  </entry>
  
  <entry>
    <title>openstack 虚机上 rabbitmqctl 超时</title>
    <link href="http://zhangguanzhang.github.io/2023/02/14/openstack-vm-rabbitmqctl/"/>
    <id>http://zhangguanzhang.github.io/2023/02/14/openstack-vm-rabbitmqctl/</id>
    <published>2023-02-14T10:40:30.000Z</published>
    <updated>2023-02-14T10:40:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下今天遇到的 openstack 上 rabbitmqctl 一直超时的问题</p><span id="more"></span><h2 id="前因"><a href="#前因" class="headerlink" title="前因"></a>前因</h2><p>现场遇到的问题，hostNetwork 启动的 rabbitmq 容器，启动后 rabbitmqctl 所有命令都超时报错，使用 ctl 是因为初始化集群后要设置用户。</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>按照时间线写了。</p><h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>调用 rabbitmqctl 报错日志为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# docker exec k8s_rabbitmq_rabbitmq-1-xx_0 rabbitmqctl --quiet add_user root &#x27;xxx&#x27;</span><br><span class="line">Error: unable to perform an operation on node &#x27;rabbit@zhxxxx-xxx-20230203150413-0008&#x27;. Please see diagnostics information and suggestions below.</span><br><span class="line"></span><br><span class="line">Most common reasons for this are:</span><br><span class="line"></span><br><span class="line"> * Target node is unreachable (e.g. due to hostname resolution, TCP connection or firewall issues)</span><br><span class="line"> * CLI tool fails to authenticate with the server (e.g. due to CLI tool&#x27;s Erlang cookie not matching that of the server)</span><br><span class="line"> * Target node is not running</span><br><span class="line"></span><br><span class="line">In addition to the diagnostics info below:</span><br><span class="line"></span><br><span class="line"> * See the CLI, clustering and networking guides on https://rabbitmq.com/documentation.html to learn more</span><br><span class="line"> * Consult server logs on node rabbit@zhxxxx-xxx-20230203150413-0008</span><br><span class="line"> * If target node is configured to use long node names, don&#x27;t forget to use --longnames with CLI tools</span><br><span class="line"></span><br><span class="line">DIAGNOSTICS</span><br><span class="line">===========</span><br><span class="line"></span><br><span class="line">attempted to contact: [&#x27;rabbit@zhxxxx-xxx-20230203150413-0008&#x27;]</span><br><span class="line"></span><br><span class="line">rabbit@zhxxxx-xxx-20230203150413-0008:</span><br><span class="line">  * connected to epmd (port 4369) on zhxxxx-xxx-20230203150413-0008</span><br><span class="line">  * epmd reports node &#x27;rabbit&#x27; uses port 25672 for inter-node and CLI tool traffic </span><br><span class="line">  * can&#x27;t establish TCP connection to the target node, reason: timeout (timed out)</span><br><span class="line">  * suggestion: check if host &#x27;zhxxxx-xxx-20230203150413-0008&#x27; resolves, is reachable and ports 25672, 4369 are not blocked by firewall</span><br><span class="line"></span><br><span class="line">Current node details:</span><br><span class="line"> * node name: &#x27;rabbitmqcli-2488-rabbit@zhxxxx-xxx-20230203150413-0008&#x27;</span><br><span class="line"> * effective user&#x27;s home directory: /var/lib/rabbitmq</span><br><span class="line"> * Erlang cookie hash: xxxx==</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看日志</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# docker logs xxx</span><br><span class="line">&#123;:query, :&quot;rabbit@zhxxxx-xxx-20230203150413-0008&quot;, &#123;:badrpc, :timeout&#125;&#125;</span><br><span class="line"></span><br><span class="line">2023-02-14 10:29:01.553 [debug] &lt;0.284.0&gt; Lager installed handler error_logger_lager_h into error_logger</span><br><span class="line">2023-02-14 10:29:01.565 [debug] &lt;0.290.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_lager_event</span><br><span class="line">2023-02-14 10:29:01.565 [debug] &lt;0.293.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_channel_lager_event</span><br><span class="line">2023-02-14 10:29:01.565 [debug] &lt;0.287.0&gt; Lager installed handler lager_forwarder_backend into error_logger_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.296.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_connection_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.305.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_ldap_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.308.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_mirroring_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.299.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_feature_flags_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.311.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_prelaunch_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.302.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_federation_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.314.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_queue_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.317.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_ra_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.323.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_upgrade_lager_event</span><br><span class="line">2023-02-14 10:29:01.566 [debug] &lt;0.320.0&gt; Lager installed handler lager_forwarder_backend into rabbit_log_shovel_lager_event</span><br><span class="line">2023-02-14 10:29:01.670 [info] &lt;0.267.0&gt; HiPE disabled: no modules were natively recompiled.</span><br><span class="line">2023-02-14 10:29:02.053 [debug] &lt;0.280.0&gt; Lager installed handler lager_backend_throttle into lager_event</span><br><span class="line">2023-02-14 10:29:02.093 [info] &lt;0.267.0&gt; </span><br><span class="line"> Starting RabbitMQ 3.8.5 on Erlang 23.0.3</span><br><span class="line"> Copyright (c) 2007-2020 VMware, Inc. or its affiliates.</span><br><span class="line"> Licensed under the MPL 1.1. Website: https://rabbitmq.com</span><br><span class="line"></span><br><span class="line">  ##  ##      RabbitMQ 3.8.5</span><br><span class="line">  ##  ##</span><br><span class="line">  ##########  Copyright (c) 2007-2020 VMware, Inc. or its affiliates.</span><br><span class="line">  ######  ##</span><br><span class="line">  ##########  Licensed under the MPL 1.1. Website: https://rabbitmq.com</span><br><span class="line"></span><br><span class="line">  Doc guides: https://rabbitmq.com/documentation.html</span><br><span class="line">  Support:    https://rabbitmq.com/contact.html</span><br><span class="line">  Tutorials:  https://rabbitmq.com/getstarted.html</span><br><span class="line">  Monitoring: https://rabbitmq.com/monitoring.html</span><br><span class="line"></span><br><span class="line">  Logs: &lt;stdout&gt;</span><br><span class="line"></span><br><span class="line">  Config file(s): /tmp/rabbitmq.conf</span><br><span class="line"></span><br><span class="line">  Starting broker...2023-02-14 10:29:02.096 [info] &lt;0.267.0&gt; </span><br><span class="line"> node           : rabbit@zhxxxx-xxx-20230203150413-0008</span><br><span class="line"> home dir       : /var/lib/rabbitmq</span><br><span class="line"> config file(s) : /tmp/rabbitmq.conf</span><br><span class="line"> cookie hash    : xxxxxxx==</span><br><span class="line"> log(s)         : &lt;stdout&gt;</span><br><span class="line"> database dir   : /var/lib/rabbitmq/mnesia/rabbit@zhxxxx-xxx-20230203150413-0008</span><br><span class="line">2023-02-14 10:29:07.360 [info] &lt;0.267.0&gt; Running boot step pre_boot defined by app rabbit</span><br><span class="line">2023-02-14 10:29:07.360 [info] &lt;0.267.0&gt; Running boot step rabbit_core_metrics defined by app rabbit</span><br><span class="line">2023-02-14 10:29:07.363 [info] &lt;0.267.0&gt; Running boot step rabbit_alarm defined by app rabbit</span><br><span class="line">2023-02-14 10:29:07.373 [info] &lt;0.355.0&gt; Memory high watermark set to 12301 MiB (12898874163 bytes) of 30753 MiB (32247185408 bytes) total</span><br><span class="line">2023-02-14 10:29:07.386 [info] &lt;0.357.0&gt; Enabling free disk space monitoring</span><br><span class="line">2023-02-14 10:29:07.386 [info] &lt;0.357.0&gt; Disk free limit set to 50MB</span><br><span class="line">2023-02-14 10:29:07.396 [info] &lt;0.267.0&gt; Running boot step code_server_cache defined by app rabbit</span><br><span class="line">2023-02-14 10:29:07.396 [info] &lt;0.267.0&gt; Running boot step file_handle_cache defined by app rabbit</span><br><span class="line">2023-02-14 10:29:07.398 [info] &lt;0.360.0&gt; Limiting to approx 1073741719 file handles (966367545 sockets)</span><br><span class="line">2023-02-14 10:29:07.398 [info] &lt;0.361.0&gt; FHC read buffering:  OFF</span><br><span class="line">2023-02-14 10:29:07.398 [info] &lt;0.361.0&gt; FHC write buffering: ON</span><br><span class="line">2023-02-14 10:29:07.399 [info] &lt;0.267.0&gt; Running boot step worker_pool defined by app rabbit</span><br><span class="line">2023-02-14 10:29:07.399 [info] &lt;0.343.0&gt; Will use 16 processes for default worker pool</span><br><span class="line">2023-02-14 10:29:07.400 [info] &lt;0.343.0&gt; Starting worker pool &#x27;worker_pool&#x27; with 16 processes in it</span><br><span class="line">2023-02-14 10:29:07.402 [info] &lt;0.267.0&gt; Running boot step database defined by app rabbit</span><br><span class="line">2023-02-14 10:29:07.403 [info] &lt;0.267.0&gt; Node database directory at /var/lib/rabbitmq/mnesia/rabbit@zhxxxx-xxx-20230203150413-0008 is empty. Assuming we need to join an existing cluster or initialise from scratch...</span><br><span class="line">2023-02-14 10:29:07.403 [info] &lt;0.267.0&gt; Configured peer discovery backend: rabbit_peer_discovery_classic_config</span><br><span class="line">2023-02-14 10:29:07.404 [info] &lt;0.267.0&gt; Will try to lock with peer discovery backend rabbit_peer_discovery_classic_config</span><br><span class="line">2023-02-14 10:29:07.404 [info] &lt;0.267.0&gt; Peer discovery backend does not support locking, falling back to randomized delay</span><br><span class="line">2023-02-14 10:29:07.404 [info] &lt;0.267.0&gt; Peer discovery backend rabbit_peer_discovery_classic_config does not support registration, skipping randomized startup delay.</span><br><span class="line">2023-02-14 10:29:07.404 [info] &lt;0.267.0&gt; All discovered existing cluster peers: </span><br><span class="line">2023-02-14 10:29:07.404 [info] &lt;0.267.0&gt; Discovered no peer nodes to cluster with. Some discovery backends can filter nodes out based on a readiness criteria. Enabling debug logging might help troubleshoot.</span><br><span class="line">2023-02-14 10:29:07.410 [info] &lt;0.44.0&gt; Application mnesia exited with reason: stopped</span><br><span class="line">2023-02-14 10:29:15.639 [info] &lt;0.267.0&gt; Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">2023-02-14 10:29:15.639 [info] &lt;0.267.0&gt; Successfully synced tables from a peer</span><br><span class="line">2023-02-14 10:29:15.700 [info] &lt;0.267.0&gt; Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">2023-02-14 10:29:15.700 [info] &lt;0.267.0&gt; Successfully synced tables from a peer</span><br><span class="line">2023-02-14 10:29:15.701 [info] &lt;0.267.0&gt; Feature flag `implicit_default_bindings`: supported, attempt to enable...</span><br><span class="line">2023-02-14 10:29:15.701 [info] &lt;0.267.0&gt; Feature flag `implicit_default_bindings`: mark as enabled=state_changing</span><br><span class="line">2023-02-14 10:29:15.713 [info] &lt;0.267.0&gt; Feature flags: list of feature flags found:</span><br><span class="line">2023-02-14 10:29:15.713 [info] &lt;0.267.0&gt; Feature flags:   [~] implicit_default_bindings</span><br><span class="line">2023-02-14 10:29:15.713 [info] &lt;0.267.0&gt; Feature flags:   [ ] quorum_queue</span><br><span class="line">2023-02-14 10:29:15.713 [info] &lt;0.267.0&gt; Feature flags:   [ ] virtual_host_metadata</span><br><span class="line">2023-02-14 10:29:15.713 [info] &lt;0.267.0&gt; Feature flags: feature flag states written to disk: yes</span><br><span class="line">2023-02-14 10:29:15.739 [info] &lt;0.267.0&gt; Waiting for Mnesia tables for 30000 ms, 0 retries left</span><br><span class="line">2023-02-14 10:29:15.739 [info] &lt;0.267.0&gt; Successfully synced tables from a peer</span><br><span class="line">2023-02-14 10:29:15.739 [info] &lt;0.267.0&gt; Feature flag `implicit_default_bindings`: mark as enabled=true</span><br><span class="line">2023-02-14 10:29:15.754 [info] &lt;0.267.0&gt; Feature flags: list of feature flags found:</span><br><span class="line">2023-02-14 10:29:15.754 [info] &lt;0.267.0&gt; Feature flags:   [x] implicit_default_bindings</span><br><span class="line">2023-02-14 10:29:15.754 [info] &lt;0.267.0&gt; Feature flags:   [ ] quorum_queue</span><br><span class="line">2023-02-14 10:29:15.754 [info] &lt;0.267.0&gt; Feature flags:   [ ] virtual_host_metadata</span><br><span class="line">2023-02-14 10:29:15.755 [info] &lt;0.267.0&gt; Feature flags: feature flag states written to disk: yes</span><br><span class="line">2023-02-14 10:29:15.784 [info] &lt;0.267.0&gt; Feature flag `quorum_queue`: supported, attempt to enable...</span><br><span class="line">2023-02-14 10:29:15.784 [info] &lt;0.267.0&gt; Feature flag `quorum_queue`: mark as enabled=state_changing</span><br><span class="line">2023-02-14 10:29:15.796 [info] &lt;0.267.0&gt; Feature flags: list of feature flags found:</span><br><span class="line">2023-02-14 10:29:15.797 [info] &lt;0.267.0&gt; Feature flags:   [x] implicit_default_bindings</span><br><span class="line">2023-02-14 10:29:15.797 [info] &lt;0.267.0&gt; Feature flags:   [~] quorum_queue</span><br><span class="line">2023-02-14 10:29:15.797 [info] &lt;0.267.0&gt; Feature flags:   [ ] virtual_host_metadata</span><br><span class="line">2023-02-14 10:29:15.797 [info] &lt;0.267.0&gt; Feature flags: feature flag states written to disk: yes</span><br><span class="line">2023-02-14 10:29:15.824 [info] &lt;0.267.0&gt; Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">2023-02-14 10:29:15.824 [info] &lt;0.267.0&gt; Successfully synced tables from a peer</span><br><span class="line">2023-02-14 10:29:15.824 [info] &lt;0.267.0&gt; Feature flag `quorum_queue`:   migrating Mnesia table rabbit_queue...</span><br><span class="line">2023-02-14 10:29:15.972 [info] &lt;0.267.0&gt; Feature flag `quorum_queue`:   migrating Mnesia table rabbit_durable_queue...</span><br><span class="line">2023-02-14 10:29:16.026 [info] &lt;0.267.0&gt; Feature flag `quorum_queue`:   Mnesia tables migration done</span><br><span class="line">2023-02-14 10:29:16.026 [info] &lt;0.267.0&gt; Feature flag `quorum_queue`: mark as enabled=true</span><br><span class="line">2023-02-14 10:29:16.040 [info] &lt;0.267.0&gt; Feature flags: list of feature flags found:</span><br><span class="line">2023-02-14 10:29:16.040 [info] &lt;0.267.0&gt; Feature flags:   [x] implicit_default_bindings</span><br><span class="line">2023-02-14 10:29:16.040 [info] &lt;0.267.0&gt; Feature flags:   [x] quorum_queue</span><br><span class="line">2023-02-14 10:29:16.041 [info] &lt;0.267.0&gt; Feature flags:   [ ] virtual_host_metadata</span><br><span class="line">2023-02-14 10:29:16.041 [info] &lt;0.267.0&gt; Feature flags: feature flag states written to disk: yes</span><br><span class="line">2023-02-14 10:29:16.067 [info] &lt;0.267.0&gt; Feature flag `virtual_host_metadata`: supported, attempt to enable...</span><br><span class="line">2023-02-14 10:29:16.067 [info] &lt;0.267.0&gt; Feature flag `virtual_host_metadata`: mark as enabled=state_changing</span><br><span class="line">2023-02-14 10:29:16.079 [info] &lt;0.267.0&gt; Feature flags: list of feature flags found:</span><br><span class="line">2023-02-14 10:29:16.079 [info] &lt;0.267.0&gt; Feature flags:   [x] implicit_default_bindings</span><br><span class="line">2023-02-14 10:29:16.079 [info] &lt;0.267.0&gt; Feature flags:   [x] quorum_queue</span><br><span class="line">2023-02-14 10:29:16.079 [info] &lt;0.267.0&gt; Feature flags:   [~] virtual_host_metadata</span><br><span class="line">2023-02-14 10:29:16.079 [info] &lt;0.267.0&gt; Feature flags: feature flag states written to disk: yes</span><br><span class="line">2023-02-14 10:29:16.106 [info] &lt;0.267.0&gt; Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">2023-02-14 10:29:16.106 [info] &lt;0.267.0&gt; Successfully synced tables from a peer</span><br><span class="line">2023-02-14 10:29:16.162 [info] &lt;0.267.0&gt; Feature flag `virtual_host_metadata`: mark as enabled=true</span><br><span class="line">2023-02-14 10:29:16.176 [info] &lt;0.267.0&gt; Feature flags: list of feature flags found:</span><br><span class="line">2023-02-14 10:29:16.176 [info] &lt;0.267.0&gt; Feature flags:   [x] implicit_default_bindings</span><br><span class="line">2023-02-14 10:29:16.176 [info] &lt;0.267.0&gt; Feature flags:   [x] quorum_queue</span><br><span class="line">2023-02-14 10:29:16.176 [info] &lt;0.267.0&gt; Feature flags:   [x] virtual_host_metadata</span><br><span class="line">2023-02-14 10:29:16.176 [info] &lt;0.267.0&gt; Feature flags: feature flag states written to disk: yes</span><br><span class="line">2023-02-14 10:29:16.200 [info] &lt;0.267.0&gt; Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">2023-02-14 10:29:16.200 [info] &lt;0.267.0&gt; Successfully synced tables from a peer</span><br><span class="line">2023-02-14 10:29:16.257 [info] &lt;0.267.0&gt; Waiting for Mnesia tables for 30000 ms, 9 retries left</span><br><span class="line">2023-02-14 10:29:16.258 [info] &lt;0.267.0&gt; Successfully synced tables from a peer</span><br><span class="line">2023-02-14 10:29:16.258 [info] &lt;0.267.0&gt; Peer discovery backend rabbit_peer_discovery_classic_config does not support registration, skipping registration.</span><br><span class="line">2023-02-14 10:29:16.258 [info] &lt;0.267.0&gt; Running boot step database_sync defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.258 [info] &lt;0.267.0&gt; Running boot step feature_flags defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.259 [info] &lt;0.267.0&gt; Running boot step codec_correctness_check defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.259 [info] &lt;0.267.0&gt; Running boot step external_infrastructure defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.259 [info] &lt;0.267.0&gt; Running boot step rabbit_registry defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.259 [info] &lt;0.267.0&gt; Running boot step rabbit_auth_mechanism_cr_demo defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.260 [info] &lt;0.267.0&gt; Running boot step rabbit_queue_location_random defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.260 [info] &lt;0.267.0&gt; Running boot step rabbit_event defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.261 [info] &lt;0.267.0&gt; Running boot step rabbit_auth_mechanism_amqplain defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.261 [info] &lt;0.267.0&gt; Running boot step rabbit_auth_mechanism_plain defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.261 [info] &lt;0.267.0&gt; Running boot step rabbit_exchange_type_direct defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.261 [info] &lt;0.267.0&gt; Running boot step rabbit_exchange_type_fanout defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.262 [info] &lt;0.267.0&gt; Running boot step rabbit_exchange_type_headers defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.262 [info] &lt;0.267.0&gt; Running boot step rabbit_exchange_type_topic defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.263 [info] &lt;0.267.0&gt; Running boot step rabbit_mirror_queue_mode_all defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.263 [info] &lt;0.267.0&gt; Running boot step rabbit_mirror_queue_mode_exactly defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.263 [info] &lt;0.267.0&gt; Running boot step rabbit_mirror_queue_mode_nodes defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.263 [info] &lt;0.267.0&gt; Running boot step rabbit_priority_queue defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.263 [info] &lt;0.267.0&gt; Priority queues enabled, real BQ is rabbit_variable_queue</span><br><span class="line">2023-02-14 10:29:16.263 [info] &lt;0.267.0&gt; Running boot step rabbit_queue_location_client_local defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.264 [info] &lt;0.267.0&gt; Running boot step rabbit_queue_location_min_masters defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.264 [info] &lt;0.267.0&gt; Running boot step kernel_ready defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.264 [info] &lt;0.267.0&gt; Running boot step rabbit_sysmon_minder defined by app rabbit</span><br><span class="line">2023-02-14 10:29:16.265 [info] &lt;0.267.0&gt; Running boot step rabbit_epmd_monitor defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.268 [info] &lt;0.607.0&gt; epmd monitor knows us, inter-node communication (distribution) port: 25672</span><br><span class="line">2023-02-14 10:29:24.269 [info] &lt;0.267.0&gt; Running boot step guid_generator defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.280 [info] &lt;0.267.0&gt; Running boot step rabbit_node_monitor defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.281 [info] &lt;0.619.0&gt; Starting rabbit_node_monitor</span><br><span class="line">2023-02-14 10:29:24.281 [info] &lt;0.267.0&gt; Running boot step delegate_sup defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.283 [info] &lt;0.267.0&gt; Running boot step rabbit_memory_monitor defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.283 [info] &lt;0.267.0&gt; Running boot step core_initialized defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.284 [info] &lt;0.267.0&gt; Running boot step upgrade_queues defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.316 [info] &lt;0.267.0&gt; message_store upgrades: 1 to apply</span><br><span class="line">2023-02-14 10:29:24.316 [info] &lt;0.267.0&gt; message_store upgrades: Applying rabbit_variable_queue:move_messages_to_vhost_store</span><br><span class="line">2023-02-14 10:29:24.316 [info] &lt;0.267.0&gt; message_store upgrades: No durable queues found. Skipping message store migration</span><br><span class="line">2023-02-14 10:29:24.317 [info] &lt;0.267.0&gt; message_store upgrades: Removing the old message store data</span><br><span class="line">2023-02-14 10:29:24.318 [info] &lt;0.267.0&gt; message_store upgrades: All upgrades applied successfully</span><br><span class="line">2023-02-14 10:29:24.358 [info] &lt;0.267.0&gt; Running boot step rabbit_connection_tracking defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.358 [info] &lt;0.267.0&gt; Running boot step rabbit_connection_tracking_handler defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.359 [info] &lt;0.267.0&gt; Running boot step rabbit_exchange_parameters defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.359 [info] &lt;0.267.0&gt; Running boot step rabbit_mirror_queue_misc defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.360 [info] &lt;0.267.0&gt; Running boot step rabbit_policies defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.363 [info] &lt;0.267.0&gt; Running boot step rabbit_policy defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.363 [info] &lt;0.267.0&gt; Running boot step rabbit_queue_location_validator defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.363 [info] &lt;0.267.0&gt; Running boot step rabbit_quorum_memory_manager defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.363 [info] &lt;0.267.0&gt; Running boot step rabbit_vhost_limit defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.364 [info] &lt;0.267.0&gt; Running boot step recovery defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.366 [info] &lt;0.267.0&gt; Running boot step definition_import_worker_pool defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.366 [info] &lt;0.343.0&gt; Starting worker pool &#x27;definition_import_pool&#x27; with 16 processes in it</span><br><span class="line">2023-02-14 10:29:24.369 [info] &lt;0.267.0&gt; Running boot step load_core_definitions defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.369 [info] &lt;0.267.0&gt; Running boot step empty_db_check defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.369 [info] &lt;0.267.0&gt; Adding vhost &#x27;/&#x27; (description: &#x27;Default virtual host&#x27;)</span><br><span class="line">2023-02-14 10:29:24.435 [info] &lt;0.674.0&gt; Making sure data directory &#x27;/var/lib/rabbitmq/mnesia/rabbit@zhxxxx-xxx-20230203150413-0008/msg_stores/vhosts/628WB79CIFDYO9LJI6DKMI09L&#x27; for vhost &#x27;/&#x27; exists</span><br><span class="line">2023-02-14 10:29:24.446 [info] &lt;0.674.0&gt; Starting message stores for vhost &#x27;/&#x27;</span><br><span class="line">2023-02-14 10:29:24.446 [info] &lt;0.678.0&gt; Message store &quot;628WB79CIFDYO9LJI6DKMI09L/msg_store_transient&quot;: using rabbit_msg_store_ets_index to provide index</span><br><span class="line">2023-02-14 10:29:24.451 [info] &lt;0.674.0&gt; Started message store of type transient for vhost &#x27;/&#x27;</span><br><span class="line">2023-02-14 10:29:24.451 [info] &lt;0.682.0&gt; Message store &quot;628WB79CIFDYO9LJI6DKMI09L/msg_store_persistent&quot;: using rabbit_msg_store_ets_index to provide index</span><br><span class="line">2023-02-14 10:29:24.453 [warning] &lt;0.682.0&gt; Message store &quot;628WB79CIFDYO9LJI6DKMI09L/msg_store_persistent&quot;: rebuilding indices from scratch</span><br><span class="line">2023-02-14 10:29:24.455 [info] &lt;0.674.0&gt; Started message store of type persistent for vhost &#x27;/&#x27;</span><br><span class="line">2023-02-14 10:29:24.466 [info] &lt;0.267.0&gt; Created user &#x27;root&#x27;</span><br><span class="line">2023-02-14 10:29:24.473 [info] &lt;0.267.0&gt; Successfully set user tags for user &#x27;root&#x27; to [administrator]</span><br><span class="line">2023-02-14 10:29:24.481 [info] &lt;0.267.0&gt; Successfully set permissions for &#x27;root&#x27; in virtual host &#x27;/&#x27; to &#x27;.*&#x27;, &#x27;.*&#x27;, &#x27;.*&#x27;</span><br><span class="line">2023-02-14 10:29:24.481 [info] &lt;0.267.0&gt; Running boot step rabbit_looking_glass defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.481 [info] &lt;0.267.0&gt; Running boot step rabbit_core_metrics_gc defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.482 [info] &lt;0.267.0&gt; Running boot step background_gc defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.483 [info] &lt;0.267.0&gt; Running boot step connection_tracking defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.494 [info] &lt;0.267.0&gt; Setting up a table for connection tracking on this node: &#x27;tracked_connection_on_node_rabbit@zhxxxx-xxx-20230203150413-0008&#x27;</span><br><span class="line">2023-02-14 10:29:24.506 [info] &lt;0.267.0&gt; Setting up a table for per-vhost connection counting on this node: &#x27;tracked_connection_per_vhost_on_node_rabbit@zhxxxx-xxx-20230203150413-0008&#x27;</span><br><span class="line">2023-02-14 10:29:24.507 [info] &lt;0.267.0&gt; Running boot step routing_ready defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.507 [info] &lt;0.267.0&gt; Running boot step pre_flight defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.507 [info] &lt;0.267.0&gt; Running boot step notify_cluster defined by app rabbit</span><br><span class="line">2023-02-14 10:29:24.507 [info] &lt;0.267.0&gt; Running boot step networking defined by app rabbit</span><br><span class="line">2023-02-14 10:29:40.515 [info] &lt;0.745.0&gt; started TCP listener on [::]:5672</span><br><span class="line">2023-02-14 10:29:48.517 [info] &lt;0.267.0&gt; Running boot step cluster_name defined by app rabbit</span><br><span class="line">2023-02-14 10:29:48.518 [info] &lt;0.267.0&gt; Initialising internal cluster ID to &#x27;rabbitmq-cluster-id-AQyx0iUPrMGenjAJo1uO7A&#x27;</span><br><span class="line">2023-02-14 10:29:48.527 [info] &lt;0.267.0&gt; Running boot step direct_client defined by app rabbit</span><br><span class="line">2023-02-14 10:29:50.216 [info] &lt;0.760.0&gt; Feature flags: list of feature flags found:</span><br><span class="line">2023-02-14 10:29:50.217 [info] &lt;0.760.0&gt; Feature flags:   [ ] drop_unroutable_metric</span><br><span class="line">2023-02-14 10:29:50.217 [info] &lt;0.760.0&gt; Feature flags:   [ ] empty_basic_get_metric</span><br><span class="line">2023-02-14 10:29:50.217 [info] &lt;0.760.0&gt; Feature flags:   [x] implicit_default_bindings</span><br><span class="line">2023-02-14 10:29:50.217 [info] &lt;0.760.0&gt; Feature flags:   [x] quorum_queue</span><br><span class="line">2023-02-14 10:29:50.217 [info] &lt;0.760.0&gt; Feature flags:   [x] virtual_host_metadata</span><br><span class="line">2023-02-14 10:29:50.217 [info] &lt;0.760.0&gt; Feature flags: feature flag states written to disk: yes</span><br><span class="line">2023-02-14 10:29:51.162 [info] &lt;0.760.0&gt; Running boot step rabbit_mgmt_db_handler defined by app rabbitmq_management_agent</span><br><span class="line">2023-02-14 10:29:51.162 [info] &lt;0.760.0&gt; Management plugin: using rates mode &#x27;basic&#x27;</span><br><span class="line">2023-02-14 10:29:51.326 [info] &lt;0.760.0&gt; Running boot step rabbit_mgmt_reset_handler defined by app rabbitmq_management</span><br><span class="line">2023-02-14 10:29:51.326 [info] &lt;0.760.0&gt; Running boot step rabbit_management_load_definitions defined by app rabbitmq_management</span><br><span class="line">2023-02-14 10:29:59.390 [info] &lt;0.827.0&gt; Management plugin: HTTP (non-TLS) listener started on port 15672</span><br><span class="line">2023-02-14 10:29:59.392 [info] &lt;0.941.0&gt; Statistics database started.</span><br><span class="line">2023-02-14 10:29:59.392 [info] &lt;0.940.0&gt; Starting worker pool &#x27;management_worker_pool&#x27; with 3 processes in it</span><br><span class="line">2023-02-14 10:30:07.530 [info] &lt;0.954.0&gt; Prometheus metrics: HTTP (non-TLS) listener started on port 15692</span><br><span class="line">2023-02-14 10:30:08.171 [info] &lt;0.760.0&gt; Server startup complete; 4 plugins started.</span><br><span class="line"> * rabbitmq_prometheus</span><br><span class="line"> * rabbitmq_management</span><br><span class="line"> * rabbitmq_web_dispatch</span><br><span class="line"> * rabbitmq_management_agent</span><br><span class="line"> completed with 4 plugins.</span><br></pre></td></tr></table></figure><p>看日志结尾也有 5672 端口监听，习惯性的从结尾看了，后面没头绪，往前翻看日志，看到了第一行的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;:query, :&quot;rabbit@zhxxxx-xxx-20230203150413-0008&quot;, &#123;:badrpc, :timeout&#125;&#125;</span><br></pre></td></tr></table></figure><p>如果 rabbitmq 多机的时候，出现 badrpc 一般是网络不通，或者成员之间的 hostname 无法解析，感觉这个单机的 mq 应该也是类似问题，看下 hosts 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# docker exec -ti k8s_rabbitmq_rabbitmq-xxxx_0 bash</span><br><span class="line">root@zhxxxx-xxx-20230203150413-0008:/# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">....</span><br><span class="line">127.0.0.1 zhxxxx-xxx-20230203150413-0008.novalocal</span><br></pre></td></tr></table></figure><p>emmm，果然没有 <code>zhxxxx-xxx-20230203150413-0008</code> 的解析记录，添加后就好了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@zhxxxx-xxx-20230203150413-0008:/# echo &#x27;127.0.0.1 zhxxxx-xxx-20230203150413-0008&#x27; &gt;&gt; /etc/hosts</span><br><span class="line">root@zhxxxx-xxx-20230203150413-0008:/# rabbitmqctl list_users</span><br><span class="line">Listing users ...</span><br><span class="line">usertags</span><br><span class="line">root[administrator]</span><br></pre></td></tr></table></figure><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p><code>.novalocal</code> 是 openstack nova dhcp_domain 添加的，不知道为啥 mq 不把后缀当作 hostname 的部分，使用 hostnamectl 看是对的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# hostnamectl status</span><br><span class="line"> Static hostname: zhxxxx-xxx-20230203150413-0008.novalocal</span><br><span class="line">       Icon name: computer-vm</span><br><span class="line">         Chassis: vm</span><br><span class="line">      Machine ID: 059dd05a412c4xxxxxxxxxbf9892c16c</span><br><span class="line">         Boot ID: 8df788ae62e54xxxxxxxx99ce87178fe</span><br><span class="line">  Virtualization: kvm</span><br><span class="line">Operating System: openEuler 22.03 LTS</span><br><span class="line">          Kernel: Linux 5.10.0-60.18.0.50.oe2203.aarch64</span><br><span class="line">    Architecture: arm64</span><br><span class="line"> Hardware Vendor: QEMU</span><br><span class="line">  Hardware Model: KVM Virtual Machine</span><br><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# echo $HOSTNAME</span><br><span class="line">zhxxxx-xxx-20230203150413-0008.novalocal</span><br><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# docker exec -ti k8s_rabbitmq_rabbitmq-1-xxxx_0  bash</span><br><span class="line">root@zhxxxx-xxx-20230203150413-0008:/# echo $HOSTNAME</span><br><span class="line">zhxxxx-xxx-20230203150413-0008.novalocal</span><br></pre></td></tr></table></figure><p>关于反向解析的，查了下默认是关闭的，暂时宿主机上手动添加 hosts 解析解决</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker exec 0ea rabbitmqctl environment | grep -i dns</span><br><span class="line">      &#123;reverse_dns_lookups,false&#125;,</span><br></pre></td></tr></table></figure><h3 id="后续解决"><a href="#后续解决" class="headerlink" title="后续解决"></a>后续解决</h3><p>打算更改下 rabbitmq 的 docker 镜像增加 entrypoint.sh ，增加去掉点后缀的 hosts 记录，避免这种情况，因为 nova 上可以改 dhcp_domain，兼容未知后缀的话就是打地鼠，而且 host 网络容器每次 restart 后都会同步宿主机的 hosts，所以镜像启动脚本兼容是最优解</p><p>在修改期间，看到了官方的 docker-entrypoint.sh 脚本里有类似判断：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># if long and short hostnames are not the same, use long hostnames</span><br><span class="line">if [ &quot;$(hostname)&quot; != &quot;$(hostname -s)&quot; ]; then</span><br><span class="line">: &quot;$&#123;RABBITMQ_USE_LONGNAME:=true&#125;&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>机器上输出则是下面，确实不一样</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# hostname</span><br><span class="line">zhxxxx-xxx-20230203150413-0008.novalocal</span><br><span class="line">[root@zhxxxx-xxx-20230203150413-0008 ~]# hostname -s</span><br><span class="line">zhxxxx-xxx-20230203150413-0008</span><br></pre></td></tr></table></figure><p>官方文档的 clustering.html 里并没有看懂 <code>RABBITMQ_USE_LONGNAME</code> 描述，还是直接 hack entrypoint.sh 在前面添加下面的了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;$(hostname)&quot; != &quot;$(hostname -s)&quot; ]; then</span><br><span class="line">  grep -Eqw `hostname -s`&#x27;$&#x27; /etc/hosts || echo &quot;127.0.0.1 `hostname -s`&quot; &gt;&gt; /etc/hosts</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/docker-library/rabbitmq/blob/master/3.11/ubuntu/docker-entrypoint.sh">https://github.com/docker-library/rabbitmq/blob/master/3.11/ubuntu/docker-entrypoint.sh</a></li><li><a href="https://www.rabbitmq.com/configure.html">https://www.rabbitmq.com/configure.html</a></li><li><a href="https://www.rabbitmq.com/clustering.html">https://www.rabbitmq.com/clustering.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录一下今天遇到的 openstack 上 rabbitmqctl 一直超时的问题&lt;/p&gt;</summary>
    
    
    
    
    <category term="openstack" scheme="http://zhangguanzhang.github.io/tags/openstack/"/>
    
    <category term="hostname" scheme="http://zhangguanzhang.github.io/tags/hostname/"/>
    
  </entry>
  
  <entry>
    <title>k8s java 容器未设置 requests.cpu 导致性能问题</title>
    <link href="http://zhangguanzhang.github.io/2022/12/30/k8s-java-request-cpu/"/>
    <id>http://zhangguanzhang.github.io/2022/12/30/k8s-java-request-cpu/</id>
    <published>2022-12-30T10:10:30.000Z</published>
    <updated>2022-12-30T10:10:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>内部性能测试发现了 java 容器在 k8s 的问题</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>内部之前 es 是 docker 起的，后面换成 k8s staticPod + hostPath ，在性能压测的时候发现性能很低</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>和版本无关，所以不放版本信息了，直接写过程了。</p><h3 id="眉目"><a href="#眉目" class="headerlink" title="眉目"></a>眉目</h3><p>当时看 Prometheus 发现机器占用很低，感觉 es 没充分利用机器资源。查看下 es 的接口：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ curl -s --user xxx:xxx xxx:9200/_nodes/os</span><br><span class="line">...</span><br><span class="line">        &quot;available_processors&quot;: 1,</span><br><span class="line">        &quot;allocated_processors&quot;: 1</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>发现不对劲，默认没限制应该和宿主机 cpu 核心一样的。然后对比了下，发现 docker run 的 es 正常， k8s 则是上面的异常，对比了 docker inspect 发现 k8s 的容器 CpuShares 设置的是 2，docker 是 1024</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;CpuShares&quot;: 2,</span><br></pre></td></tr></table></figure><p>利用代码测测试下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /opt/test.java &lt;&lt; EOF</span><br><span class="line">public class Main &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        System.out.println(&quot;Available Processors = &quot; +Runtime.getRuntime().availableProcessors());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ docker run --rm -v /opt/test.java:/test.java --cpu-shares=2 elasticsearch:7.9.3 /usr/share/elasticsearch/jdk/bin/java /test.java</span><br><span class="line">Available Processors = 1</span><br><span class="line">$ docker run --rm -v /opt/test.java:/test.java elasticsearch:7.9.3 /usr/share/elasticsearch/jdk/bin/java /test.java</span><br><span class="line">Available Processors = 16</span><br></pre></td></tr></table></figure><p>通过一下步骤可以看到 docker 默认参数下 cpu.share 是 1024：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ docker inspect xxx | grep Pid</span><br><span class="line">$ grep cpu /proc/$pid/cgroup</span><br><span class="line"># 用获得的 cgroup path查</span><br><span class="line">$ find /sys  -type d -name xxxxxxxxxxxxxxxxxxx</span><br><span class="line">$ cat /sys/fs/cgroup/cpu,cpuacct/docker/xxxxxxxxxxxxxxxxxxx/cpu.shares</span><br><span class="line">1024</span><br><span class="line">$ docker run --rm -v /opt/test.java:/test.java --cpu-shares=1024 elasticsearch:7.9.3 /usr/share/elasticsearch/jdk/bin/java /test.java</span><br><span class="line">Available Processors = 16</span><br></pre></td></tr></table></figure><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>可以看文章结尾参考关于大佬们的讨论，java 计算可用核心是关于 cpu 几个 cgroup 的值，默认在接口调用 docker 的时候，没给 docker 传递的话，cpu.shares 默认就是 2，可以给 pod 一个 request.cpu 1 即可避免这种问题</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://docs.docker.com/config/containers/resource_constraints/#cpu">https://docs.docker.com/config/containers/resource_constraints/#cpu</a></li><li><a href="https://github.com/aws/amazon-ecs-agent/issues/1735">https://github.com/aws/amazon-ecs-agent/issues/1735</a></li><li><a href="https://bugs.openjdk.org/browse/JDK-8146115">https://bugs.openjdk.org/browse/JDK-8146115</a></li><li><a href="https://bugs.openjdk.org/browse/JDK-8140793">https://bugs.openjdk.org/browse/JDK-8140793</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;内部性能测试发现了 java 容器在 k8s 的问题&lt;/p&gt;</summary>
    
    
    
    
    <category term="ttf" scheme="http://zhangguanzhang.github.io/tags/ttf/"/>
    
    <category term="woff" scheme="http://zhangguanzhang.github.io/tags/woff/"/>
    
  </entry>
  
  <entry>
    <title>如何打包一个不依赖 python 的 supervisor 的包去离线部署</title>
    <link href="http://zhangguanzhang.github.io/2022/11/03/pyinstaller-supervisor/"/>
    <id>http://zhangguanzhang.github.io/2022/11/03/pyinstaller-supervisor/</id>
    <published>2022-11-03T17:10:30.000Z</published>
    <updated>2022-11-03T17:10:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>近期的一次信创适配，需要 supervisor 离线安装</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><h3 id="信创涉密设备介绍"><a href="#信创涉密设备介绍" class="headerlink" title="信创涉密设备介绍"></a>信创涉密设备介绍</h3><p>无网络的信创涉密设备适配，要求列举下就是:</p><ol><li>光盘拷贝 rpm 包上去，实际可以绕过去(电脑网线直连服务器，scp)，但是不重要</li><li>不能使用容器，必须全部 rpm 包方式部署业务，有专门的图形软件添加和安装 rpm 包，意味着你搭建本地源啥的都用不了，只能一个个 rpm 包添加到界面安装</li><li>系统的 rpm 包不能升级和降级，这个代表着你拷贝了全套的 rpm 包，很可能都用不了，例如自带的某个 so 版本和你的业务的冲突或者不匹配</li><li>只有系统里注册的 rpm 包里的文件和脚本才具备执行能力，比如你在设备上 vi test.sh 写 echo 123, 然后 bash test.sh 都无法执行</li><li>自己的要求: 对于业务进程以及方便现场的实施查看我们的所有业务服务和支持日志轮转。</li></ol><h3 id="选型考虑"><a href="#选型考虑" class="headerlink" title="选型考虑"></a>选型考虑</h3><p>业务侧是业务研发处理，大部分都是 golang 开发的（都是静态编译的），我们需要做的就是进程纳管，这方面的轮子简单对比下，大概需求列为表格就是下面的需求</p><table><thead><tr><th align="left">选型</th><th align="center">安装过程</th><th align="center">支持 env file</th><th align="center">支持给进程 logfile rotate</th><th align="center">组(或者label)纳管</th><th align="center">备注</th></tr></thead><tbody><tr><td align="left">systemd</td><td align="center">不老的系统都自带</td><td align="center">✔</td><td align="center">✔</td><td align="center">有通配符，但是没组的概念</td><td align="center"></td></tr><tr><td align="left">supervisor-py版本</td><td align="center">需要pip或者包管理安装</td><td align="center">x</td><td align="center">✔</td><td align="center">✔</td><td align="center">env file 的支持 hack 就行</td></tr><tr><td align="left">supervisor-go版本</td><td align="center">单独二进制</td><td align="center">x</td><td align="center">✔</td><td align="center">✔</td><td align="center"></td></tr><tr><td align="left">pm2</td><td align="center">还得安装node</td><td align="center">✔</td><td align="center">x</td><td align="center">x</td><td align="center">除了安装过程，貌似我都不了解</td></tr></tbody></table><p>pm2 从安装就不考虑了，supervisor-go 版本感觉开发者都不看 issue，而且它的 master 提交记录修复的 bug 描述看都是很常见的场景，出现这个问题我认为是单元测试覆盖不全面以及用户面小，现阶段和生产上我不会考虑它。而 systemd 不支持组纳管，初步是选用 python 版本的 supervisor，但是 supervisor 安装无非就两种，一种是 pip 安装，一种是全套 rpm 包安装：</p><ul><li>python-meld3 &gt;&#x3D; 0.6.5</li><li>python2-meld3</li><li>supervisor</li></ul><p>涉密设备系统上不一定有 pip，使用 rpm 的方式的话，在 arm64 的麒麟里下的上面三个 rpm 可能在另一个 arm64 的 uos 上就无法使用了，并且实际上麒麟也有好几个类型，穷举的话可以，但是同一个系统还可能在迭代升级，之前的 rpm 包里的某个依赖可能就不满足了，再穷举每个os的所有版本去维护的话，难度就非常痛苦。</p><p>之前组长的解决办法就是起容器安装 supervisor，然后把 site-package 目录打包，事实证明这个确实可以，但是部署的客户数量少，如果碰到部分客户已经安装了某个 site-package 的依赖，我们这个 supervisor 的包就安装不上去了。</p><p>这次就想着如何把 supervisor 打包成一个没有任何依赖的 rpm 包，想着是利用 pyinstaller，搜了下这块没人做过，现在是已经搞完了验证可行了。</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><h3 id="pyinstaller-打包"><a href="#pyinstaller-打包" class="headerlink" title="pyinstaller 打包"></a>pyinstaller 打包</h3><p>pyinstaller 是一个把 py 脚本打包成独立的可执行文件的工具，推荐 pip 安装 pyinstaller，我的思路是容器里安装 supervisor 后打包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -ti -w /test --entrypoint bash centos:7</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">python3 会自带 python3-pip ， binutils 是 pyinstaller 打包的时候依赖</span></span><br><span class="line">yum install -y python3 binutils gcc zlib-devel which</span><br><span class="line"></span><br><span class="line">pip3 install wheel</span><br><span class="line">pip3 install pyinstaller</span><br><span class="line">pip3 install supervisor</span><br></pre></td></tr></table></figure><h4 id="过程和遇到的错误"><a href="#过程和遇到的错误" class="headerlink" title="过程和遇到的错误"></a>过程和遇到的错误</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller `which supervisord`</span><br></pre></td></tr></table></figure><p>打包完后：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@6ec6694c6688 test]# ls -l</span><br><span class="line">total 4</span><br><span class="line">drwxr-xr-x 3 root root   25 Nov  3 10:57 build</span><br><span class="line">drwxr-xr-x 3 root root   25 Nov  3 10:57 dist</span><br><span class="line">-rw-r--r-- 1 root root 1150 Nov  3 10:57 supervisord.spec</span><br><span class="line">[root@6ec6694c6688 test]# ls -l dist/</span><br><span class="line">total 4</span><br><span class="line">drwxr-xr-x 3 root root 4096 Nov  3 10:57 supervisord</span><br><span class="line">[root@6ec6694c6688 test]# ls -l dist/supervisord/</span><br><span class="line">total 11580</span><br><span class="line">-rw-r--r-- 1 root root  786953 Nov  3 10:57 base_library.zip</span><br><span class="line">drwxr-xr-x 2 root root    4096 Nov  3 10:57 lib-dynload</span><br><span class="line">-rwxr-xr-x 1 root root   68192 Nov 20  2015 libbz2.so.1</span><br><span class="line">-rwxr-xr-x 1 root root   15856 Sep 30  2020 libcom_err.so.2</span><br><span class="line">-rwxr-xr-x 1 root root 2521144 Aug  9  2019 libcrypto.so.10</span><br><span class="line">-rwxr-xr-x 1 root root  173320 Sep 30  2020 libexpat.so.1</span><br><span class="line">-rwxr-xr-x 1 root root   32328 Apr  1  2020 libffi.so.6</span><br><span class="line">-rwxr-xr-x 1 root root  320720 Sep 30  2020 libgssapi_krb5.so.2</span><br><span class="line">-rwxr-xr-x 1 root root  210784 Sep 30  2020 libk5crypto.so.3</span><br><span class="line">-rwxr-xr-x 1 root root   15688 Jun 10  2014 libkeyutils.so.1</span><br><span class="line">-rwxr-xr-x 1 root root  967840 Sep 30  2020 libkrb5.so.3</span><br><span class="line">-rwxr-xr-x 1 root root   67104 Sep 30  2020 libkrb5support.so.0</span><br><span class="line">-rwxr-xr-x 1 root root  157424 Nov  5  2016 liblzma.so.5</span><br><span class="line">-rwxr-xr-x 1 root root  402384 Aug  2  2017 libpcre.so.1</span><br><span class="line">-rwxr-xr-x 1 root root 3144192 Nov 16  2020 libpython3.6m.so.1.0</span><br><span class="line">-rwxr-xr-x 1 root root  285136 Aug  8  2019 libreadline.so.6</span><br><span class="line">-rwxr-xr-x 1 root root  155744 Apr  1  2020 libselinux.so.1</span><br><span class="line">-rwxr-xr-x 1 root root  470376 Aug  9  2019 libssl.so.10</span><br><span class="line">-rwxr-xr-x 1 root root  174576 Sep  6  2017 libtinfo.so.5</span><br><span class="line">-rwxr-xr-x 1 root root   90160 May 12 14:58 libz.so.1</span><br><span class="line">-rwxr-xr-x 1 root root 1749176 Nov  3 10:57 supervisord</span><br></pre></td></tr></table></figure><p>然后执行了下，发现报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./supervisord --version</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;supervisord&quot;, line 7, in &lt;module&gt;</span><br><span class="line">  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load</span><br><span class="line">  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked</span><br><span class="line">  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked</span><br><span class="line">  File &quot;PyInstaller/loader/pyimod03_importers.py&quot;, line 495, in exec_module</span><br><span class="line">  File &quot;supervisor/supervisord.py&quot;, line 41, in &lt;module&gt;</span><br><span class="line">  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load</span><br><span class="line">  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked</span><br><span class="line">  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked</span><br><span class="line">  File &quot;PyInstaller/loader/pyimod03_importers.py&quot;, line 495, in exec_module</span><br><span class="line">  File &quot;supervisor/options.py&quot;, line 63, in &lt;module&gt;</span><br><span class="line">  File &quot;supervisor/options.py&quot;, line 61, in _read_version_txt</span><br><span class="line">FileNotFoundError: [Errno 2] No such file or directory: &#x27;/root/supervisor/dist/supervisord/supervisor/version.txt&#x27;</span><br><span class="line">[18114] Failed to execute script &#x27;supervisord&#x27; due to unhandled exception!</span><br></pre></td></tr></table></figure><p>解决办法就是创建文件 <code>supervisor/version.txt</code> ，但是实际上可以 hack 下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ver=$(find /usr/ -type f -name &#x27;version.txt&#x27; -path &#x27;*/supervisor/*&#x27; -exec cat &#123;&#125; \; )</span><br><span class="line">op_file=$(find /usr/ -type f -name &#x27;options.py&#x27; -path &#x27;*/supervisor/*&#x27;)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置 VERSION</span></span><br><span class="line">sed -ri &#x27;/^VERSION\s+=\s+/s#= .+#= &quot;&#x27;&quot;$&#123;ver&#125;&quot;&#x27;&quot;#&#x27; $op_file</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">打包</span></span><br><span class="line">pyinstaller `which supervisord`</span><br></pre></td></tr></table></figure><p>然后发现移动路径会无法执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> dist/supervisord/supervisord .</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./supervisord --<span class="built_in">help</span></span></span><br><span class="line">[1648] Error loading Python lib &#x27;/test/libpython3.6m.so.1.0&#x27;: dlopen: /test/libpython3.6m.so.1.0: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure><p>看样子是 dlopen 的形式打开相对目录的 so，但是我想打包成一个文件，看了下不支持静态编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller --help |&amp; grep -P &#x27;link|static&#x27;</span><br></pre></td></tr></table></figure><p>最后搜索了下，加上选项 <code>-F, --onefile         Create a one-file bundled executable.</code> 打包成一个文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rm -rf supervisord* dist/ build</span><br><span class="line"></span><br><span class="line">pyinstaller --onefile `which supervisord`</span><br></pre></td></tr></table></figure><p>然后发现容器内可以执行，但是在 arm64 的 os 上执行会报错 <code>No moudle named supervisor.xxx</code>，需要加 <code>-p DIR, --paths DIR   A path to search for imports (like using PYTHONPATH).</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyinstaller --onefile -p /usr/local/lib/python3.7/site-packages `which supervisord`</span><br></pre></td></tr></table></figure><p>打包成 rpm 包的话，以下文件需要打包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可以 rpm 包安装下 supervisor ，看下哪些文件需要</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">rpm -ql supervisor | grep -Pv <span class="string">&#x27;\.py|/site-packages/|/share/&#x27;</span></span></span><br><span class="line">/etc/logrotate.d/supervisor</span><br><span class="line">/etc/supervisord.conf</span><br><span class="line">/etc/supervisord.d</span><br><span class="line">/etc/tmpfiles.d/supervisor.conf</span><br><span class="line">/usr/bin/echo_supervisord_conf</span><br><span class="line">/usr/bin/pidproxy</span><br><span class="line">/usr/bin/supervisorctl</span><br><span class="line">/usr/bin/supervisord</span><br><span class="line">/usr/lib/systemd/system/supervisord.service</span><br><span class="line">/var/log/supervisor</span><br><span class="line">/var/run/supervisor</span><br></pre></td></tr></table></figure><h4 id="完整流程"><a href="#完整流程" class="headerlink" title="完整流程"></a>完整流程</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -ti -w /test --entrypoint bash centos:7</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">python3 会自带 python3-pip ， binutils 是 pyinstaller 打包的时候依赖</span></span><br><span class="line">yum install -y python3 binutils gcc zlib-devel which</span><br><span class="line"></span><br><span class="line">pip3 install wheel</span><br><span class="line">pip3 install pyinstaller</span><br><span class="line">pip3 install supervisor</span><br><span class="line"></span><br><span class="line">ver=$(find /usr/ -type f -name &#x27;version.txt&#x27; -path &#x27;*/supervisor/*&#x27; -exec cat &#123;&#125; \; )</span><br><span class="line">op_file=$(find /usr/ -type f -name &#x27;options.py&#x27; -path &#x27;*/supervisor/*&#x27;)</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置 VERSION</span></span><br><span class="line">sed -ri &#x27;/^VERSION\s+=\s+/s#= .+#= &quot;&#x27;&quot;$&#123;ver&#125;&quot;&#x27;&quot;#&#x27; $op_file</span><br><span class="line"></span><br><span class="line">dir=$(find /usr -type d -name supervisor -path &#x27;*/site-packages/*&#x27;  -exec dirname &#123;&#125; \;)</span><br><span class="line">pyinstaller --onefile -p $dir `which pidproxy`</span><br><span class="line">pyinstaller --onefile -p $dir `which supervisord`</span><br><span class="line">pyinstaller --onefile -p $dir `which supervisorctl`</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="制作-rpm-包"><a href="#制作-rpm-包" class="headerlink" title="制作 rpm 包"></a>制作 rpm 包</h3><p>其实这个步骤也可以用于容器内无 python 添加 supervisor<br>相关步骤和文件存放在我 github 上<br><a href="https://github.com/zhangguanzhang/compile-and-packages">https://github.com/zhangguanzhang/compile-and-packages</a></p><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>歇逼了，部署上去无法运行，运行报错:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error while loading shared libraries: libz.so.1: failed to map segment from shared object</span><br></pre></td></tr></table></figure><p>设置 <code>TMPDIR</code> 啥的都不行，用 staticx 打包成静态依赖也不行，现在取消 <code>--onefile</code> 参数打包整个目录 + 软连接能运行</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://rhel.pkgs.org/7/epel-aarch64/supervisor-3.1.4-1.el7.noarch.rpm.html">https://rhel.pkgs.org/7/epel-aarch64/supervisor-3.1.4-1.el7.noarch.rpm.html</a></li><li><a href="https://src.fedoraproject.org/rpms/supervisor/blob/epel7/f/supervisor.spec">https://src.fedoraproject.org/rpms/supervisor/blob/epel7/f/supervisor.spec</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;近期的一次信创适配，需要 supervisor 离线安装&lt;/p&gt;</summary>
    
    
    
    
    <category term="python" scheme="http://zhangguanzhang.github.io/tags/python/"/>
    
    <category term="supervisor" scheme="http://zhangguanzhang.github.io/tags/supervisor/"/>
    
    <category term="pyinstaller" scheme="http://zhangguanzhang.github.io/tags/pyinstaller/"/>
    
  </entry>
  
  <entry>
    <title>磁盘问题宕机后 docker 的 containerd 无法启动</title>
    <link href="http://zhangguanzhang.github.io/2022/10/09/docker-containerd-panic/"/>
    <id>http://zhangguanzhang.github.io/2022/10/09/docker-containerd-panic/</id>
    <published>2022-10-09T18:10:30.000Z</published>
    <updated>2022-10-09T18:10:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>机器磁盘出问题后，重启后 docker 无法启动，和之前的 <a href="https://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/">docker-panic</a> 一样解决后，发现 containerd 无法启动</p><span id="more"></span><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>包管理安装的 docker，所以会有一个 systemd 纳管的 containerd 进程，解决了 docker 启动 panic 后，containerd 也是启动后就 panic 了，前台调试下 containerd </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl cat containerd</span><br><span class="line">...</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/bin/containerd</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>查看下 containerd 的 log level 参数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ containerd --help</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">   --log-level value, -l value  set the logging level [trace, debug, info, warn, error, fatal, panic]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>前台运行 debug level </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop containerd</span><br><span class="line">$ containerd --log-level debug</span><br><span class="line">INFO[2022-10-09T18:08:06.383235215+08:00] loading plugin &quot;io.containerd.snapshotter.v1.aufs&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.385029947+08:00] skip loading plugin &quot;io.containerd.snapshotter.v1.aufs&quot;...  error=&quot;aufs is not supported (modprobe aufs failed: exit status 1 \&quot;modprobe: FATAL: Module aufs not found.\\n\&quot;): skip plugin&quot; type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.385109752+08:00] loading plugin &quot;io.containerd.snapshotter.v1.devmapper&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">WARN[2022-10-09T18:08:06.385166683+08:00] failed to load plugin io.containerd.snapshotter.v1.devmapper  error=&quot;devmapper not configured&quot;</span><br><span class="line">INFO[2022-10-09T18:08:06.385195079+08:00] loading plugin &quot;io.containerd.snapshotter.v1.native&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.385254847+08:00] loading plugin &quot;io.containerd.snapshotter.v1.overlayfs&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.385399412+08:00] loading plugin &quot;io.containerd.snapshotter.v1.zfs&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.385742371+08:00] skip loading plugin &quot;io.containerd.snapshotter.v1.zfs&quot;...  error=&quot;path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin&quot; type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.385776799+08:00] loading plugin &quot;io.containerd.metadata.v1.bolt&quot;...  type=io.containerd.metadata.v1</span><br><span class="line">WARN[2022-10-09T18:08:06.385799868+08:00] could not use snapshotter devmapper in metadata plugin  error=&quot;devmapper not configured&quot;</span><br><span class="line">INFO[2022-10-09T18:08:06.385811466+08:00] metadata content store policy set             policy=shared</span><br><span class="line">INFO[2022-10-09T18:08:06.385937321+08:00] loading plugin &quot;io.containerd.differ.v1.walking&quot;...  type=io.containerd.differ.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.385958141+08:00] loading plugin &quot;io.containerd.gc.v1.scheduler&quot;...  type=io.containerd.gc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386007407+08:00] loading plugin &quot;io.containerd.service.v1.introspection-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386045975+08:00] loading plugin &quot;io.containerd.service.v1.containers-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386060301+08:00] loading plugin &quot;io.containerd.service.v1.content-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386075847+08:00] loading plugin &quot;io.containerd.service.v1.diff-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386092598+08:00] loading plugin &quot;io.containerd.service.v1.images-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386111730+08:00] loading plugin &quot;io.containerd.service.v1.leases-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386129763+08:00] loading plugin &quot;io.containerd.service.v1.namespaces-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386143081+08:00] loading plugin &quot;io.containerd.service.v1.snapshots-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386156250+08:00] loading plugin &quot;io.containerd.runtime.v1.linux&quot;...  type=io.containerd.runtime.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386204083+08:00] loading plugin &quot;io.containerd.runtime.v2.task&quot;...  type=io.containerd.runtime.v2</span><br><span class="line">INFO[2022-10-09T18:08:06.386258107+08:00] loading plugin &quot;io.containerd.monitor.v1.cgroups&quot;...  type=io.containerd.monitor.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386855490+08:00] loading plugin &quot;io.containerd.service.v1.tasks-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.386949124+08:00] loading plugin &quot;io.containerd.internal.v1.restart&quot;...  type=io.containerd.internal.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387096230+08:00] loading plugin &quot;io.containerd.grpc.v1.containers&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387135843+08:00] loading plugin &quot;io.containerd.grpc.v1.content&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387171618+08:00] loading plugin &quot;io.containerd.grpc.v1.diff&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387200578+08:00] loading plugin &quot;io.containerd.grpc.v1.events&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387229772+08:00] loading plugin &quot;io.containerd.grpc.v1.healthcheck&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387264273+08:00] loading plugin &quot;io.containerd.grpc.v1.images&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387293089+08:00] loading plugin &quot;io.containerd.grpc.v1.leases&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387321593+08:00] loading plugin &quot;io.containerd.grpc.v1.namespaces&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387349820+08:00] loading plugin &quot;io.containerd.internal.v1.opt&quot;...  type=io.containerd.internal.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387426214+08:00] loading plugin &quot;io.containerd.grpc.v1.snapshots&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387460111+08:00] loading plugin &quot;io.containerd.grpc.v1.tasks&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387515152+08:00] loading plugin &quot;io.containerd.grpc.v1.version&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387547837+08:00] loading plugin &quot;io.containerd.grpc.v1.introspection&quot;...  type=io.containerd.grpc.v1</span><br><span class="line">INFO[2022-10-09T18:08:06.387942931+08:00] serving...                                    address=/run/containerd/containerd.sock.ttrpc</span><br><span class="line">INFO[2022-10-09T18:08:06.388045242+08:00] serving...                                    address=/run/containerd/containerd.sock</span><br><span class="line">DEBU[2022-10-09T18:08:06.388086098+08:00] sd notification                               error=&quot;&lt;nil&gt;&quot; notified=false state=&quot;READY=1&quot;</span><br><span class="line">INFO[2022-10-09T18:08:06.388117730+08:00] containerd successfully booted in 0.039426s  </span><br><span class="line">panic: invalid page type: 12: 10</span><br><span class="line"></span><br><span class="line">goroutine 69 [running]:</span><br><span class="line">github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Cursor).search(0xc000530fe0, 0x7f7c3c366040, 0xa, 0xa, 0xc)</span><br><span class="line">        /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/cursor.go:250 +0x355</span><br><span class="line">github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Cursor).searchPage(0xc000530fe0, 0x7f7c3c366040, 0xa, 0xa, 0x7f7c3c36c000)</span><br><span class="line">        /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/cursor.go:308 +0x164</span><br><span class="line">github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Cursor).search(0xc000530fe0, 0x7f7c3c366040, 0xa, 0xa, 0x14)</span><br><span class="line">        /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/cursor.go:265 +0x18c</span><br><span class="line">github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Cursor).seek(0xc000530fe0, 0x7f7c3c366040, 0xa, 0xa, 0xe, 0x556761035f72, 0x18, 0x556761017762, 0x4, 0x556761035f8a, ...)</span><br><span class="line">        /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/cursor.go:159 +0x7f</span><br><span class="line">github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Bucket).Bucket(0xc00051c0c0, 0x7f7c3c366040, 0xa, 0xa, 0x55676102559d)</span><br><span class="line">        /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/bucket.go:105 +0xd6</span><br><span class="line">github.com/containerd/containerd/metadata.scanRoots.func2(0x7f7c3c366040, 0xa, 0xa, 0x0, 0x0, 0x0, 0x0, 0x55675fff4520)</span><br><span class="line">        /go/src/github.com/containerd/containerd/metadata/gc.go:98 +0xba</span><br><span class="line">github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Bucket).ForEach(0xc00051c0c0, 0xc000531658, 0x6, 0x6)</span><br><span class="line">        /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/bucket.go:390 +0x100</span><br><span class="line">github.com/containerd/containerd/metadata.scanRoots(0x556761ae0860, 0xc00051c000, 0xc000518000, 0xc000522000, 0xc000520000, 0x51a000)</span><br><span class="line">        /go/src/github.com/containerd/containerd/metadata/gc.go:94 +0x86c</span><br><span class="line">github.com/containerd/containerd/metadata.(*DB).getMarked.func1(0xc000518000, 0x0, 0x0)</span><br><span class="line">        /go/src/github.com/containerd/containerd/metadata/db.go:384 +0x193</span><br><span class="line">github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*DB).View(0xc0000f4200, 0xc000080850, 0x0, 0x0)</span><br><span class="line">        /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/db.go:725 +0xaa</span><br><span class="line">github.com/containerd/containerd/metadata.(*DB).getMarked(0xc0000d0c40, 0x556761ae08a0, 0xc000040098, 0x203000, 0x203000, 0x0)</span><br><span class="line">        /go/src/github.com/containerd/containerd/metadata/db.go:367 +0x7e</span><br><span class="line">github.com/containerd/containerd/metadata.(*DB).GarbageCollect(0xc0000d0c40, 0x556761ae08a0, 0xc000040098, 0x0, 0x656d2e6472656e01, 0x762e617461646174, 0x746c6f622e31)</span><br><span class="line">        /go/src/github.com/containerd/containerd/metadata/db.go:284 +0xa3</span><br><span class="line">github.com/containerd/containerd/gc/scheduler.(*gcScheduler).run(0xc0000c0b40, 0x556761ae08a0, 0xc000040098)</span><br><span class="line">        /go/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:310 +0x516</span><br><span class="line">created by github.com/containerd/containerd/gc/scheduler.init.0.func1</span><br><span class="line">        /go/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:132 +0x429</span><br></pre></td></tr></table></figure><p>和之前的 docker panic 一样，可以从 panic 的信息确定 containerd 也使用了 boltdb 存储一些信息，从调用链看就是 <code>metadata.(*DB).getMarked</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l /var/lib/containerd/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 4 root root 33 May 26  2021 io.containerd.content.v1.content</span><br><span class="line">drwx--x--x. 2 root root 21 May 26  2021 io.containerd.metadata.v1.bolt</span><br><span class="line">drwx--x--x. 2 root root  6 May 26  2021 io.containerd.runtime.v1.linux</span><br><span class="line">drwx--x--x. 3 root root 18 Jun 18  2021 io.containerd.runtime.v2.task</span><br><span class="line">drwx------. 3 root root 23 May 26  2021 io.containerd.snapshotter.v1.native</span><br><span class="line">drwx------. 3 root root 23 May 26  2021 io.containerd.snapshotter.v1.overlayfs</span><br><span class="line">drwx------. 2 root root  6 May 26  2021 tmpmounts</span><br><span class="line">$ ls -l /var/lib/containerd/io.containerd.metadata.v1.bolt/</span><br><span class="line">total 264</span><br><span class="line">-rw-r--r--. 1 root root 270336 Oct  9 15:01 meta.db</span><br></pre></td></tr></table></figure><p>改名 meta.db 启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ mv /var/lib/containerd/io.containerd.metadata.v1.bolt/meta.db&#123;,.bak&#125;</span><br><span class="line">$ systemctl start containerd</span><br><span class="line">$ systemctl status containerd</span><br><span class="line">● containerd.service - containerd container runtime</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/containerd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Sun 2022-10-09 18:08:36 CST; 10s ago</span><br><span class="line">     Docs: https://containerd.io</span><br><span class="line">  Process: 10573 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 10576 (containerd)</span><br><span class="line">    Tasks: 14</span><br><span class="line">   Memory: 27.5M</span><br><span class="line">   CGroup: /system.slice/containerd.service</span><br><span class="line">           └─10576 /usr/bin/containerd</span><br><span class="line"></span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664056264+08:00&quot; level=info msg=&quot;loading plugin \&quot;io.containerd.grpc.v1.leases\&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664069435+08:00&quot; level=info msg=&quot;loading plugin \&quot;io.containerd.grpc.v1.namespaces\&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664082887+08:00&quot; level=info msg=&quot;loading plugin \&quot;io.containerd.internal.v1.opt\&quot;...&quot; type=io.containerd.internal.v1</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664135587+08:00&quot; level=info msg=&quot;loading plugin \&quot;io.containerd.grpc.v1.snapshots\&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664152107+08:00&quot; level=info msg=&quot;loading plugin \&quot;io.containerd.grpc.v1.tasks\&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664167475+08:00&quot; level=info msg=&quot;loading plugin \&quot;io.containerd.grpc.v1.version\&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664183225+08:00&quot; level=info msg=&quot;loading plugin \&quot;io.containerd.grpc.v1.introspection\&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664386986+08:00&quot; level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664433173+08:00&quot; level=info msg=serving... address=/run/containerd/containerd.sock</span><br><span class="line">Oct 09 18:08:36 localhost.localdomain containerd[10576]: time=&quot;2022-10-09T18:08:36.664508047+08:00&quot; level=info msg=&quot;containerd successfully booted in 0.048852s&quot;</span><br></pre></td></tr></table></figure><p>没啥技术含量，写出来是给人看看排错过程和思路</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;机器磁盘出问题后，重启后 docker 无法启动，和之前的 &lt;a href=&quot;https://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/&quot;&gt;docker-panic&lt;/a&gt; 一样解决后，发现 containerd 无法启动&lt;/p&gt;</summary>
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="containerd" scheme="http://zhangguanzhang.github.io/tags/containerd/"/>
    
    <category term="panic" scheme="http://zhangguanzhang.github.io/tags/panic/"/>
    
  </entry>
  
  <entry>
    <title>低版本docker veth没清理造成容器网络问题</title>
    <link href="http://zhangguanzhang.github.io/2022/09/07/docker-veth-not-clean/"/>
    <id>http://zhangguanzhang.github.io/2022/09/07/docker-veth-not-clean/</id>
    <published>2022-09-07T11:10:30.000Z</published>
    <updated>2022-09-07T11:10:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>帮助同事看一个 gitlab-runner 的问题，最终发现是 docker 低版本的 bug，没清理掉 veth 导致的</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>同事 <code>gitlab-ci.yml</code> 大概内容为下面：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">include:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">project:</span> <span class="string">&quot;xxx/ci-template&quot;</span></span><br><span class="line">    <span class="attr">file:</span> <span class="string">&quot;/backend/common_mini.yml&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">test:</span></span><br><span class="line">  <span class="attr">services:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">minio/minio</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;server&quot;</span>,<span class="string">&quot;/data&quot;</span>]</span><br><span class="line">      <span class="attr">alias:</span> <span class="string">minio</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql:5.7.17</span></span><br><span class="line">  <span class="attr">variables:</span></span><br><span class="line">    <span class="attr">FILTER_COVER_PACKAGES:</span> <span class="string">&quot;grep -E &#x27;impl&#x27;&quot;</span></span><br><span class="line">    <span class="attr">MYSQL_DATABASE:</span> <span class="string">&quot;docmini&quot;</span></span><br><span class="line">    <span class="attr">MINIO_UPODATE:</span> <span class="string">&quot;off&quot;</span></span><br></pre></td></tr></table></figure><p>使用的是 <a href="https://docs.gitlab.com/runner/executors/docker.html">Docker executor</a> 去跑的构建，他反馈说构建容器内部无法访问 minio 的 9000 端口，我大致看了下，发现 service 的 alias 实际上用的是 docker run 的 –link 实现的，也就是容器的 hosts 文件里添加记录指向容器IP，官方文档 <a href="https://docs.gitlab.com/runner/executors/docker.html#the-services-keyword">service 字段</a> 也是如此说明</p><h2 id="排错过程"><a href="#排错过程" class="headerlink" title="排错过程"></a>排错过程</h2><h3 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h3><p>构建过程是 <code>7a9ff9c8ee95</code> 无法访问 minio 里的 9000 端口，直接用 ip，不用 alias 别名都无法访问</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                      PORTS               NAMES</span><br><span class="line">7a9ff9c8ee95        83daaac121e6        &quot;sh -c &#x27;if [ -x /usr…&quot;   41 seconds ago       Up 40 seconds                                   runner-s4f3kt7-project-15927-concurrent-0-eed4eb59d5dfa521-build-2</span><br><span class="line">883faff17a2d        e59a4655709b        &quot;/usr/bin/dumb-init …&quot;   42 seconds ago       Exited (0) 41 seconds ago                       runner-s4f3kt7-project-15927-concurrent-0-eed4eb59d5dfa521-predefined-1</span><br><span class="line">2662fd58cad5        e59a4655709b        &quot;/usr/bin/dumb-init …&quot;   43 seconds ago       Exited (0) 42 seconds ago                       runner-s4f3kt7-project-15927-concurrent-0-eed4eb59d5dfa521-predefined-0</span><br><span class="line">139e1a01b50f        9546ca122d3a        &quot;docker-entrypoint.s…&quot;   About a minute ago   Up About a minute           3306/tcp            runner-s4f3kt7-project-15927-concurrent-0-eed4eb59d5dfa521-mysql-1</span><br><span class="line">de4647deead4        c15374551d3a        &quot;/usr/bin/docker-ent…&quot;   About a minute ago   Up About a minute           9000/tcp            runner-s4f3kt7-project-15927-concurrent-0-eed4eb59d5dfa521-minio__minio-0</span><br><span class="line">$ docker inspect de46 | grep -i pid</span><br><span class="line">            &quot;Pid&quot;: 23888,</span><br><span class="line">            &quot;PidMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;PidsLimit&quot;: 0,</span><br><span class="line">$ nsenter --net -t 23888 curl 172.25.0.2:9000</span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Access Denied.&lt;/Message&gt;&lt;Resource&gt;/&lt;/Resource&gt;&lt;RequestId&gt;17127033A4D00006&lt;/RequestId&gt;&lt;HostId&gt;ec1fb8ef-f0f0-488e-a71e-da444933f2ed&lt;/HostId&gt;&lt;/Error&gt;</span><br><span class="line">$ docker exec -ti 7a9ff9c8ee95 curl 172.25.0.2:9000</span><br><span class="line">curl: (7) Failed to connect to 172.25.0.2 port 9000: Connection refused</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>看了下 iptables 和转发参数都没有任何问题，发现宿主机上都无法访问，只有该容器内部才可以访问</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ curl 172.25.0.2:9000</span><br><span class="line">curl: (7) Failed to connect to 172.25.0.2 port 9000: Connection refused</span><br><span class="line">$ nsenter --net -t 23888 curl 172.25.0.2:9000</span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Access Denied.&lt;/Message&gt;&lt;Resource&gt;/&lt;/Resource&gt;&lt;RequestId&gt;17127033A4D00006&lt;/RequestId&gt;&lt;HostId&gt;ec1fb8ef-f0f0-488e-a71e-da444933f2ed&lt;/HostId&gt;&lt;/Error&gt;</span><br></pre></td></tr></table></figure><p>为了排除 minio 服务问题，清理掉上面的容器后，用官方的 nginx 镜像测试下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">$ docker run -d --name t1 --rm -p 81:80 nginx:alpine</span><br><span class="line">91fa481376cbbbdf04dd7ed027048ad20f40eee18f4e7d916d9edba8da102412</span><br><span class="line">$ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS                NAMES</span><br><span class="line">91fa481376cb        nginx:alpine        &quot;/docker-entrypoint.…&quot;   About a minute ago   Up About a minute   0.0.0.0:81-&gt;80/tcp   t1</span><br><span class="line">$ docker inspect t1 | grep IPAddress</span><br><span class="line">     &quot;IPAddress&quot;: &quot;172.25.0.2&quot;,</span><br></pre></td></tr></table></figure><p>发现访问还是有问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl 172.25.0.2</span><br><span class="line">curl: (7) Failed to connect to 172.25.0.2 port 80: Connection refused</span><br><span class="line">$ ip link set docker0 promisc on</span><br><span class="line">$ curl 172.25.0.2</span><br><span class="line">curl: (7) Failed to connect to 172.25.0.2 port 80: Connection refused</span><br><span class="line"></span><br><span class="line">$ curl localhost:81</span><br><span class="line">curl: (56) Recv failure: Connection reset by peer</span><br></pre></td></tr></table></figure><p>清理掉容器后发现 veth 不对：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ ip a s </span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 00:50:56:87:52:b5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.xx.48.239/23 brd 10.xx.49.255 scope global ens160</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::250:56ff:fe87:52b5/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: docker0: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 02:42:c0:7b:8b:bf brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.25.0.1/16 brd 172.25.255.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:c0ff:fe7b:8bbf/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9005: veth4a669ee@if9004: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default </span><br><span class="line">    link/ether aa:ad:04:ea:b9:a1 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet6 fe80::a8ad:4ff:feea:b9a1/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9007: vethe0ddac0@if9006: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default </span><br><span class="line">    link/ether 02:9c:d5:3d:18:8f brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">    inet6 fe80::9c:d5ff:fe3d:188f/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>一个容器都没有后，不应该有上面的 veth，安装 bridge-utils 查看下果然残留了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install -y bridge-utils </span><br><span class="line">$ brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">docker08000.0242c07b8bbfnoveth4a669ee</span><br><span class="line">vethe0ddac0</span><br></pre></td></tr></table></figure><p>因为 docker 容器分配 IP 是从前到后的，验证残留造成的可以多起几个容器，后续容器能访问就确定是 veth 没清理导致的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d  --rm -p 81:80 nginx:alpine</span><br><span class="line">a0952e42f6a0da9d1969b327e696022c2dea041061cee2fbf080134037c9c93b</span><br><span class="line">$ docker run -d  --rm -p 82:80 nginx:alpine</span><br><span class="line">dcf7d4635f1379b321603760c71a94bf70b1b954bb5528d384f7f9d38d4ed005</span><br><span class="line">$ docker run -d  --rm -p 83:80 nginx:alpine</span><br><span class="line">44d5611125bb773ecc24baf760d4ba19f90a33c0b5c802e6afbeee462f200df0</span><br><span class="line">$ docker run -d  --rm -p 84:80 nginx:alpine</span><br><span class="line">145b99fea102f71bc99132f2ae5aa8401f890df5d10177964df3e3f4e3fd8281</span><br><span class="line">$ curl localhost:82</span><br><span class="line">curl: (56) Recv failure: Connection reset by peer</span><br><span class="line">$ curl localhost:83</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">html &#123; color-scheme: light dark; &#125;</span><br><span class="line">body &#123; width: 35em; margin: 0 auto;</span><br><span class="line">font-family: Tahoma, Verdana, Arial, sans-serif; &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>果然是残留导致的，然后看了下 docker 版本很低：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Server Version: 18.03.0-ce</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: extfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: cfd04396dc68220d1cecbe686a6cc3aa5ce3667c</span><br><span class="line">runc version: N/A (expected: 4fc53a81fb7c994640722ac585fa9ca548971871)</span><br><span class="line">init version: N/A (expected: )</span><br><span class="line">Security Options:</span><br><span class="line"> apparmor</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 4.4.0-184-generic</span><br><span class="line">Operating System: Ubuntu 16.04.6 LTS</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br></pre></td></tr></table></figure><p>搜了下 <code>docker bridge network veth not clean up</code> 发现很多人遇到了，属于低版本的 bug，卸载用官方脚本安装后就正常了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL &quot;https://get.docker.com/&quot; | bash -s -- --mirror Aliyun</span><br></pre></td></tr></table></figure><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul><li><a href="https://gitlab.com/gitlab-org/gitlab-runner/-/issues/27570">Support variable in services:*:alias</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;帮助同事看一个 gitlab-runner 的问题，最终发现是 docker 低版本的 bug，没清理掉 veth 导致的&lt;/p&gt;</summary>
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="veth" scheme="http://zhangguanzhang.github.io/tags/veth/"/>
    
  </entry>
  
  <entry>
    <title>exsi 使用 redhat8.4 搭建k8s集群，flannel vxlan 模式的不正常排错</title>
    <link href="http://zhangguanzhang.github.io/2022/07/28/redhat84-vxlan-esxi/"/>
    <id>http://zhangguanzhang.github.io/2022/07/28/redhat84-vxlan-esxi/</id>
    <published>2022-07-28T19:18:30.000Z</published>
    <updated>2022-07-28T19:18:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>这几天处理的一个问题，exsi 上 redhat8.4 搭建 k8s 环境，使用 flannel vxlan 模式，pod 的网络表现得非常异常</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>测试使用我们的工具部署，发现部署中有问题，无法顺利部署完，看到我负责的 etcd 有问题，我就上去看看</p><h2 id="排错过程"><a href="#排错过程" class="headerlink" title="排错过程"></a>排错过程</h2><h3 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h3><p>etcd 的 pod 信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -o wide | grep etcd</span><br><span class="line">etcd1-10.xx.xx.188              1/1     Running            0          27m   172.27.2.11    10.xx.xx.188   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd2-10.xx.xx.201              1/1     Running            0          27m   172.27.1.11    10.xx.xx.201   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd3-10.xx.xx.208              1/1     Running            0          27m   172.27.0.12    10.xx.xx.208   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>看了下三个 etcd 的日志，报错无法连到其他的 etcd ，看下他们的互相通信，执行命令的主机是在 188 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl 172.27.2.11:2380</span><br><span class="line">404 page not found</span><br><span class="line">$ curl 172.27.1.11:2380</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>很奇怪，跨节点不通，但是更奇怪的是可以 ping 通非本机的 pod ip：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ping 172.27.1.11 </span><br><span class="line">PING 172.27.1.11 (172.27.1.11) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.27.1.11: icmp_seq=1 ttl=63 time=0.534 ms</span><br><span class="line">64 bytes from 172.27.1.11: icmp_seq=2 ttl=63 time=0.464 ms</span><br><span class="line">^C</span><br><span class="line">--- 172.27.1.11 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1030ms</span><br></pre></td></tr></table></figure><h3 id="抓包现象"><a href="#抓包现象" class="headerlink" title="抓包现象"></a>抓包现象</h3><p>其实这个就已经很奇怪了，因为无论 ping 还是发应用层请求到另一个节点上的 pod，都会走 flannel 的 vxlan 封包的，不应该 icmp 通，而 curl 不通。</p><p>下面是在 201 上抓包， icmp 能抓到， curl 的抓不到：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i flannel.1  host 172.27.1.11     </span><br><span class="line">tcpdump: verbose output suppressed, use -v[v]... for full protocol decode</span><br><span class="line">listening on flannel.1, link-type EN10MB (Ethernet), snapshot length 262144 bytes</span><br><span class="line">03:52:49.136420 IP 172.27.2.0 &gt; 172.27.1.11: ICMP echo request, id 30780, seq 1, length 64</span><br><span class="line">03:52:49.136586 IP 172.27.1.11 &gt; 172.27.2.0: ICMP echo reply, id 30780, seq 1, length 64</span><br><span class="line">03:52:50.165997 IP 172.27.2.0 &gt; 172.27.1.11: ICMP echo request, id 30780, seq 2, length 64</span><br><span class="line">03:52:50.166129 IP 172.27.1.11 &gt; 172.27.2.0: ICMP echo reply, id 30780, seq 2, length 64</span><br></pre></td></tr></table></figure><h3 id="错误的尝试"><a href="#错误的尝试" class="headerlink" title="错误的尝试"></a>错误的尝试</h3><p>虽然最终解决了，但是还是按照时间线写下我的处理过程，看下 iptables 规则：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -S</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P FORWARD ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-N LIBVIRT_INP</span><br><span class="line">-N LIBVIRT_OUT</span><br><span class="line">-N LIBVIRT_FWO</span><br><span class="line">-N LIBVIRT_FWI</span><br><span class="line">-N LIBVIRT_FWX</span><br><span class="line">-N DOCKER</span><br><span class="line">-N DOCKER-ISOLATION-STAGE-1</span><br><span class="line">-N DOCKER-ISOLATION-STAGE-2</span><br><span class="line">-N DOCKER-USER</span><br><span class="line">-N KUBE-PROXY-CANARY</span><br><span class="line">-N KUBE-EXTERNAL-SERVICES</span><br><span class="line">-N KUBE-SERVICES</span><br><span class="line">-N KUBE-FORWARD</span><br><span class="line">-N KUBE-FIREWALL</span><br><span class="line">-N KUBE-KUBELET-CANARY</span><br><span class="line">-A INPUT -j KUBE-FIREWALL</span><br><span class="line">-A INPUT -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes externally-visible service portals&quot; -j KUBE-EXTERNAL-SERVICES</span><br><span class="line">-A INPUT -j LIBVIRT_INP</span><br><span class="line">-A INPUT -i cni0 -j ACCEPT</span><br><span class="line">-A FORWARD -m comment --comment &quot;kubernetes forwarding rules&quot; -j KUBE-FORWARD</span><br><span class="line">-A FORWARD -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line">-A FORWARD -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes externally-visible service portals&quot; -j KUBE-EXTERNAL-SERVICES</span><br><span class="line">-A FORWARD -j DOCKER-USER</span><br><span class="line">-A FORWARD -j DOCKER-ISOLATION-STAGE-1</span><br><span class="line">-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A FORWARD -o docker0 -j DOCKER</span><br><span class="line">-A FORWARD -i docker0 ! -o docker0 -j ACCEPT</span><br><span class="line">-A FORWARD -i docker0 -o docker0 -j ACCEPT</span><br><span class="line">-A FORWARD -j LIBVIRT_FWX</span><br><span class="line">-A FORWARD -j LIBVIRT_FWI</span><br><span class="line">-A FORWARD -j LIBVIRT_FWO</span><br><span class="line">-A FORWARD -j ACCEPT</span><br><span class="line">-A OUTPUT -j KUBE-FIREWALL</span><br><span class="line">-A OUTPUT -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line">-A OUTPUT -j LIBVIRT_OUT</span><br><span class="line">-A LIBVIRT_INP -i virbr0 -p udp -m udp --dport 53 -j ACCEPT</span><br><span class="line">-A LIBVIRT_INP -i virbr0 -p tcp -m tcp --dport 53 -j ACCEPT</span><br><span class="line">-A LIBVIRT_INP -i virbr0 -p udp -m udp --dport 67 -j ACCEPT</span><br><span class="line">-A LIBVIRT_INP -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT</span><br><span class="line">-A LIBVIRT_OUT -o virbr0 -p udp -m udp --dport 53 -j ACCEPT</span><br><span class="line">-A LIBVIRT_OUT -o virbr0 -p tcp -m tcp --dport 53 -j ACCEPT</span><br><span class="line">-A LIBVIRT_OUT -o virbr0 -p udp -m udp --dport 68 -j ACCEPT</span><br><span class="line">-A LIBVIRT_OUT -o virbr0 -p tcp -m tcp --dport 68 -j ACCEPT</span><br><span class="line">-A LIBVIRT_FWO -s 192.168.122.0/24 -i virbr0 -j ACCEPT</span><br><span class="line">-A LIBVIRT_FWO -i virbr0 -j REJECT --reject-with icmp-port-unreachable</span><br><span class="line">-A LIBVIRT_FWI -d 192.168.122.0/24 -o virbr0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A LIBVIRT_FWI -o virbr0 -j REJECT --reject-with icmp-port-unreachable</span><br><span class="line">-A LIBVIRT_FWX -i virbr0 -o virbr0 -j ACCEPT</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-1 -j RETURN</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-2 -j RETURN</span><br><span class="line">-A DOCKER-USER -j RETURN</span><br><span class="line">-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP</span><br><span class="line">-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding rules&quot; -m mark --mark 0x4000/0x4000 -j ACCEPT</span><br><span class="line">-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding conntrack pod source rule&quot; -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding conntrack pod destination rule&quot; -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment &quot;block incoming localnet connections&quot; -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP</span><br><span class="line"># Warning: iptables-legacy tables present, use iptables-legacy to see them</span><br></pre></td></tr></table></figure><p>看到 iptables 里有 libvirt 的规则，问了下，这个机器是安装的过程中选的 <code>Server with GUI</code>，一般 GUI 的 centos 也会带 libvirt 相关的包，先尝试关闭所有节点上的服务：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable --now \</span><br><span class="line">    libvirtd-admin.socket \</span><br><span class="line">    libvirtd-ro.socket \</span><br><span class="line">    libvirtd.socket \</span><br><span class="line">    libvirtd</span><br></pre></td></tr></table></figure><p>重启后看看规则：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -S</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P FORWARD ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-N KUBE-PROXY-CANARY</span><br><span class="line">-N DOCKER</span><br><span class="line">-N DOCKER-ISOLATION-STAGE-1</span><br><span class="line">-N DOCKER-ISOLATION-STAGE-2</span><br><span class="line">-N DOCKER-USER</span><br><span class="line">-N KUBE-EXTERNAL-SERVICES</span><br><span class="line">-N KUBE-SERVICES</span><br><span class="line">-N KUBE-FORWARD</span><br><span class="line">-N KUBE-FIREWALL</span><br><span class="line">-N KUBE-KUBELET-CANARY</span><br><span class="line">-A INPUT -j KUBE-FIREWALL</span><br><span class="line">-A INPUT -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes externally-visible service portals&quot; -j KUBE-EXTERNAL-SERVICES</span><br><span class="line">-A INPUT -i cni0 -j ACCEPT</span><br><span class="line">-A FORWARD -m comment --comment &quot;kubernetes forwarding rules&quot; -j KUBE-FORWARD</span><br><span class="line">-A FORWARD -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line">-A FORWARD -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes externally-visible service portals&quot; -j KUBE-EXTERNAL-SERVICES</span><br><span class="line">-A FORWARD -j DOCKER-USER</span><br><span class="line">-A FORWARD -j DOCKER-ISOLATION-STAGE-1</span><br><span class="line">-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A FORWARD -o docker0 -j DOCKER</span><br><span class="line">-A FORWARD -i docker0 ! -o docker0 -j ACCEPT</span><br><span class="line">-A FORWARD -i docker0 -o docker0 -j ACCEPT</span><br><span class="line">-A FORWARD -j ACCEPT</span><br><span class="line">-A OUTPUT -j KUBE-FIREWALL</span><br><span class="line">-A OUTPUT -m conntrack --ctstate NEW -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-1 -j RETURN</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP</span><br><span class="line">-A DOCKER-ISOLATION-STAGE-2 -j RETURN</span><br><span class="line">-A DOCKER-USER -j RETURN</span><br><span class="line">-A KUBE-FORWARD -m conntrack --ctstate INVALID -j DROP</span><br><span class="line">-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding rules&quot; -m mark --mark 0x4000/0x4000 -j ACCEPT</span><br><span class="line">-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding conntrack pod source rule&quot; -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A KUBE-FORWARD -m comment --comment &quot;kubernetes forwarding conntrack pod destination rule&quot; -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment &quot;block incoming localnet connections&quot; -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP</span><br><span class="line"># Warning: iptables-legacy tables present, use iptables-legacy to see them</span><br></pre></td></tr></table></figure><p>之前没注意看结尾的 <code>Warning</code>，redhat 8 换成了 <code>nf_tables</code> 了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -V</span><br><span class="line">iptables v1.8.4 (nf_tables)</span><br></pre></td></tr></table></figure><p>想看下是不是 <code>legacy</code> 的 iptables 规则影响了，但是机器上并没有 <code>iptables-legacy</code> 命令，rpmfind 的网站下也没找到相关的 rpm 包，然后突发奇想，拉下 kube-proxy 的镜像看看 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull registry.aliyuncs.com/k8sxio/kube-proxy:v1.21.11</span><br><span class="line">v1.21.11: Pulling from k8sxio/kube-proxy</span><br><span class="line">20b09fbd3037: Pull complete </span><br><span class="line">89906a4ae339: Pull complete </span><br><span class="line">Digest: sha256:2dde58797be0da1f63ba386016c3b11d4447cfbf9b9bad9b72763ea24d9016f3</span><br><span class="line">Status: Downloaded newer image for registry.aliyuncs.com/k8sxio/kube-proxy:v1.21.11</span><br><span class="line">registry.aliyuncs.com/k8sxio/kube-proxy:v1.21.11</span><br><span class="line">$ docker run --rm -ti --privileged -v /run/xtables.lock:/run/xtables.lock \</span><br><span class="line">    -v /lib/modules:/lib/modules \</span><br><span class="line">    registry.aliyuncs.com/k8sxio/kube-proxy:v1.21.11 sh</span><br><span class="line"># iptables -V</span><br><span class="line">iptables v1.8.5 (legacy)</span><br><span class="line"># iptables -S</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P FORWARD ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line"># iptables -t nat -S</span><br><span class="line">-P PREROUTING ACCEPT</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-P POSTROUTING ACCEPT</span><br><span class="line"># exit</span><br></pre></td></tr></table></figure><p>规则也是对的，使用 <code>nft list ruleset </code> 看了下 nf tables 的规则也没啥问题，和 iptables 看到的是一样的。</p><h3 id="开始有头绪"><a href="#开始有头绪" class="headerlink" title="开始有头绪"></a>开始有头绪</h3><p>我们之前客户遇到过深信服的超融合和 aCloud 虚拟化平台会使用 8472&#x2F;udp 导致虚机搭建的 k8s 集群使用 flannel 的 vxlan 模式有问题，所以我们的 flannel 的 vxlan 端口改为了 8475 端口了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get cm kube-flannel-cfg -o yaml | grep -PB2 &#x27;847\d\s*$&#x27;</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;,</span><br><span class="line">        &quot;Port&quot;: 8475</span><br></pre></td></tr></table></figure><p>然后我突发奇想的改下端口试试：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system edit cm kube-flannel-cfg</span><br><span class="line">$ kubectl -n kube-system get cm kube-flannel-cfg -o yaml | grep -PB2 &#x27;847\d\s*$&#x27;</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;,</span><br><span class="line">        &quot;Port&quot;: 8472</span><br><span class="line">$ kubectl -n kube-system get pod -o wide -l k8s-app=kube-dns</span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE     IP             NODE           NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-6jwmg                         1/1     Running   2          3h54m   172.27.4.2     10.xx.xx.222   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-bxqx5                         1/1     Running   0          3h54m   172.27.0.2     10.xx.xx.201   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-k84bn                         1/1     Running   0          3h54m   172.27.5.2     10.xx.xx.224   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-pzvgq                         1/1     Running   3          3h54m   172.27.2.2     10.xx.xx.188   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-rrhql                         1/1     Running   1          3h54m   172.27.1.2     10.xx.xx.208   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-s2kn4                         1/1     Running   0          3h54m   172.27.7.2     10.xx.xx.223   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-wh8qc                         1/1     Running   0          3h54m   172.27.6.2     10.xx.xx.225   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-wqpsw                         1/1     Running   0          3h54m   172.27.3.2     10.xx.xx.221   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">$ kubectl -n kube-system delete pod -l app=flannel</span><br><span class="line">pod &quot;kube-flannel-ds-6jjkb&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-6xvwg&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-7kpvl&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-8zzbc&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-hgnzz&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-jcthk&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-l757g&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-mmnh8&quot; deleted</span><br></pre></td></tr></table></figure><p>然后发现 curl 能通( coredns 的 metrics 9153 是 http 应用，平时测跨节点通信的时候去 curl 访问下就行了，毕竟不是每台机器都有 telnet 命令的)，改回 8475 就不通:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ curl 172.27.4.2:9153</span><br><span class="line">404 page not found</span><br><span class="line">$ kubectl -n kube-system edit cm kube-flannel-cfg</span><br><span class="line">configmap/kube-flannel-cfg edited</span><br><span class="line">$ kubectl -n kube-system get cm kube-flannel-cfg -o yaml | grep -PB2 &#x27;847\d\s*$&#x27;</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;,</span><br><span class="line">        &quot;Port&quot;: 8475</span><br><span class="line">$ kubectl -n kube-system delete pod -l app=flannel</span><br><span class="line">pod &quot;kube-flannel-ds-2622n&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-2zssl&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-85kbs&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-89qp9&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-c6qgm&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-dtxzv&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-kjldq&quot; deleted</span><br><span class="line">pod &quot;kube-flannel-ds-ntfbs&quot; deleted</span><br><span class="line">$ curl 172.27.5.2:9153</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>验证了下，发现非 8472 的端口就不行，搜了下相关关键字 <code>redhat 8 vxlan exsi</code>，搜到红帽的一个文章 vxlan 的使用，给的端口也是 8472 端口。</p><p>我们加了 NetworkManager 的子配置文件不让 NetworkManager 管理 flannel.1 接口，尝试纳管后，调整 flannel 的 vxlan 端口，对比了下属性，没啥大致区别：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">$ nmcli conn show flannel.1</span><br><span class="line">vxlan.parent:                           ens192</span><br><span class="line">vxlan.id:                               1</span><br><span class="line">vxlan.local:                            10.xx.xx.222</span><br><span class="line">vxlan.remote:                           --</span><br><span class="line">vxlan.source-port-min:                  0</span><br><span class="line">vxlan.source-port-max:                  0</span><br><span class="line">vxlan.destination-port:                 8475</span><br><span class="line">vxlan.tos:                              0</span><br><span class="line">vxlan.ttl:                              0</span><br><span class="line">vxlan.ageing:                           300</span><br><span class="line">vxlan.limit:                            0</span><br><span class="line">vxlan.learning:                         no</span><br><span class="line">vxlan.proxy:                            no</span><br><span class="line">vxlan.rsc:                              no</span><br><span class="line">vxlan.l2-miss:                          no</span><br><span class="line">vxlan.l3-miss:                          no</span><br><span class="line">$ nmcli conn show flannel.1 &gt; 8475.txt</span><br><span class="line">$ nmcli conn show flannel.1 &gt; 8472.txt</span><br><span class="line">$ diff 847*.txt</span><br><span class="line">2c2</span><br><span class="line">&lt; connection.uuid:                        7da6bb14-a551-49e9-affe-2569dc04c800</span><br><span class="line">---</span><br><span class="line">&gt; connection.uuid:                        1af65851-a9a5-4899-816e-f8c064882643</span><br><span class="line">11c11</span><br><span class="line">&lt; connection.timestamp:                   1658910968</span><br><span class="line">---</span><br><span class="line">&gt; connection.timestamp:                   1658910776</span><br><span class="line">81c81</span><br><span class="line">&lt; vxlan.destination-port:                 8472</span><br><span class="line">---</span><br><span class="line">&gt; vxlan.destination-port:                 8475</span><br><span class="line">96c96</span><br><span class="line">&lt; GENERAL.UUID:                           7da6bb14-a551-49e9-affe-2569dc04c800</span><br><span class="line">---</span><br><span class="line">&gt; GENERAL.UUID:                           1af65851-a9a5-4899-816e-f8c064882643</span><br><span class="line">104,105c104,105</span><br><span class="line">&lt; GENERAL.DBUS-PATH:                      /org/freedesktop/NetworkManager/ActiveConnection/3</span><br><span class="line">&lt; GENERAL.CON-PATH:                       /org/freedesktop/NetworkManager/Settings/3</span><br><span class="line">---</span><br><span class="line">&gt; GENERAL.DBUS-PATH:                      /org/freedesktop/NetworkManager/ActiveConnection/2</span><br><span class="line">&gt; GENERAL.CON-PATH:                       /org/freedesktop/NetworkManager/Settings/2</span><br><span class="line">110,117c110,117</span><br><span class="line">&lt; IP4.ROUTE[1]:                           dst = 172.27.3.0/24, nh = 172.27.3.0, mt = 0</span><br><span class="line">&lt; IP4.ROUTE[2]:                           dst = 172.27.7.0/24, nh = 172.27.7.0, mt = 0</span><br><span class="line">&lt; IP4.ROUTE[3]:                           dst = 172.27.5.0/24, nh = 172.27.5.0, mt = 0</span><br><span class="line">&lt; IP4.ROUTE[4]:                           dst = 172.27.6.0/24, nh = 172.27.6.0, mt = 0</span><br><span class="line">&lt; IP4.ROUTE[5]:                           dst = 172.27.2.0/24, nh = 172.27.2.0, mt = 0</span><br><span class="line">&lt; IP4.ROUTE[6]:                           dst = 172.27.0.0/24, nh = 172.27.0.0, mt = 0</span><br><span class="line">&lt; IP4.ROUTE[7]:                           dst = 172.27.1.0/24, nh = 172.27.1.0, mt = 0</span><br><span class="line">&lt; IP6.ADDRESS[1]:                         fe80::f8a4:b9ff:fe5d:6c9d/64</span><br><span class="line">---</span><br><span class="line">&gt; IP4.ROUTE[1]:                           dst = 172.27.0.0/24, nh = 172.27.0.0, mt = 0</span><br><span class="line">&gt; IP4.ROUTE[2]:                           dst = 172.27.1.0/24, nh = 172.27.1.0, mt = 0</span><br><span class="line">&gt; IP4.ROUTE[3]:                           dst = 172.27.2.0/24, nh = 172.27.2.0, mt = 0</span><br><span class="line">&gt; IP4.ROUTE[4]:                           dst = 172.27.3.0/24, nh = 172.27.3.0, mt = 0</span><br><span class="line">&gt; IP4.ROUTE[5]:                           dst = 172.27.5.0/24, nh = 172.27.5.0, mt = 0</span><br><span class="line">&gt; IP4.ROUTE[6]:                           dst = 172.27.6.0/24, nh = 172.27.6.0, mt = 0</span><br><span class="line">&gt; IP4.ROUTE[7]:                           dst = 172.27.7.0/24, nh = 172.27.7.0, mt = 0</span><br><span class="line">&gt; IP6.ADDRESS[1]:                         fe80::60f1:eeff:fe77:1633/64</span><br><span class="line">119,120c119,120</span><br><span class="line">&lt; IP6.ROUTE[1]:                           dst = ff00::/8, nh = ::, mt = 256, table=255</span><br><span class="line">&lt; IP6.ROUTE[2]:                           dst = fe80::/64, nh = ::, mt = 256</span><br><span class="line">---</span><br><span class="line">&gt; IP6.ROUTE[1]:                           dst = fe80::/64, nh = ::, mt = 256</span><br><span class="line">&gt; IP6.ROUTE[2]:                           dst = ff00::/8, nh = ::, mt = 256, table=255</span><br></pre></td></tr></table></figure><h3 id="对照组对比"><a href="#对照组对比" class="headerlink" title="对照组对比"></a>对照组对比</h3><p>最开始 redhat8.4 的适配任务应该是分配给我的，我手上在忙其他项目事情，分配给了另一个同事。他说他在武汉的 exsi 上的 redhat8.4 搞的跨节点没问题。</p><p>我上去看了下确实，有问题的这个虚机所在的 exsi 是在珠海，武汉这个虚机是 <code>minimal install</code> 的，然后让几个人一起，搞了个对照组。</p><p>系统的镜像都是一样的 redhat 8.4 ，安装类型为 gui 和 minimal :</p><table><thead><tr><th align="left">类型</th><th align="center">exsi地区+版本</th><th align="center">pod 跨节点通信正常？</th><th align="center">flannel vxlan的端口单独改为8472才通</th></tr></thead><tbody><tr><td align="left">gui</td><td align="center">珠海6.7.0</td><td align="center">x</td><td align="center">yes</td></tr><tr><td align="left">minimal</td><td align="center">珠海6.7.0</td><td align="center">x</td><td align="center">yes</td></tr><tr><td align="left">gui</td><td align="center">武汉6.0.0</td><td align="center">✔</td><td align="center">不需要</td></tr><tr><td align="left">minimal</td><td align="center">武汉6.0.0</td><td align="center">✔</td><td align="center">不需要</td></tr></tbody></table><p>而且四个场景的 vxlan 模块信息是一模一样的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">$ modinfo vxlan</span><br><span class="line">filename:       /lib/modules/4.18.0-305.el8.x86_64/kernel/drivers/net/vxlan.ko.xz</span><br><span class="line">alias:          rtnl-link-vxlan</span><br><span class="line">description:    Driver for VXLAN encapsulated traffic</span><br><span class="line">author:         Stephen Hemminger &lt;stephen@networkplumber.org&gt;</span><br><span class="line">version:        0.1</span><br><span class="line">license:        GPL</span><br><span class="line">rhelversion:    8.4</span><br><span class="line">srcversion:     C4B9CCC8F1BB3F9CDEEDACF</span><br><span class="line">depends:        udp_tunnel,ip6_udp_tunnel</span><br><span class="line">intree:         Y</span><br><span class="line">name:           vxlan</span><br><span class="line">vermagic:       4.18.0-305.el8.x86_64 SMP mod_unload modversions </span><br><span class="line">sig_id:         PKCS#7</span><br><span class="line">signer:         Red Hat Enterprise Linux kernel signing key</span><br><span class="line">sig_key:        0D:85:6D:FE:90:3F:7B:A0:D7:04:19:55:4C:9C:D5:EE:1D:42:8D:B6</span><br><span class="line">sig_hashalgo:   sha256</span><br><span class="line">signature:      93:09:AA:FE:BA:D1:10:CD:12:8F:2A:F9:43:D4:50:36:4F:36:51:0C:</span><br><span class="line">4B:BD:3D:89:65:1F:5D:7E:24:EE:4E:90:8B:38:99:24:EE:0B:31:4F:</span><br><span class="line">E5:DC:57:C8:60:4A:6F:FE:43:27:43:B1:EC:A4:A1:A4:9F:47:65:91:</span><br><span class="line">0C:6D:6D:E0:A8:4C:97:95:75:27:D5:B0:CD:0A:77:40:A9:A6:ED:E6:</span><br><span class="line">C9:72:26:23:07:4D:B7:D3:B8:B9:AF:C5:18:AF:EA:F8:B7:6C:90:B9:</span><br><span class="line">FD:F1:8F:CE:73:A8:1F:92:F2:FA:A7:5E:53:BE:D6:64:55:06:5B:54:</span><br><span class="line">29:DB:E3:2E:CC:DF:CF:1C:7D:DC:53:CB:92:38:BC:42:7D:89:1F:21:</span><br><span class="line">0A:47:07:63:E6:B9:C6:1E:26:C5:4E:B2:9A:9F:DB:0D:86:31:EE:2A:</span><br><span class="line">DA:87:AE:16:AA:6F:0D:B3:11:0B:44:FD:5E:11:82:8E:83:9D:E8:4F:</span><br><span class="line">2E:1B:A9:AC:66:2D:12:11:43:B0:9B:1E:2C:1C:8B:8B:80:B8:16:9B:</span><br><span class="line">8C:A3:C8:73:C5:D7:0F:E7:B5:F7:30:7D:57:CA:CE:74:3C:A2:DB:9F:</span><br><span class="line">D6:ED:F3:A4:EE:D7:D2:FF:F0:46:1E:18:52:92:A5:6E:BA:30:7F:18:</span><br><span class="line">BB:1C:49:A1:03:05:87:A2:6A:FE:07:8A:CE:14:1F:EE:C9:82:84:B4:</span><br><span class="line">CC:2B:2E:BF:21:BF:78:7B:39:01:1C:EE:C4:48:7B:9C:BA:8C:3B:D3:</span><br><span class="line">75:B5:1D:5A:57:9F:C6:FA:D7:2C:C3:30:49:3E:94:FC:1E:C2:5E:AA:</span><br><span class="line">F4:D8:80:46:46:C1:BF:3C:80:54:46:78:5F:4D:A5:93:41:65:CC:E4:</span><br><span class="line">ED:78:0E:28:2A:DF:EE:C4:8E:EF:25:82:9C:28:07:7D:C1:95:AA:AD:</span><br><span class="line">E8:5C:A7:CC:91:22:03:BB:1F:AD:87:E9:AD:E3:DE:4B:6C:33:A2:FD:</span><br><span class="line">15:E3:41:3B:C5:A7:84:89:25:2F:B7:1B:EF:1F:6A:D6:FE:A6:36:D6:</span><br><span class="line">19:1E:0F:06</span><br><span class="line">parm:           udp_port:Destination UDP port (ushort)</span><br><span class="line">parm:           log_ecn_error:Log packets received with corrupted ECN (bool)</span><br></pre></td></tr></table></figure><p>exsi 的 DVS 虚拟交换机开混杂模式了（中间还把几台机器迁移到同一台 exsi 上，可是还是有问题）还是不行，询问了内部的 exsi 人员，他让我看看是不是 NSX 影响的，给我发了个文章：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://kb.vmware.com/s/article/2149996?lang=zh_cn 这个文章说：</span><br><span class="line">从 NSX 6.2.4 开始，默认 VXLAN 端口为 4789，这是由 IANA 分配的标准端口。在 NSX 6.2.4 之前的版本中，默认 VXLAN UDP 端口号为 8472</span><br></pre></td></tr></table></figure><p>对于 NSX 完全懵逼，然后找了个 vmware 的大佬 spark-go 咨询了下，大佬帮我看了下我们的 exsi 上没有 NSX。大佬帮我搜索了下，搜到一个 redhat 发布的 <a href="https://access.redhat.com/errata/RHSA-2021:2570">kernel bug RHSA-2021:2570</a> :</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ESXi][RHEL-8] VMXNET3 v4 causes invalid checksums of inner packets of VXLAN tunnel (BZ#1960702)</span><br></pre></td></tr></table></figure><p>其实昨天我搜素结果里有这个文章的，但是没有点进去看，光去看第一个 redhat 的使用 libvirtd 和 vxlan 的文档了。<code>vmxnet3</code> 是 exsi 给虚机的虚拟网卡，看 bug 描述和内核版本对比了下，就应该是这个问题了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux localhost.localdomain 4.18.0-305.el8.x86_64 #1 SMP Thu Apr 29 08:54:30 EDT 2021 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure><p>但是找不到下载内核的地方，redhat 使用 yum 要注册，中间试过安装 centos 8 的内核后起不来：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ grubby --info DEFAULT</span><br><span class="line">index=0</span><br><span class="line">kernel=&quot;/boot/vmlinuz-4.18.0-348.7.1.el8_5.x86_64&quot;</span><br><span class="line">args=&quot;ro crashkernel=auto resume=/dev/mapper/rhel-swap rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet $tuned_params&quot;</span><br><span class="line">root=&quot;/dev/mapper/rhel-root&quot;</span><br><span class="line">initrd=&quot;/boot/initramfs-4.18.0-348.7.1.el8_5.x86_64.img $tuned_initrd&quot;</span><br><span class="line">title=&quot;CentOS Linux (4.18.0-348.7.1.el8_5.x86_64) 8&quot;</span><br><span class="line">id=&quot;d0b79c361c3d4f728f1f9b86bb54acd5-4.18.0-348.7.1.el8_5.x86_64&quot;</span><br></pre></td></tr></table></figure><p>然后使用简单粗暴的办法，既然触发的 checksum 错误，关闭 flannel.1 的校验看看：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get cm kube-flannel-cfg -o yaml | grep -PB2 &#x27;847\d\s*$&#x27;</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;,</span><br><span class="line">        &quot;Port&quot;: 8475</span><br><span class="line">$ curl 172.27.1.2:9153 </span><br><span class="line">^C</span><br><span class="line">$ /sbin/ethtool -K flannel.1 tx-checksum-ip-generic off</span><br><span class="line">Actual changes:</span><br><span class="line">tx-checksum-ip-generic: off</span><br><span class="line">tx-tcp-segmentation: off [not requested]</span><br><span class="line">tx-tcp-ecn-segmentation: off [not requested]</span><br><span class="line">tx-tcp-mangleid-segmentation: off [not requested]</span><br><span class="line">tx-tcp6-segmentation: off [not requested]</span><br><span class="line">$ curl 172.27.1.2:9153 </span><br><span class="line">404 page not found</span><br></pre></td></tr></table></figure><p>看了下珠海的虚机网络适配器类型就是 <code>VMXNET 3</code> ，武汉的是 <code>E1000</code>。</p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul><li><a href="https://access.redhat.com/errata/RHSA-2021:2570">RHSA-2021:2570</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;这几天处理的一个问题，exsi 上 redhat8.4 搭建 k8s 环境，使用 flannel vxlan 模式，pod 的网络表现得非常异常&lt;/p&gt;</summary>
    
    
    
    
    <category term="exsi" scheme="http://zhangguanzhang.github.io/tags/exsi/"/>
    
    <category term="redhat8.4" scheme="http://zhangguanzhang.github.io/tags/redhat8-4/"/>
    
    <category term="vxlan" scheme="http://zhangguanzhang.github.io/tags/vxlan/"/>
    
  </entry>
  
  <entry>
    <title>k8s pod 没有 IP ，报错 failed to read pod IP from plugin/docker</title>
    <link href="http://zhangguanzhang.github.io/2022/07/12/pod-no-ip-addr/"/>
    <id>http://zhangguanzhang.github.io/2022/07/12/pod-no-ip-addr/</id>
    <published>2022-07-12T19:11:20.000Z</published>
    <updated>2022-07-12T19:11:20.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>有事回到工位上，还没坐下同事就过来喊我，让我帮忙看个客户的生产环境问题，大致就是客户为了搞安全，开了 ipset，然后发现业务受影响了。</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -o wide | grep -v Runn</span><br><span class="line">NAME                                                  READY   STATUS             RESTARTS   AGE     IP            NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">xxxx-privilege-r97z4                                  0/1     CrashLoopBackOff   51         107m    &lt;none&gt;        10.x.xx.xx   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd1-10.x.xx.xx                                      0/1     CrashLoopBackOff   61         44m     &lt;none&gt;        10.x.xx.xx   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">promtail-7jk8j                                        0/1     CrashLoopBackOff   51         107m    &lt;none&gt;        10.x.xx.xx   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">zookeeper-1-10.x.xx.xx                                0/1     CrashLoopBackOff   83         107m    &lt;none&gt;        10.x.xx.xx   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>看下 etcd 日志，谁让 etcd 是 golang 写的，golang 服务的日志比 java 的日志更清晰 😉</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -a |grep etcd | head -n 3</span><br><span class="line">a01d26e5668a        mirrorgooglecontainers/pause-amd64:3.1                                                                       &quot;/pause&quot;                 1 second ago         Created                                                                 k8s_POD_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_3182</span><br><span class="line">2434b43b2026        mirrorgooglecontainers/pause-amd64:3.1                                                                       &quot;/pause&quot;                 3 seconds ago        Exited (0) 1 second ago                                                 k8s_POD_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_3181</span><br><span class="line">8e434211ee24        b5d94f31df3a                                                                                                 &quot;/app/etcd --name=et…&quot;   5 seconds ago        Exited (1) 4 seconds ago                                                k8s_etcd1_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_62</span><br><span class="line">$ docker logs 8e43</span><br><span class="line">2022-07-12 09:11:45.659987 W | pkg/flags: unrecognized environment variable ETCD_PORT_2379_TCP_PORT=2379</span><br><span class="line">2022-07-12 09:11:45.660118 W | pkg/flags: unrecognized environment variable ETCD_SERVICE_PORT_ETCD_CLIENT_2379=2379</span><br><span class="line">2022-07-12 09:11:45.660149 W | pkg/flags: unrecognized environment variable ETCD_PORT_2379_TCP_ADDR=xxx.xx.145.219</span><br><span class="line">2022-07-12 09:11:45.660161 W | pkg/flags: unrecognized environment variable ETCD_PORT_2379_TCP_PROTO=tcp</span><br><span class="line">2022-07-12 09:11:45.660180 W | pkg/flags: unrecognized environment variable ETCD_SERVICE_HOST=xxx.xx.145.219</span><br><span class="line">2022-07-12 09:11:45.660202 W | pkg/flags: unrecognized environment variable ETCD_PORT_2379_TCP=tcp://xxx.xx.145.219:2379</span><br><span class="line">2022-07-12 09:11:45.660227 W | pkg/flags: unrecognized environment variable ETCD_SERVICE_PORT=2379</span><br><span class="line">2022-07-12 09:11:45.660322 W | pkg/flags: unrecognized environment variable ETCD_PORT=tcp://xxx.xx.145.219:2379</span><br><span class="line">2022-07-12 09:11:45.660376 E | etcdmain: error verifying flags, expected IP in URL for binding (http://:2380). See &#x27;etcd --help&#x27;.</span><br></pre></td></tr></table></figure><p>日志报错没有 IP，看了下 flannel 的 pod 都是正常运行的，看下 kubelet 日志：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ journalctl -xe --no-pager -u kubelet</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: with error: exit status 1</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: I0712 17:13:31.384403     761 kubelet.go:1933] SyncLoop (PLEG): &quot;xxxx-privilege-r97z4_default(3b3e512c-61e4-4e0f-ae19-217f9d23bdce)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;3b3e512c-61e4-4e0f-ae19-217f9d23bdce&quot;, Type:&quot;ContainerStarted&quot;, Data:&quot;9584c21ffd29fcc723f9855b3235e652058c8f8dd66bcc1539fd8d213c059482&quot;&#125;</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: I0712 17:13:31.385046     761 kuberuntime_manager.go:434] Sandbox for pod &quot;xxxx-privilege-r97z4_default(3b3e512c-61e4-4e0f-ae19-217f9d23bdce)&quot; has no IP address.  Need to start a new one</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: W0712 17:13:31.398508     761 docker_sandbox.go:384] failed to read pod IP from plugin/docker: NetworkPlugin cni failed on the status hook for pod &quot;zookeeper-1-10.x.xx.xx_default&quot;: Unexpected command output nsenter: failed to execute ip: No such file or directory</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: with error: exit status 1</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: I0712 17:13:31.418775     761 kubelet.go:1933] SyncLoop (PLEG): &quot;zookeeper-1-10.x.xx.xx_default(9ede2a2352bf8cd0cf86767166391721)&quot;, event: &amp;pleg.PodLifecycleEvent&#123;ID:&quot;9ede2a2352bf8cd0cf86767166391721&quot;, Type:&quot;ContainerStarted&quot;, Data:&quot;1f7169d3e335fd0e79c717290d8ac42de5d089dae5fa4647a13ebe53b8611c88&quot;&#125;</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: I0712 17:13:31.419167     761 kuberuntime_manager.go:434] Sandbox for pod &quot;zookeeper-1-10.x.xx.xx_default(9ede2a2352bf8cd0cf86767166391721)&quot; has no IP address.  Need to start a new one</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: W0712 17:13:31.430948     761 docker_sandbox.go:384] failed to read pod IP from plugin/docker: NetworkPlugin cni failed on the status hook for pod &quot;xxxx-gateway-7dd6cdc85d-6hsz7_default&quot;: Unexpected command output nsenter: failed to execute ip: No such file or directory</span><br><span class="line">Jul 12 17:13:31 xxx.xxx.xxx kubelet[761]: with error: exit status 1</span><br></pre></td></tr></table></figure><p>看报错意思是执行 ip netns 报错，看了下，果然没 ip 命令了，系统是 centos 7.9，需要安装 <code>iproute</code> 包，在 <a href="http://www.rpmfind.net/">http://www.rpmfind.net/</a> 上下了个 centos7 的 rpm 后让人传上去安装后就好了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -a |grep etcd  | head -n 5</span><br><span class="line">52b7b3a6a5b2        b5d94f31df3a                                                                                                 &quot;/app/etcd --name=et…&quot;   30 seconds ago       Up 29 seconds                                                         k8s_etcd1_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_68</span><br><span class="line">93f00c95f7ba        mirrorgooglecontainers/pause-amd64:3.1                                                                       &quot;/pause&quot;                 33 seconds ago       Up 32 seconds                                                         k8s_POD_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_3378</span><br><span class="line">fb75458a53b8        mirrorgooglecontainers/pause-amd64:3.1                                                                       &quot;/pause&quot;                 36 seconds ago       Exited (0) 34 seconds ago                                             k8s_POD_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_3377</span><br><span class="line">e291ec9ed471        mirrorgooglecontainers/pause-amd64:3.1                                                                       &quot;/pause&quot;                 39 seconds ago       Exited (0) 37 seconds ago                                             k8s_POD_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_3376</span><br><span class="line">e6eeed41ef31        b5d94f31df3a                                                                                                 &quot;/app/etcd --name=et…&quot;   3 minutes ago        Exited (1) 3 minutes ago                                              k8s_etcd1_etcd1-10.x.xx.xx_default_90bb7a6b237dd87a85e03ed7981e90f3_67</span><br></pre></td></tr></table></figure><p>按理说不应该有人去卸载它，是不是有其他依赖给它卸载了，看下日志果然：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ grep -C 20  iprout /var/log/yum.log</span><br><span class="line">Jul 04 22:43:24 Erased: plymouth-0.8.9-0.34.20140113.el7.centos.x86_64</span><br><span class="line">Jul 04 22:43:24 Erased: plymouth-scripts-0.8.9-0.34.20140113.el7.centos.x86_64</span><br><span class="line">Jul 04 22:43:24 Erased: iptables-services-1.4.21-35.el7.x86_64</span><br><span class="line">Jul 04 22:43:25 Erased: kbd-1.15.5-15.el7.x86_64</span><br><span class="line">Jul 04 22:43:25 Erased: kexec-tools-2.0.15-51.el7_9.3.x86_64</span><br><span class="line">Jul 04 22:43:25 Erased: dracut-network-033-572.el7.x86_64</span><br><span class="line">Jul 04 22:43:25 Erased: 12:dhclient-4.2.5-82.el7.centos.x86_64</span><br><span class="line">Jul 04 22:43:26 Erased: initscripts-9.49.53-1.el7_9.1.x86_64</span><br><span class="line">Jul 04 22:43:27 Erased: open-vm-tools-11.0.5-3.el7_9.3.x86_64</span><br><span class="line">Jul 04 22:43:27 Erased: iproute-4.11.0-30.el7.x86_64</span><br><span class="line">Jul 04 22:43:27 Erased: iptables-1.4.21-35.el7.x86_64</span><br><span class="line">Jul 04 22:44:33 Installed: iptables-1.4.21-35.el7.x86_64</span><br><span class="line">$ uptime -s</span><br><span class="line">2022-07-12 13:37:19</span><br></pre></td></tr></table></figure><p>上面的 <code>-C 20</code> 就这几行，说明日志文件内容就这么点，看就是客户之前自己去安装 iptables 的那个一次性导入规则服务导致的，客户自己的锅</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;有事回到工位上，还没坐下同事就过来喊我，让我帮忙看个客户的生产环境问题，大致就是客户为了搞安全，开了 ipset，然后发现业务受影响了。&lt;/</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>docker info无warning，iptables规则正常，宿主机就是不转发</title>
    <link href="http://zhangguanzhang.github.io/2022/06/03/container-not-forward/"/>
    <id>http://zhangguanzhang.github.io/2022/06/03/container-not-forward/</id>
    <published>2022-06-03T15:18:30.000Z</published>
    <updated>2022-06-03T15:18:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>06、02 凌晨被喊醒帮忙看问题，客户侧重启部分 k8s 节点机器后，业务的部分接口出现问题，环境无法向日葵之类的远程，只能发命令后，现场人员执行。</p><h3 id="具体现象"><a href="#具体现象" class="headerlink" title="具体现象"></a>具体现象</h3><p>业务 pod 日志看是无法连到非 k8s 机器上的 mysql 的 3306， <code>docker info</code> 命令无 warning 也就是代表下面的几个内核参数正常：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-arptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure><p>查看 <code>iptables -S</code> 规则没有优先级高的 drop 之类的规则，默认 FORWARD Chain 的最后行为规则也不是 DROP。因为用了 cni plugins ，pod 的容器都是挂在 cni0 下的，可以 -i 插入优先放行的规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables --<span class="built_in">wait</span> -I INPUT -i cni0 -j ACCEPT</span><br></pre></td></tr></table></figure><p>让现场插入了还是不行，然后让看看安全软件，<code>ps aux | grep xxx</code> 关键字后找到了个 <code>titanagent</code>，搜了下是青藤云安全，之前遇到过安全软件拦截 pod 的网络请求，凌晨无法找到相关青藤云人员，暂时把几个节点 <code>drain</code> 了 </p><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><p>06&#x2F;03 号客户找人卸载了青藤云，发现还是不行，提供了远程，我上去看。</p><h3 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h3><p>查了下，发现重启的节点上的容器，无法访问非宿主机的ip和端口，ping 另一个 k8s 主机都不行，用 tcpdump 抓包发现宿主机没转发，然后用 docker 起个默认桥接网络的试下也是不行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -ti --entrypoint bash xxx telnet xxx 3306</span><br></pre></td></tr></table></figure><p>然后看了下 iptables 状态：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -vnL -w | grep -A10 FORWARD</span><br><span class="line">Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination         </span><br><span class="line">    0     0 KUBE-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */</span><br><span class="line">    0     0 KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */</span><br><span class="line">    0     0 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0           </span><br><span class="line">    0     0 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0           </span><br><span class="line">    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED</span><br><span class="line">    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0           </span><br><span class="line">    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0           </span><br><span class="line">    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0           </span><br><span class="line">    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0           </span><br><span class="line">    0     0 ACCEPT     all  --  *      *       172.27.0.0/16        0.0.0.0/0           </span><br><span class="line">    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            172.27.0.0/16</span><br></pre></td></tr></table></figure><p>果然没转发，仔细看是所有的都没转发，思考了下是不是客户 <code>/etc/sysctl.conf</code> 里改了啥 docker info 没检测到的转发相关的内核参数，然后重启后就没转发的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/sysctl.conf /etc/sysctl.conf.bak</span><br><span class="line">vi /etc/sysctl.conf</span><br></pre></td></tr></table></figure><p>看了下参数很多，就直接粗暴的处理了，备份文件，然后二分排除。第一行到中间注释，重启后上面的 docker 命令测试，最后找到是下面这个参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.conf.default.forwarding=0</span><br></pre></td></tr></table></figure><h3 id="net-ipv4-conf-default-forwarding-的测试"><a href="#net-ipv4-conf-default-forwarding-的测试" class="headerlink" title="net.ipv4.conf.default.forwarding 的测试"></a>net.ipv4.conf.default.forwarding 的测试</h3><p>其实问题还没完，我试了下如果设置为0重启，然后起容器了，然后设置为1还是不行，然后找个干净的环境来下面步骤复现：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sed -ri &#x27;/net.ipv4.conf.default.forwarding/s#1#0#&#x27; /etc/sysctl.conf</span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker run --rm -tid --name test --entrypoint bash nicolaka/netshoot</span><br><span class="line">timeout 3 docker exec test ping -c 1 114.114.114.114</span><br><span class="line"></span><br><span class="line">sysctl -w net.ipv4.conf.default.forwarding=1</span><br><span class="line">timeout 3 docker exec test ping -c 1 114.114.114.114</span><br></pre></td></tr></table></figure><p>然后看下参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -a |&amp; grep forwarding </span><br><span class="line">net.ipv4.conf.all.forwarding = 1</span><br><span class="line">net.ipv4.conf.all.mc_forwarding = 0</span><br><span class="line">net.ipv4.conf.default.forwarding = 1</span><br><span class="line">net.ipv4.conf.default.mc_forwarding = 0</span><br><span class="line">net.ipv4.conf.docker0.forwarding = 0</span><br><span class="line">net.ipv4.conf.docker0.mc_forwarding = 0</span><br><span class="line">net.ipv4.conf.eth0.forwarding = 1</span><br><span class="line">net.ipv4.conf.eth0.mc_forwarding = 0</span><br><span class="line">net.ipv4.conf.lo.forwarding = 1</span><br><span class="line">net.ipv4.conf.lo.mc_forwarding = 0</span><br><span class="line">net.ipv4.conf.veth10d8150.forwarding = 0</span><br><span class="line">net.ipv4.conf.veth10d8150.mc_forwarding = 0</span><br><span class="line">net.ipv6.conf.all.forwarding = 0</span><br><span class="line">net.ipv6.conf.all.mc_forwarding = 0</span><br><span class="line">net.ipv6.conf.default.forwarding = 0</span><br><span class="line">net.ipv6.conf.default.mc_forwarding = 0</span><br><span class="line">net.ipv6.conf.docker0.forwarding = 0</span><br><span class="line">net.ipv6.conf.docker0.mc_forwarding = 0</span><br><span class="line">net.ipv6.conf.eth0.forwarding = 0</span><br><span class="line">net.ipv6.conf.eth0.mc_forwarding = 0</span><br><span class="line">net.ipv6.conf.lo.forwarding = 0</span><br><span class="line">net.ipv6.conf.lo.mc_forwarding = 0</span><br><span class="line">net.ipv6.conf.veth10d8150.forwarding = 0</span><br><span class="line">net.ipv6.conf.veth10d8150.mc_forwarding = 0</span><br></pre></td></tr></table></figure><p>桥接工具看下，因为只有 docker 并且上面一个容器：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">docker08000.0242b6e42b7fnoveth10d8150</span><br></pre></td></tr></table></figure><p>把容器的网卡所在的网桥 docker0 转发开启下再试试：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -w net.ipv4.conf.docker0.forwarding=1</span><br><span class="line">$ timeout 3 docker exec test ping -c 1 114.114.114.114</span><br><span class="line">PING 114.114.114.114 (114.114.114.114) 56(84) bytes of data.</span><br><span class="line">64 bytes from 114.114.114.114: icmp_seq=1 ttl=67 time=17.5 ms</span><br><span class="line"></span><br><span class="line">--- 114.114.114.114 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 17.500/17.500/17.500/0.000 ms</span><br></pre></td></tr></table></figure><p>所以如果是关闭后开机，需要设置总开关和相关的网桥：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.conf.default.forwarding=1</span><br><span class="line">net.ipv4.conf.docker0.forwarding=1</span><br><span class="line">net.ipv4.conf.cni0.forwarding=1</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个参数不知道为啥 docker info 不检查它</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;06、02 凌晨被喊醒帮忙看问题，客户侧重启部分 k8s 节点机器后，业务的部分接口出现问题，环境无法向日葵之类的远程，只能发命令后，现场人</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="kernal" scheme="http://zhangguanzhang.github.io/tags/kernal/"/>
    
  </entry>
  
  <entry>
    <title>低版本内核下容器内部无法使用 unix socket 通信</title>
    <link href="http://zhangguanzhang.github.io/2022/05/13/overlayfs-unix-socket/"/>
    <id>http://zhangguanzhang.github.io/2022/05/13/overlayfs-unix-socket/</id>
    <published>2022-05-13T14:18:30.000Z</published>
    <updated>2022-05-13T14:18:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>最近几次遇到的机器上所有容器内，都无法使用 unix:&#x2F;&#x2F; 去通信的的一个问题。</p><h2 id="遇到的几个错误现象"><a href="#遇到的几个错误现象" class="headerlink" title="遇到的几个错误现象"></a>遇到的几个错误现象</h2><p>最开始是我们部署容器内无法使用 ansible 去操作其他机器，后面是该机器上所有容器内的 supervisorctl 无法通过 unix sock 连接 supervisord </p><h3 id="ansible"><a href="#ansible" class="headerlink" title="ansible"></a>ansible</h3><p>报错如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">failt: [10.x.x.x]: UNREACHABLE! =&gt; &#123;</span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;msg&quot;: &quot;Data could not be sent to remote host \&quot;10.x.x.x\&quot;. Make sure this host can be reached over ssh: Control socket connect (/tmp/ansible-ssh-10.x.x.x-22-root): Connection refused</span><br><span class="line">    Failed to connect to new control master&quot;, &quot;unreachable&quot;: true&#125;</span><br></pre></td></tr></table></figure><p>ansible 的命令加上 <code>-vvvv</code> 后复制 ssh 的所有参数执行，然后发现去掉 socket 的参数就能连上，以前遇到过这个问题，当时搜到 issue 后是改 ansible.cfg 把 ssh 的持久化 socket 路径换了下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ssh_connection]</span><br><span class="line">#control_path = /tmp/ansible-ssh-%%h-%%p-%%r</span><br><span class="line">control_path = /dev/shm/cp%%h-%%p-%%r</span><br></pre></td></tr></table></figure><h3 id="supervisor-的-unix-x2F-x2F-refused"><a href="#supervisor-的-unix-x2F-x2F-refused" class="headerlink" title="supervisor 的 unix:&#x2F;&#x2F; refused"></a>supervisor 的 unix:&#x2F;&#x2F; refused</h3><p>然后昨天我们开发机器上，发现有个业务容器内不能用 <code>supervisorctl</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ supervisorctl status </span><br><span class="line">unix:///var/run/supervisor.sock refused connection</span><br></pre></td></tr></table></figure><p>然后今天我们的一个部署容器发现也是这样，大致分析了下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 启动参数</span><br><span class="line">#/usr/local/bin/python3.7 /usr/local/bin/supervisord -c /root/xxx/supervisord.conf.containd --nodaemon</span><br><span class="line"># 指定配置文件还是一样</span><br><span class="line">$ supervisorctl -c   /root/xxx/supervisord.conf.containd status</span><br><span class="line">unix:///var/run/supervisor.sock refused connection</span><br><span class="line"></span><br><span class="line"># socket 文件存在，权限也对</span><br><span class="line">ls -l /var/run/supervisor.sock </span><br><span class="line">srw-rw---- 1 root root 0 5月  13 11:32 /var/run/supervisor.sock</span><br></pre></td></tr></table></figure><p>配置里相关的都是对的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[unix_http_server]</span><br><span class="line">file=/var/run/supervisor.sock   ; (the path to the socket file)</span><br><span class="line">chmod=0700                       ; sockef file mode (default 0700)</span><br><span class="line"></span><br><span class="line">[rpcinterface:supervisor]</span><br><span class="line">supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface</span><br><span class="line"></span><br><span class="line">[supervisorctl]</span><br><span class="line">serverurl=unix:///var/run/supervisor.sock ; use a unix:// URL  for a unix socket</span><br></pre></td></tr></table></figure><p>然后对比另一个我自己的正常的 CentOS 机器上部署容器内部的 supervisor 配置文件发现是一模一样的，断定和配置无关，然后搜了下发现是内核问题。低版本的 overlayfs 内部使用 unix sock 文件通信会出问题，机器相关信息为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/os-release</span><br><span class="line">NAME=&quot;Ubuntu&quot;</span><br><span class="line">VERSION=&quot;16.04 LTS (Xenial Xerus)&quot;</span><br><span class="line">ID=ubuntu</span><br><span class="line">ID_LIKE=debian</span><br><span class="line">PRETTY_NAME=&quot;Ubuntu 16.04 LTS&quot;</span><br><span class="line">VERSION_ID=&quot;16.04&quot;</span><br><span class="line">HOME_URL=&quot;http://www.ubuntu.com/&quot;</span><br><span class="line">SUPPORT_URL=&quot;http://help.ubuntu.com/&quot;</span><br><span class="line">BUG_REPORT_URL=&quot;http://bugs.launchpad.net/ubuntu/&quot;</span><br><span class="line">UBUNTU_CODENAME=xenial</span><br><span class="line">$ uname -a</span><br><span class="line">Linux xxx 4.4.0-21-generic #37-Ubuntu ....</span><br></pre></td></tr></table></figure><h2 id="解决容器内使用-sock-文件通信的问题"><a href="#解决容器内使用-sock-文件通信的问题" class="headerlink" title="解决容器内使用 sock 文件通信的问题"></a>解决容器内使用 sock 文件通信的问题</h2><p>ubuntu 官方说这个在内核 <code>4.4.0-36.55</code> 修复了，需要升级内核，先说下升级内核的步骤</p><h3 id="升级内核"><a href="#升级内核" class="headerlink" title="升级内核"></a>升级内核</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ dpkg --get-selections |grep -E &#x27;^linux&#x27;</span><br><span class="line">linux-baseinstall</span><br><span class="line">linux-firmwareinstall</span><br><span class="line">linux-genericinstall</span><br><span class="line">linux-headers-4.4.0-21install</span><br><span class="line">linux-headers-4.4.0-21-genericinstall</span><br><span class="line">linux-headers-genericinstall</span><br><span class="line">linux-image-4.4.0-21-genericinstall</span><br><span class="line">linux-image-extra-4.4.0-21-genericinstall</span><br><span class="line">linux-image-genericinstall</span><br></pre></td></tr></table></figure><p>查找指定内核</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ apt-cache search linux| grep 4.4.0-36</span><br><span class="line">linux-cloud-tools-4.4.0-36 - Linux kernel version specific cloud tools for version 4.4.0-36</span><br><span class="line">linux-cloud-tools-4.4.0-36-generic - Linux kernel version specific cloud tools for version 4.4.0-36</span><br><span class="line">linux-cloud-tools-4.4.0-36-lowlatency - Linux kernel version specific cloud tools for version 4.4.0-36</span><br><span class="line">linux-headers-4.4.0-36 - Header files related to Linux kernel version 4.4.0</span><br><span class="line">linux-headers-4.4.0-36-generic - Linux kernel headers for version 4.4.0 on 64 bit x86 SMP</span><br><span class="line">linux-headers-4.4.0-36-lowlatency - Linux kernel headers for version 4.4.0 on 64 bit x86 SMP</span><br><span class="line">linux-image-4.4.0-36-generic - Linux kernel image for version 4.4.0 on 64 bit x86 SMP</span><br><span class="line">linux-image-4.4.0-36-lowlatency - Linux kernel image for version 4.4.0 on 64 bit x86 SMP</span><br><span class="line">linux-image-extra-4.4.0-36-generic - Linux kernel extra modules for version 4.4.0 on 64 bit x86 SMP</span><br><span class="line">linux-signed-image-4.4.0-36-generic - Signed kernel image generic</span><br><span class="line">linux-signed-image-4.4.0-36-lowlatency - Signed kernel image lowlatency</span><br><span class="line">linux-tools-4.4.0-36 - Linux kernel version specific tools for version 4.4.0-36</span><br><span class="line">linux-tools-4.4.0-36-generic - Linux kernel version specific tools for version 4.4.0-36</span><br><span class="line">linux-tools-4.4.0-36-lowlatency - Linux kernel version specific tools for version 4.4.0-36</span><br></pre></td></tr></table></figure><p>安装指定内核：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y linux-&#123;headers,image&#125;-4.4.0-36-generic</span><br></pre></td></tr></table></figure><p>可以看下 <code>/boot/grub/grub.cfg</code> 里第一个 menuentry 段的 <code>initrd</code> 是否指定到新的内核了，整体开机 grub 菜单可以通过下面命令查看。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ grep -Ei &#x27;submenu|menuentry &#x27; /boot/grub/grub.cfg | sed -re &quot;s/(.? )&#x27;([^&#x27;]+)&#x27;.*/\1 \2/&quot;</span><br><span class="line">menuentry  Ubuntu</span><br><span class="line">submenu  Advanced options for Ubuntu</span><br><span class="line">menuentry  Ubuntu, with Linux 4.4.0-36-generic</span><br><span class="line">menuentry  Ubuntu, with Linux 4.4.0-36-generic (recovery mode)</span><br><span class="line">menuentry  Ubuntu, with Linux 4.4.0-21-generic</span><br><span class="line">menuentry  Ubuntu, with Linux 4.4.0-21-generic (recovery mode)</span><br></pre></td></tr></table></figure><p>如果想开机启动到 submenu 的 第三个，可以改文件 <code>/etc/default/grub</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GRUB_DEFAULT=&quot;1&gt;2&quot;</span><br></pre></td></tr></table></figure><p>前面是外层菜单，后面是子菜单，0 开始，更新下 grub:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update-grub</span><br></pre></td></tr></table></figure><p>重启后进入发现好了。</p><h3 id="不升级内核的办法"><a href="#不升级内核的办法" class="headerlink" title="不升级内核的办法"></a>不升级内核的办法</h3><p>就是容器内的 sock 文件存放到容器内部的 <code>/dev/shm/</code> 里即可。</p><h2 id="一些说明"><a href="#一些说明" class="headerlink" title="一些说明"></a>一些说明</h2><p>Centos 7 没问题是因为 centos 的内核是 backport 的，可以理解为 centos 的 overlay 内核模块实际上是 4.10 内核代码移植的。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/Supervisor/supervisor/issues/654">https://github.com/Supervisor/supervisor/issues/654</a></li><li><a href="https://github.com/moby/moby/issues/12080">https://github.com/moby/moby/issues/12080</a></li><li><a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1214500">https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1214500</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;最近几次遇到的机器上所有容器内，都无法使用 unix:&amp;#x2F;&amp;#x2F; 去通信的的一个问题。&lt;/p&gt;
&lt;h2 id=&quot;遇到的几个错误</summary>
      
    
    
    
    
    <category term="overlay" scheme="http://zhangguanzhang.github.io/tags/overlay/"/>
    
    <category term="kernal" scheme="http://zhangguanzhang.github.io/tags/kernal/"/>
    
  </entry>
  
  <entry>
    <title>nfc 折腾笔记，ACR122U/proxmark3</title>
    <link href="http://zhangguanzhang.github.io/2022/04/23/nfc/"/>
    <id>http://zhangguanzhang.github.io/2022/04/23/nfc/</id>
    <published>2022-04-23T14:28:30.000Z</published>
    <updated>2022-04-23T14:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>近期折腾 nfc 相关，以后 nfc 的折腾也会更新在这个文章内</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>以前买过 ACR122U ，想着帮人门禁解密导入手环挣点外快，没想到弄不开，然后群友送了个 proxmark3（pm3）我，折腾了下，做个笔记</p><h2 id="折腾"><a href="#折腾" class="headerlink" title="折腾"></a>折腾</h2><h3 id="卡片"><a href="#卡片" class="headerlink" title="卡片"></a>卡片</h3><p>卡片介绍，通常分为 IC 卡和 ID 卡：</p><ol><li>ID 卡一般低频，不可写入数据，只读卡号，还有 HID 卡（一些大厂工牌），卡表明有编号，里面一般是圆形线圈。</li><li>IC 卡高频，大部分可以擦写，银行，交通，门禁，饭卡，工牌之类都用使用。RFID、NFC 属于 IC 卡，线圈一般是矩形。IC 卡我们破解啥的一般是下面几种类型相关:</li></ol><table><thead><tr><th>类型</th><th>说明</th><th>频率</th><th>擦写情况</th></tr></thead><tbody><tr><td>M1</td><td>NXP Mifare系列卡, 工作在高频（13.56Mhz），全称Mifare S50，是最常见的卡，出厂固化UID</td><td>13.56Mhz</td><td>UID 部分不可擦写</td></tr><tr><td>M0</td><td>相当于M1卡的精简版，容量更小、功能更少，但价格更低</td><td>13.56Mhz</td><td>UID 部分不可擦写</td></tr><tr><td>UID</td><td>全称Mifare UID Chinese magic card，国外叫做中国魔术卡，M1卡的变异版本，使用后门指令(magic指令)，可修改UID（UID在block0分区），可以用来完整克隆M1卡的数据；但是现在新的读卡系统通过检测卡片对后门指令的回应，可以检测出UID卡，因此可以来拒绝UID卡的访问，来达到屏蔽复制卡的功能（即UID防火墙系统）；</td><td>13.56Mhz</td><td>UID 部分可擦写</td></tr><tr><td>CUID</td><td>为了避开UID防火墙系统，CUID卡应运而生，取消响应后门指令(magic指令)，可修改UID，是目前市场上最常用的复制卡；但是现在貌似也有 CUID 卡的防火墙</td><td>13.56Mhz</td><td>UID 部分可擦写</td></tr><tr><td>FUID</td><td>FUID卡只能写一次UID，写完之后自动固化UID所在分区，就等同M1卡，目前任何防火墙系统都无法屏蔽，复制的卡几乎和原卡一模一样；</td><td>13.56Mhz</td><td>可擦写一次</td></tr><tr><td>UFUID</td><td>集UID卡和FUID卡的优点于一身，使用后门指令，可修改UID，再手动锁卡，变成M1卡。可先反复读写UID，确认数据无误，手动锁卡变成M1，解决了UID卡的UID防火墙屏蔽，也解决FUID的一次性写入容易写错的问题，且价格比FUID卡还便宜；</td><td>13.56Mhz</td><td>可擦写一次</td></tr></tbody></table><p>判断是 M0卡(Mifare UltraLight)，还是 M1卡(Mifare Classic 1k)，可以通过 SAK 值判断。</p><table><thead><tr><th>产品</th><th>ATQA</th><th>SAK</th><th>UID长度</th></tr></thead><tbody><tr><td>Mifare Mini</td><td>00 04</td><td>09</td><td>4 bytes</td></tr><tr><td>Mifare Classic 1k</td><td>00 04</td><td>08</td><td>4 bytes</td></tr><tr><td>Mifare Classic 4k</td><td>00 02</td><td>18</td><td>4 bytes</td></tr><tr><td>Mifare Ultraligh</td><td>00 44</td><td>00</td><td>4 bytes写</td></tr><tr><td>Mifare Plus</td><td>00 44</td><td>20</td><td>4 bytes</td></tr></tbody></table><p>SAK 为 20 的是 CPU 模拟卡，基本无解，还有种 SAK 是 28的，是带 M1 数据的 cpu 模拟卡，只能读取 M1 的数据部分，不一定能模拟成功。</p><h3 id="卡片数据存储介绍"><a href="#卡片数据存储介绍" class="headerlink" title="卡片数据存储介绍"></a>卡片数据存储介绍</h3><p>M1(Mifare classic 1K) 卡片，即存储容量 1K &#x3D; 1024Byte，有 16 个扇区，每个扇区有 4 个块，每个块 16 个字节：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">0 扇区</span><br><span class="line">0区块:00000000000000000000000000000000</span><br><span class="line">1区块:00000000000000000000000000000000</span><br><span class="line">2区块:00000000000000000000000000000000</span><br><span class="line">3区块:FFFFFFFFFFFFFF078069FFFFFFFFFFFF</span><br><span class="line"></span><br><span class="line">1 扇区</span><br><span class="line">0区块:00000000000000000000000000000000</span><br><span class="line">1区块:00000000000000000000000000000000</span><br><span class="line">2区块:00000000000000000000000000000000</span><br><span class="line">3区块:FFFFFFFFFFFFFF078069FFFFFFFFFFFF</span><br></pre></td></tr></table></figure><ol><li>0 扇区的 0 区块出厂赋予的，前 4 个字节是 UID，第 5 个字节是 UID，第 6 个字节是 SAK，包括后面的一起是厂商码，所有卡片的厂商码都无法修改。</li><li>除了 0 扇区以外，每个扇区的块0、块1、块2 为数据块，可用于存储数据。</li><li>每个扇区的块3为控制块，包括密码keyA，存取控制，密码keyB。存取控制的作用是控制对应扇区记录的读写权限与keyA和keyB的关系。由于每个扇区都有独立的key和存取控制，因此M1卡可以做到一卡多用互不干扰。</li></ol><p>更多详情见 <a href="https://hceng.cn/2019/07/12/NFC%E6%89%8B%E6%9C%BA%E6%A8%A1%E6%8B%9F%E5%8A%A0%E5%AF%86%E9%97%A8%E7%A6%81%E5%8D%A1/">NFC手机模拟加密门禁卡</a> 里的基础知识。</p><h3 id="ACR122U"><a href="#ACR122U" class="headerlink" title="ACR122U"></a>ACR122U</h3><p>Linux 下面有个复制脚本，<a href="https://www.lostserver.com/static/nfc-cloner.sh">nfc-clone</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y mfoc  libnfc-bin libnfc-bin libnfc-examples</span><br></pre></td></tr></table></figure><p>ACR122U 上面的命令就可以驱动了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ nfc-list</span><br><span class="line">nfc-list uses libnfc 1.7.1</span><br><span class="line">errorlibnfc.driver.acr122_usbUnable to claim USB interface (Device or resource busy)</span><br><span class="line">nfc-list: ERROR: Unable to open NFC device: acr122_usb:001:015</span><br><span class="line">$ lsmod | grep pn533</span><br><span class="line">pn533_usb              20480  0</span><br><span class="line">pn533                  36864  1 pn533_usb</span><br><span class="line">nfc                   110592  1 pn533</span><br></pre></td></tr></table></figure><p>上面报错参考 <a href="https://github.com/nfc-tools/libnfc/issues/402">nfc-tools&#x2F;libnfc</a> 解决：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat &gt; /etc/modprobe.d/blacklist-libnfc.conf</span><br><span class="line">blacklist nfc</span><br><span class="line">blacklist pn533</span><br><span class="line">blacklist pn533_usb</span><br><span class="line">$ sudo modprobe -rf pn533_usb</span><br><span class="line">$ nfc-list</span><br><span class="line">nfc-list uses libnfc 1.7.1</span><br><span class="line">NFC device: ACS / ACR122U PICC Interface opened</span><br></pre></td></tr></table></figure><p>读卡：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ nfc-list</span><br><span class="line">nfc-list uses libnfc 1.7.1</span><br><span class="line">NFC device: ACS / ACR122U PICC Interface opened</span><br><span class="line">1 ISO14443A passive target(s) found:</span><br><span class="line">ISO/IEC 14443A (106 kbps) target:</span><br><span class="line">    ATQA (SENS_RES): 00  04  </span><br><span class="line">       UID (NFCID1): xx  xx  0f  7d  </span><br><span class="line">      SAK (SEL_RES): 28  </span><br><span class="line">                ATS: 78  80  90  02  20  90  00  00  00  00  00  xx  xx  0f  7d  </span><br></pre></td></tr></table></figure><p>更多参考上面的脚本。</p><h3 id="pm3"><a href="#pm3" class="headerlink" title="pm3"></a>pm3</h3><h4 id="固件编译"><a href="#固件编译" class="headerlink" title="固件编译"></a>固件编译</h4><p>官方固件仓库没看到更新，有名的就是 iceman（冰人）的 <a href="https://github.com/RfidResearchGroup/proxmark3">RfidResearchGroup&#x2F;proxmark3</a>。</p><p>我是在我的 r2s 上用 docker 起特权容器玩的，<code>apt</code> 系列系统 <code>ldd --version</code> 看看 glibc 版本，大于等于 2.27，推荐起 <code>ubuntu:18.04</code> 容器编译：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Linux系统中可能存在ModemManager，从而干扰系统和proxmark3的通信。</span><br><span class="line"> apt remove modemmanager</span><br><span class="line"># 获取关闭</span><br><span class="line">sudo systemctl stop ModemManager</span><br><span class="line">sudo systemctl disable ModemManager</span><br><span class="line"></span><br><span class="line">git clone https://github.com/RfidResearchGroup/proxmark3</span><br><span class="line">docker run --name t1 --restart always -tid --privileged -v $PWD/proxmark3:/opt/proxmark3 -w /opt/proxmark3 ubuntu:18.04</span><br></pre></td></tr></table></figure><p>开始编译的依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ -f /etc/apt/sources.list ];then sed -ri &#x27;s/(deb|security|archive|ports).(debian.org|ubuntu.com)/mirrors.aliyun.com/g&#x27; /etc/apt/sources.list; fi &amp;&amp; \</span><br><span class="line">    apt-get update &amp;&amp; \</span><br><span class="line">    DEBIAN_FRONTEND=noninteractive apt install gcc g++ make autoconf pkg-config cmake git libbz2-dev libreadline-dev gcc-arm-none-eabi libssl-dev usbutils -y</span><br></pre></td></tr></table></figure><p>编译之前看下 <code>Makefile.platform.sample</code> ，代码里默认是 <code>pm3 rdv4</code> 版本，就是 1k 多元带蓝牙的版本，我们一般是 pm3 GENERIC（右侧一个按钮，中间一个 usb 口，线圈内就 LF Antenna&#x2F;Rreq: 125kHZ的版本），所以编译命令为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">make all PLATFORM=PM3GENERIC</span><br><span class="line"># 单独编译客户端的话</span><br><span class="line"># make client</span><br></pre></td></tr></table></figure><p>我的 openwrt 能用是因为有很多模块，例如下面的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ opkg list-installed | grep -P &#x27;cdc-|usb-&#x27;</span><br><span class="line">kmod-usb-acm - 5.10.102-1</span><br><span class="line">kmod-usb-core - 5.10.102-1</span><br><span class="line">kmod-usb-net - 5.10.102-1</span><br><span class="line">kmod-usb-net-cdc-eem - 5.10.102-1</span><br><span class="line">kmod-usb-net-cdc-ether - 5.10.102-1</span><br><span class="line">kmod-usb-net-cdc-ncm - 5.10.102-1</span><br><span class="line">kmod-usb-net-rtl8152 - 5.10.102-1</span><br><span class="line">kmod-usb-storage - 5.10.102-1</span><br><span class="line">kmod-usb-storage-extras - 5.10.102-1</span><br><span class="line">kmod-usb-storage-uas - 5.10.102-1</span><br><span class="line">libusb-1.0-0 - 1.0.24-5</span><br></pre></td></tr></table></figure><p>后续手机想操作 pm3 最好看下手机编译了 <code>usb-acm</code> 没，一般的官方 rom 都不会开这个的。</p><h4 id="pm3-操作"><a href="#pm3-操作" class="headerlink" title="pm3 操作"></a>pm3 操作</h4><p>pm3 无论啥时候插入 linux，dmesg 里都有下面信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[862180.046239] usb 2-1: USB disconnect, device number 3</span><br><span class="line">[863606.280680] cdc_acm 2-1:1.0: ttyACM0: USB ACM device</span><br></pre></td></tr></table></figure><p>有 <code>ttyACM0</code> 才是对的，否则就是你 Linux 没 USB-ACM 相关模块，有了后会多出一个字符设备 <code>/dev/ttyACM0</code> 。</p><h5 id="刷写固件"><a href="#刷写固件" class="headerlink" title="刷写固件"></a>刷写固件</h5><p>上面的 make all 会编译固件和 client。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">./pm3-flash-fullimage </span><br><span class="line">[=] Session log /root/.proxmark3/logs/log_20220420.txt</span><br><span class="line">[+] About to use the following file:</span><br><span class="line">[+]    /opt/proxmark3/client/../armsrc/obj/fullimage.elf</span><br><span class="line">[+] Loading ELF file /opt/proxmark3/client/../armsrc/obj/fullimage.elf</span><br><span class="line">[+] ELF file version Iceman/master/v4.14831-551-gda81c6806 2022-04-20 20:23:34 617717b0d</span><br><span class="line"></span><br><span class="line">[+] Waiting for Proxmark3 to appear on /dev/ttyACM0</span><br><span class="line"> 🕑  59 found</span><br><span class="line">[+] Entering bootloader...</span><br><span class="line">[+] (Press and release the button only to abort)</span><br><span class="line">[+] Waiting for Proxmark3 to appear on /dev/ttyACM0</span><br><span class="line"> 🕔  58 found</span><br><span class="line">[=] Available memory on this board: 512K bytes</span><br><span class="line"></span><br><span class="line">[=] Permitted flash range: 0x00102000-0x00180000</span><br><span class="line">[+] Loading usable ELF segments:</span><br><span class="line">[+]    1: V 0x00102000 P 0x00102000 (0x0004e974-&gt;0x0004e974) [R X] @0xb8</span><br><span class="line">[+]    2: V 0x00200000 P 0x00150974 (0x00001ba1-&gt;0x00001ba1) [R X] @0x4ea30</span><br><span class="line">[=] Note: Extending previous segment from 0x4e974 to 0x50515 bytes</span><br><span class="line"></span><br><span class="line">[+] Flashing...</span><br><span class="line">[+] Writing segments for file: /opt/proxmark3/client/../armsrc/obj/fullimage.elf</span><br><span class="line">[+]  0x00102000..0x00152514 [0x50515 / 643 blocks]</span><br><span class="line">...................................................................</span><br><span class="line">        @@@  @@@@@@@ @@@@@@@@ @@@@@@@@@@   @@@@@@  @@@  @@@</span><br><span class="line">        @@! !@@      @@!      @@! @@! @@! @@!  @@@ @@!@!@@@</span><br><span class="line">        !!@ !@!      @!!!:!   @!! !!@ @!@ @!@!@!@! @!@@!!@!</span><br><span class="line">        !!: :!!      !!:      !!:     !!: !!:  !!! !!:  !!!</span><br><span class="line">        :    :: :: : : :: :::  :      :    :   : : ::    : </span><br><span class="line">        .    .. .. . . .. ...  .      .    .   . . ..    . </span><br><span class="line">...................................................................</span><br><span class="line">...................................................................</span><br><span class="line">...................................................................... ok</span><br><span class="line"></span><br><span class="line">[+] All done</span><br><span class="line"></span><br><span class="line">[=] Have a nice day!</span><br></pre></td></tr></table></figure><p>连接 pm3</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">$ ./pm3 --list</span><br><span class="line">1: /dev/ttyACM0</span><br><span class="line"></span><br><span class="line">$ ./client/proxmark3 /dev/ttyACM0 </span><br><span class="line">[=] Session log /root/.proxmark3/logs/log_20220420.txt</span><br><span class="line">[+] loaded from JSON file /root/.proxmark3/preferences.json</span><br><span class="line">[=] Using UART port /dev/ttyACM0</span><br><span class="line">[=] Communicating with PM3 over USB-CDC</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  8888888b.  888b     d888  .d8888b.   </span><br><span class="line">  888   Y88b 8888b   d8888 d88P  Y88b  </span><br><span class="line">  888    888 88888b.d88888      .d88P  </span><br><span class="line">  888   d88P 888Y88888P888     8888&quot;  </span><br><span class="line">  8888888P&quot;  888 Y888P 888      &quot;Y8b.  </span><br><span class="line">  888        888  Y8P  888 888    888  </span><br><span class="line">  888        888   &quot;   888 Y88b  d88P </span><br><span class="line">  888        888       888  &quot;Y8888P&quot;    [ ❄️ ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  [ Proxmark3 RFID instrument ]</span><br><span class="line"></span><br><span class="line">    MCU....... AT91SAM7S512 Rev B</span><br><span class="line">    Memory.... 512 Kb ( 58% used )</span><br><span class="line"></span><br><span class="line">    Client.... Iceman/master/v4.14831-551-gda81c6806 2022-04-20 22:00:05</span><br><span class="line">    Bootrom... Iceman/master/v4.14831-551-gda81c6806 2022-04-20 22:02:34 </span><br><span class="line">    OS........ Iceman/master/v4.14831-551-gda81c6806 2022-04-20 22:04:23 </span><br><span class="line">    Target.... PM3 GENERIC</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[usb] pm3 --&gt; hw version</span><br><span class="line"></span><br><span class="line"> [ Proxmark3 RFID instrument ]</span><br><span class="line"></span><br><span class="line"> [ CLIENT ]</span><br><span class="line">  Iceman/master/v4.14831-551-gda81c6806 2022-04-20 22:00:05 617717b0d</span><br><span class="line">  compiled with............. GCC 9.4.0</span><br><span class="line">  platform.................. Linux / aarch64</span><br><span class="line">  Readline support.......... present</span><br><span class="line">  QT GUI support............ absent</span><br><span class="line">  native BT support......... absent</span><br><span class="line">  Python script support..... absent</span><br><span class="line">  Lua SWIG support.......... present</span><br><span class="line">  Python SWIG support....... absent</span><br><span class="line"></span><br><span class="line"> [ PROXMARK3 ]</span><br><span class="line">  firmware.................. PM3 GENERIC</span><br><span class="line"></span><br><span class="line"> [ ARM ]</span><br><span class="line">  bootrom: Iceman/master/v4.14831-551-gda81c6806 2022-04-20 22:02:34 617717b0d</span><br><span class="line">       os: Iceman/master/v4.14831-551-gda81c6806 2022-04-20 22:04:23 617717b0d</span><br><span class="line">  compiled with GCC 9.2.1 20191025 (release) [ARM/arm-9-branch revision 277599]</span><br><span class="line"></span><br><span class="line"> [ FPGA ] </span><br><span class="line">  LF image 2s30vq100 2022-03-23 17:21:05</span><br><span class="line">  HF image 2s30vq100 2022-03-23 17:21:16</span><br><span class="line">  HF FeliCa image 2s30vq100 2022-03-23 17:21:27</span><br><span class="line">  HF 15 image 2s30vq100 2022-03-23 17:21:38</span><br><span class="line"></span><br><span class="line"> [ Hardware ]</span><br><span class="line">  --= uC: AT91SAM7S512 Rev B</span><br><span class="line">  --= Embedded Processor: ARM7TDMI</span><br><span class="line">  --= Internal SRAM size: 64K bytes</span><br><span class="line">  --= Architecture identifier: AT91SAM7Sxx Series</span><br><span class="line">  --= Embedded flash memory 512K bytes ( 58% used )</span><br><span class="line"></span><br><span class="line">[usb] pm3 --&gt; hw status</span><br><span class="line">[#] Memory</span><br><span class="line">[#]   BigBuf_size............. 42760</span><br><span class="line">[#]   Available memory........ 42760</span><br><span class="line">[#] Tracing</span><br><span class="line">[#]   tracing ................ 1</span><br><span class="line">[#]   traceLen ............... 0</span><br><span class="line">[#] Current FPGA image</span><br><span class="line">[#]   mode.................... HF image 2s30vq100 2022-03-23 17:21:16</span><br><span class="line">[#] LF Sampling config</span><br><span class="line">[#]   [q] divisor............. 95 ( 125.00 kHz )</span><br><span class="line">[#]   [b] bits per sample..... 8</span><br><span class="line">[#]   [d] decimation.......... 1</span><br><span class="line">[#]   [a] averaging........... yes</span><br><span class="line">[#]   [t] trigger threshold... 0</span><br><span class="line">[#]   [s] samples to skip..... 0 </span><br><span class="line">[#] </span><br><span class="line">[#] LF T55XX config</span><br><span class="line">[#]            [r]               [a]   [b]   [c]   [d]   [e]   [f]   [g]</span><br><span class="line">[#]            mode            |start|write|write|write| read|write|write</span><br><span class="line">[#]                            | gap | gap |  0  |  1  | gap |  2  |  3</span><br><span class="line">[#] ---------------------------+-----+-----+-----+-----+-----+-----+------</span><br><span class="line">[#] fixed bit length (default) |  31 |  20 |  18 |  50 |  15 | N/A | N/A | </span><br><span class="line">[#]     long leading reference |  31 |  20 |  18 |  50 |  15 | N/A | N/A | </span><br><span class="line">[#]               leading zero |  31 |  20 |  18 |  40 |  15 | N/A | N/A | </span><br><span class="line">[#]    1 of 4 coding reference |  31 |  20 |  18 |  34 |  15 |  50 |  66 | </span><br><span class="line">[#] </span><br><span class="line">[#] HF 14a config</span><br><span class="line">[#]   [a] Anticol override.... std    ( follow standard )</span><br><span class="line">[#]   [b] BCC override........ std    ( follow standard )</span><br><span class="line">[#]   [2] CL2 override........ std    ( follow standard )</span><br><span class="line">[#]   [3] CL3 override........ std    ( follow standard )</span><br><span class="line">[#]   [r] RATS override....... std    ( follow standard )</span><br><span class="line">[#] Transfer Speed</span><br><span class="line">[#]   Sending packets to client...</span><br><span class="line">[#]   Time elapsed................... 500ms</span><br><span class="line">[#]   Bytes transferred.............. 295424</span><br><span class="line">[#]   Transfer Speed PM3 -&gt; Client... 590848 bytes/s</span><br><span class="line">[#] Various</span><br><span class="line">[#]   Max stack usage......... 4088 / 8480 bytes</span><br><span class="line">[#]   Debug log level......... 1 ( error )</span><br><span class="line">[#]   ToSendMax............... -1</span><br><span class="line">[#]   ToSend BUFFERSIZE....... 2308</span><br><span class="line">[#]   Slow clock.............. 30644 Hz</span><br><span class="line">[#] Installed StandAlone Mode</span><br><span class="line">[#]   LF HID26 standalone - aka SamyRun (Samy Kamkar)</span><br><span class="line">[#] </span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="pm3-的操作"><a href="#pm3-的操作" class="headerlink" title="pm3 的操作"></a>pm3 的操作</h5><h6 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h6><p>上面连接后，进入到一个交互，交互里输入命令就行了，有点类似交换机和单片机里的 AT 指令，可以 <code>help</code> 查看命令，查看命令的帮助就 <code>命令 --help</code>、<code>命令 子命令 --help</code> 依次类推，你搜到的很多文章都是老命令，根据固件里实际来。</p><p><code>lf</code> 和 <code>hf</code> 对应低频和高频命令，例如我们扫描 M1 卡片：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[usb] pm3 --&gt; hf search</span><br><span class="line"> ??  Searching for ISO14443-A tag...          </span><br><span class="line">[+]  UID: XX XX 0F 7D </span><br><span class="line">[+] ATQA: 00 04</span><br><span class="line">[+]  SAK: 28 [1]</span><br><span class="line">[+] Possible types:</span><br><span class="line">[+]    SmartMX with MIFARE Classic 1K</span><br><span class="line">[=] -------------------------- ATS --------------------------</span><br><span class="line">[+] ATS: 10 78 80 90 02 20 90 00 00 00 00 00 XX XX 0F 7D [ ED 00 ]</span><br><span class="line">[=]      10...............  TL    length is 16 bytes</span><br><span class="line">[=]         78............  T0    TA1 is present, TB1 is present, TC1 is present, FSCI is 8 (FSC = 256)</span><br><span class="line">[=]            80.........  TA1   different divisors are NOT supported, DR: [], DS: []</span><br><span class="line">[=]               90......  TB1   SFGI = 0 (SFGT = (not needed) 0/fc), FWI = 9 (FWT = 2097152/fc)</span><br><span class="line">[=]                  02...  TC1   NAD is NOT supported, CID is supported</span><br><span class="line"></span><br><span class="line">[=] -------------------- Historical bytes --------------------</span><br><span class="line">[+]   20900000000000XXXX0F7D</span><br><span class="line"></span><br><span class="line">[+] Prng detection: weak</span><br><span class="line">[#] Auth error</span><br><span class="line">[?] Hint: try `hf mf` commands</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[+] Valid ISO 14443-A tag found</span><br><span class="line"></span><br><span class="line">[=] Short AID search:</span><br><span class="line">[?] Hint: try emv commands</span><br></pre></td></tr></table></figure><h6 id="嗅探"><a href="#嗅探" class="headerlink" title="嗅探"></a>嗅探</h6><p>主要利用 sniff 命令嗅探，比如常见的 14a 卡片：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[usb] pm3 --&gt; hf 14a sniff</span><br><span class="line"></span><br><span class="line">[#] Starting to sniff。 Press PM3 Button to stop.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>然后保持上电，拿去刷卡，<code>卡</code> – <code>pm3</code> – <code>刷卡设备</code> ，多刷几下，等 pm3 两个灯交叉闪烁后常亮，按下 Button 按钮，pm3 客户端会显示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[#] trace len = xxxx</span><br></pre></td></tr></table></figure><p>此刻赶紧保存下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[usb] pm3 --&gt; data save -f xxx</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>分析：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[usb] pm3 --&gt; trace list -t 14a</span><br></pre></td></tr></table></figure><p>运气好能分析到 <code>PWD-AUTH KEY</code> ，然后破解，参考 <a href="https://raycn.pub/2021/08/21/reset-xiaomi-air-purifier-filters/">【PM3】重置小米空气净化器滤芯</a>。运气不好就无法嗅探。</p><h3 id="破解的一些说明"><a href="#破解的一些说明" class="headerlink" title="破解的一些说明"></a>破解的一些说明</h3><ol><li>把卡片解密后 dump 出来，dump 的数据是没加密的，再写入白卡，手机或者手环再读取白卡模拟</li><li>设备可以读取 dump 数据写入手机的 nfc 卡片里，但是最好不要这样做，很多手机的 nfc 是单独一块电路，不跟随刷机恢复出厂设置的，读写可能会损坏 nfc 相关硬件只有换主板了。</li><li>有些门禁设备是还校验厂商码的，这种就放弃吧，手机模拟白卡不会让写厂商码的，涉及到法律问题。</li><li>有些公司的 oa 软件支持添加电子工牌，实际添加行为是 oa 软件里点了添加后会给手机的钱包-nfc 添加一张模拟卡片，这个卡片是 cpu 模拟卡，SAK 显示 20，说明门禁联网的。</li></ol><h3 id="其他补充"><a href="#其他补充" class="headerlink" title="其他补充"></a>其他补充</h3><ol><li>windows 我有安装串口驱动，但是下的 client 和 GUI 都无法通过 COM 口连上 pm3 ，进安全模式安装 <a href="https://github.com/RfidResearchGroup/proxmark3/tree/master/driver">官方的驱动</a> 也不行。尝试过串口软件打开 com 口，看源码是 <code>baud=115200 parity=N data=8 stop=1</code>，但是还是不行</li><li>安卓客户端 <a href="https://github.com/AndProx/AndProx">AndProx&#x2F;AndProx</a> 因为我手机内核编译没开 <code>USB_ACM</code> 导致无法使用，<a href="https://github.com/RfidResearchGroup/proxmark3/blob/master/doc/termux_notes.md">termux</a> 同样。</li><li>多看看 <a href="https://github.com/RfidResearchGroup/proxmark3/blob/master/README.md">官方的 README.md</a></li></ol><p>也有离线嗅探固件，我就不折腾了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://raycn.pub/2021/08/21/reset-xiaomi-air-purifier-filters/">【PM3】重置小米空气净化器滤芯</a></li><li><a href="https://pm3.echo.cool/index.php/2018/08/21/pm3%E7%A6%BB%E7%BA%BF%E5%97%85%E6%8E%A2%E8%AF%B4%E6%98%8E/">PM3离线嗅探说明</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;近期折腾 nfc 相关，以后 nfc 的折腾也会更新在这个文章内&lt;/p&gt;</summary>
    
    
    
    
    <category term="nfc" scheme="http://zhangguanzhang.github.io/tags/nfc/"/>
    
    <category term="pm3" scheme="http://zhangguanzhang.github.io/tags/pm3/"/>
    
  </entry>
  
  <entry>
    <title>docker containerd 不定时的 segfault 的一次处理过程</title>
    <link href="http://zhangguanzhang.github.io/2022/04/22/segfault-often/"/>
    <id>http://zhangguanzhang.github.io/2022/04/22/segfault-often/</id>
    <published>2022-04-22T10:15:30.000Z</published>
    <updated>2022-04-22T10:15:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>客户环境 docker 和 containerd 启动时不时 segment fault 的一次处理过程。</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>问题拉我处理是 <code>04/15</code> 号，现象是客户的根分区被撑爆了，后台 tty 进去看了下是根目录充满了 <code>core.$pid</code> 的 coredump 文件。然后清理后重启发现很多容器起不来。然后喊我来看下。</p><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><p>客户系统是 centos7.9 ，先使用 <code>systemctl status docker</code> 看了下 docker 运行一段时间后就崩了，前台启动调试下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ dockerd --version</span><br><span class="line">Docker version 19.03.14, build 5eb3275</span><br><span class="line">$ systemctl stop kubelet docker</span><br><span class="line">$ dockerd --debug</span><br></pre></td></tr></table></figure><p>然后发现每次 dockerd 退出日志不一样，有时候是 <code>segment fault</code>，有时候报错无法通过 <code>/var/run/docker/containerd/containerd.sock</code> 连接 containerd，该 sock 文件不存在，根据这个报错可以看出来 containerd 无法启动。可以通过 <a href="https://github.com/docker/docker-ce/blob/d7080c17a580919f5340a15a8e5e013133089680/components/engine/libcontainerd/remote_daemon.go#L205-244">源码</a> 得知，如果没启动 containerd ，docker 则会 os.Exec 起一个 <code>containerd</code> 。</p><p>我们的 docker 是官方的 static bin 安装的，如果是官方包管理安装的话，containerd 会由 systemd 启动，docker bin 的方式的话，会由 dockerd 使用 exec 方式启动一个 containerd，找个同版本的查询下 cmdline 后手动前台 debug log-level 启动下 containerd：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br></pre></td><td class="code"><pre><span class="line">$ containerd --config /var/run/docker/containerd/containerd.toml --log-level debug   </span><br><span class="line">INFO[2022-04-15T14:14:01.755841221+08:00] starting containerd                           revision=ea765aba0d05254012b0b9e595e995c09186427f version=v1.3.9</span><br><span class="line">DEBU[2022-04-15T14:14:01.755989919+08:00] changing OOM score to -500                   </span><br><span class="line">INFO[2022-04-15T14:14:01.787587081+08:00] loading plugin &quot;io.containerd.content.v1.content&quot;...  type=io.containerd.content.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.787837808+08:00] loading plugin &quot;io.containerd.snapshotter.v1.btrfs&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.790684367+08:00] skip loading plugin &quot;io.containerd.snapshotter.v1.btrfs&quot;...  error=&quot;path /data/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs (xfs) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin&quot; type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.790779651+08:00] loading plugin &quot;io.containerd.snapshotter.v1.devmapper&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">WARN[2022-04-15T14:14:01.790827944+08:00] failed to load plugin io.containerd.snapshotter.v1.devmapper  error=&quot;devmapper not configured&quot;</span><br><span class="line">INFO[2022-04-15T14:14:01.790850382+08:00] loading plugin &quot;io.containerd.snapshotter.v1.aufs&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.793069978+08:00] skip loading plugin &quot;io.containerd.snapshotter.v1.aufs&quot;...  error=&quot;modprobe aufs failed: \&quot;modprobe: FATAL: Module aufs not found.\\n\&quot;: exit status 1: skip plugin&quot; type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.793133838+08:00] loading plugin &quot;io.containerd.snapshotter.v1.native&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.793229323+08:00] loading plugin &quot;io.containerd.snapshotter.v1.overlayfs&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.793392157+08:00] loading plugin &quot;io.containerd.snapshotter.v1.zfs&quot;...  type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.794994033+08:00] skip loading plugin &quot;io.containerd.snapshotter.v1.zfs&quot;...  error=&quot;path /data/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin&quot; type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.795033213+08:00] loading plugin &quot;io.containerd.metadata.v1.bolt&quot;...  type=io.containerd.metadata.v1</span><br><span class="line">WARN[2022-04-15T14:14:01.795068521+08:00] could not use snapshotter devmapper in metadata plugin  error=&quot;devmapper not configured&quot;</span><br><span class="line">INFO[2022-04-15T14:14:01.795097127+08:00] metadata content store policy set             policy=shared</span><br><span class="line">INFO[2022-04-15T14:14:01.795925864+08:00] loading plugin &quot;io.containerd.differ.v1.walking&quot;...  type=io.containerd.differ.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.795966063+08:00] loading plugin &quot;io.containerd.gc.v1.scheduler&quot;...  type=io.containerd.gc.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796083519+08:00] loading plugin &quot;io.containerd.service.v1.containers-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796116546+08:00] loading plugin &quot;io.containerd.service.v1.content-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796143806+08:00] loading plugin &quot;io.containerd.service.v1.diff-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796174035+08:00] loading plugin &quot;io.containerd.service.v1.images-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796201988+08:00] loading plugin &quot;io.containerd.service.v1.leases-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796236554+08:00] loading plugin &quot;io.containerd.service.v1.namespaces-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796274839+08:00] loading plugin &quot;io.containerd.service.v1.snapshots-service&quot;...  type=io.containerd.service.v1</span><br><span class="line">INFO[2022-04-15T14:14:01.796309507+08:00] loading plugin &quot;io.containerd.runtime.v1.linux&quot;...  type=io.containerd.runtime.v1</span><br><span class="line">DEBU[2022-04-15T14:14:01.796470242+08:00] loading tasks in namespace                    namespace=moby</span><br><span class="line">ERRO[2022-04-15T14:14:01.796797091+08:00] connecting to shim                            error=&quot;dial unix \x00/containerd-shim/moby/427b5abebd744817fe9cf8c0aa2febadff17d5905e830d3236bb46fa58d6858b/shim.sock: connect: connection refused&quot; id=427b5abebd744817fe9cf8c0aa2febadff17d5905e830d3236bb46fa58d6858b namespace=moby</span><br><span class="line">WARN[2022-04-15T14:14:01.796844852+08:00] cleaning up after shim dead                   id=427b5abebd744817fe9cf8c0aa2febadff17d5905e830d3236bb46fa58d6858b namespace=moby</span><br><span class="line">DEBU[2022-04-15T14:14:01.809292656+08:00] event published                               ns=moby topic=/tasks/exit type=containerd.events.TaskExit</span><br><span class="line">DEBU[2022-04-15T14:14:01.810296940+08:00] event published                               ns=moby topic=/tasks/delete type=containerd.events.TaskDelete</span><br><span class="line">ERRO[2022-04-15T14:14:01.810491037+08:00] connecting to shim                            error=&quot;dial unix /run/containerd/s/058682ed3ebcc6c9b8d37022b1d379d2d11dbf583467c8d834cc09b3d0c76fea: connect: connection refused&quot; id=44d4a81a122079c684c2a45fcd412c8dc2eef3e4e3569ecae2332ac450b80076 namespace=moby</span><br><span class="line">WARN[2022-04-15T14:14:01.810533770+08:00] cleaning up after shim dead                   id=44d4a81a122079c684c2a45fcd412c8dc2eef3e4e3569ecae2332ac450b80076 namespace=moby</span><br><span class="line">DEBU[2022-04-15T14:14:01.823164929+08:00] event published                               ns=moby topic=/tasks/exit type=containerd.events.TaskExit</span><br><span class="line">DEBU[2022-04-15T14:14:01.823795277+08:00] event published                               ns=moby topic=/tasks/delete type=containerd.events.TaskDelete</span><br><span class="line">ERRO[2022-04-15T14:14:01.823953161+08:00] connecting to shim                            error=&quot;dial unix /run/containerd/s/96bd5e94f86fc8e5752989ee5f22f46924d7deea59c7b8ef11087186f81ca50b: connect: connection refused&quot; id=52aa8ad1be01f3f947bcd2f03197771f1b965b0733f3d4026a58d77979618966 namespace=moby</span><br><span class="line">WARN[2022-04-15T14:14:01.823995417+08:00] cleaning up after shim dead                   id=52aa8ad1be01f3f947bcd2f03197771f1b965b0733f3d4026a58d77979618966 namespace=moby</span><br><span class="line">DEBU[2022-04-15T14:14:01.835741082+08:00] event published                               ns=moby topic=/tasks/exit type=containerd.events.TaskExit</span><br><span class="line">DEBU[2022-04-15T14:14:01.836774408+08:00] event published                               ns=moby topic=/tasks/delete type=containerd.events.TaskDelete</span><br><span class="line">ERRO[2022-04-15T14:14:01.836931317+08:00] connecting to shim                            error=&quot;dial unix \x00/containerd-shim/moby/6b1fb39b4c5ad46612b6755f652d608ef75b3374f719b08a67cac1b7f4ddf646/shim.sock: connect: connection refused&quot; id=6b1fb39b4c5ad46612b6755f652d608ef75b3374f719b08a67cac1b7f4ddf646 namespace=moby</span><br><span class="line">WARN[2022-04-15T14:14:01.836979896+08:00] cleaning up after shim dead                   id=6b1fb39b4c5ad46612b6755f652d608ef75b3374f719b08a67cac1b7f4ddf646 namespace=moby</span><br><span class="line">DEBU[2022-04-15T14:14:01.849530967+08:00] event published                               ns=moby topic=/tasks/exit type=containerd.events.TaskExit</span><br><span class="line">ERRO[2022-04-15T14:14:01.862000189+08:00] delete bundle                                 error=&quot;rename /data/kube/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/6b1fb39b4c5ad46612b6755f652d608ef75b3374f719b08a67cac1b7f4ddf646 /data/kube/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/.6b1fb39b4c5ad46612b6755f652d608ef75b3374f719b08a67cac1b7f4ddf646: file exists&quot;</span><br><span class="line">DEBU[2022-04-15T14:14:01.862166378+08:00] event published                               ns=moby topic=/tasks/delete type=containerd.events.TaskDelete</span><br><span class="line">ERRO[2022-04-15T14:14:01.862300707+08:00] connecting to shim                            error=&quot;dial unix \x00/containerd-shim/moby/b49a6d51e1a8997440bbcb6e9267a76d8ce2f24a684454890ae98600de364f64/shim.sock: connect: connection refused&quot; id=b49a6d51e1a8997440bbcb6e9267a76d8ce2f24a684454890ae98600de364f64 namespace=moby</span><br><span class="line">WARN[2022-04-15T14:14:01.862334378+08:00] cleaning up after shim dead                   id=b49a6d51e1a8997440bbcb6e9267a76d8ce2f24a684454890ae98600de364f64 namespace=moby</span><br><span class="line">DEBU[2022-04-15T14:14:01.877502812+08:00] event published                               ns=moby topic=/tasks/exit type=containerd.events.TaskExit</span><br><span class="line">DEBU[2022-04-15T14:14:01.877994394+08:00] event published                               ns=moby topic=/tasks/delete type=containerd.events.TaskDelete</span><br><span class="line">ERRO[2022-04-15T14:14:01.878143864+08:00] connecting to shim                            error=&quot;dial unix /run/containerd/s/b538e3e9b252cc635009139b544114a233b3cc34ca48925588da77aa15cdf90a: connect: connection refused&quot; id=b902f9e634100578e9ab38eeaf9a27224d843035ae33a368d65efc071393bd2f namespace=moby</span><br><span class="line">WARN[2022-04-15T14:14:01.878183396+08:00] cleaning up after shim dead                   id=b902f9e634100578e9ab38eeaf9a27224d843035ae33a368d65efc071393bd2f namespace=moby</span><br><span class="line">DEBU[2022-04-15T14:14:01.895566880+08:00] event published                               ns=moby topic=/tasks/exit type=containerd.events.TaskExit</span><br><span class="line">DEBU[2022-04-15T14:14:01.896242084+08:00] event published                               ns=moby topic=/tasks/delete type=containerd.events.TaskDelete</span><br><span class="line">ERRO[2022-04-15T14:14:01.896450640+08:00] connecting to shim                            error=&quot;dial unix /run/containerd/s/4d17ae4e022c51c38d6eff249e4a50d6eaceba757f043204ae68b9382271b0da: connect: connection refused&quot; id=dc31940495385987c90e96c333014f1ac7dc7eddb5e3f45acb19877c79e22893 namespace=moby</span><br><span class="line">WARN[2022-04-15T14:14:01.896496784+08:00] cleaning up after shim dead                   id=dc31940495385987c90e96c333014f1ac7dc7eddb5e3f45acb19877c79e22893 namespace=moby</span><br><span class="line">DEBU[2022-04-15T14:14:01.904116791+08:00] garbage collected                             d=7.541562ms</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">0x40c05f</span><br><span class="line">fatal error: bad lfnode address</span><br><span class="line">[signal SIGSEGV: segmentation violation code=0x80 addr=0x0 pc=0x45cf1f]</span><br><span class="line"></span><br><span class="line">runtime stack:</span><br><span class="line">runtime: unexpected return pc for runtime.lfnodeValidate called from 0x0</span><br><span class="line">stack: frame=&#123;sp:0x7f5918d8f5d8, fp:0x7f5918d8f600&#125; stack=[0x7f5918594148,0x7f5918d93d48)</span><br><span class="line">00007f5918d8f4d8:  000000c000000900  01000000000003e8 </span><br><span class="line">00007f5918d8f4e8:  0000000000000004  000000000000001f </span><br><span class="line">00007f5918d8f4f8:  000000000045cf1f &lt;runtime.GoroutineProfile.func2+47&gt;  0000000000000000 </span><br><span class="line">00007f5918d8f508:  0000000000000080  00000000013f1515 </span><br><span class="line">00007f5918d8f518:  00007f5918d8f560  000000000045d961 &lt;runtime.fatalthrow.func1+97&gt; </span><br><span class="line">00007f5918d8f528:  000000c000000900  0000000000430dd4 &lt;runtime.throw+116&gt; </span><br><span class="line">00007f5918d8f538:  00007f5918d8f5a8  0000000000000001 </span><br><span class="line">00007f5918d8f548:  00007f5918d8f5a8  0000000000430dd4 &lt;runtime.throw+116&gt; </span><br><span class="line">00007f5918d8f558:  000000c000000900  00007f5918d8f598 </span><br><span class="line">00007f5918d8f568:  0000000000430fa9 &lt;runtime.fatalthrow+89&gt;  00007f5918d8f578 </span><br><span class="line">00007f5918d8f578:  000000000045d900 &lt;runtime.fatalthrow.func1+0&gt;  000000c000000900 </span><br><span class="line">00007f5918d8f588:  0000000000430dd4 &lt;runtime.throw+116&gt;  00007f5918d8f5a8 </span><br><span class="line">00007f5918d8f598:  00007f5918d8f5c8  0000000000430dd4 &lt;runtime.throw+116&gt; </span><br><span class="line">00007f5918d8f5a8:  00007f5918d8f5b0  000000000045d870 &lt;runtime.throw.func1+0&gt; </span><br><span class="line">00007f5918d8f5b8:  00000000013dce54  0000000000000012 </span><br><span class="line">00007f5918d8f5c8:  00007f5918d8f608  000000000040beba &lt;runtime.lfnodeValidate+170&gt; </span><br><span class="line">00007f5918d8f5d8: &lt;00000000013dce54  0000000000000012 </span><br><span class="line">00007f5918d8f5e8:  00000000004317a0 &lt;runtime.recordForPanic+304&gt;  000000000274594b </span><br><span class="line">00007f5918d8f5f8: !0000000000000000 &gt;0000000000000000 </span><br><span class="line">00007f5918d8f608:  00007f5918d8f648  0000000000000000 </span><br><span class="line">00007f5918d8f618:  00007f5918d8f650  00000000004317a0 &lt;runtime.recordForPanic+304&gt; </span><br><span class="line">00007f5918d8f628:  000000000274594b  0000000000000004 </span><br><span class="line">00007f5918d8f638:  00007f5918d8f670  00000000004317a0 &lt;runtime.recordForPanic+304&gt; </span><br><span class="line">00007f5918d8f648:  00007f5918d8f668  000000000043183d &lt;runtime.printlock+109&gt; </span><br><span class="line">00007f5918d8f658:  00000000027445b0  000000c000074380 </span><br><span class="line">00007f5918d8f668:  00007f5918d8f698  000000000045d8a6 &lt;runtime.throw.func1+54&gt; </span><br><span class="line">00007f5918d8f678:  0000000000431967 &lt;runtime.gwrite+167&gt;  0000000000000002 </span><br><span class="line">00007f5918d8f688:  000000000000002a  00000000014054d6 </span><br><span class="line">00007f5918d8f698:  00007f5918d8f6c8  0000000000430dad &lt;runtime.throw+77&gt; </span><br><span class="line">00007f5918d8f6a8:  00007f5918d8f6b0  000000000045d870 &lt;runtime.throw.func1+0&gt; </span><br><span class="line">00007f5918d8f6b8:  00000000014054d6  000000000000002a </span><br><span class="line">00007f5918d8f6c8:  00007f5918d8f6f8  0000000000446a60 &lt;runtime.sigpanic+1152&gt; </span><br><span class="line">00007f5918d8f6d8:  00000000014054d6  000000000000002a </span><br><span class="line">00007f5918d8f6e8:  00000000013c9053  0000000000000001 </span><br><span class="line">00007f5918d8f6f8:  00007f5918d8f720 </span><br><span class="line">runtime.throw(0x13dce54, 0x12)</span><br><span class="line">        /usr/local/go/src/runtime/panic.go:774 +0x74</span><br><span class="line">runtime: unexpected return pc for runtime.lfnodeValidate called from 0x0</span><br><span class="line">stack: frame=&#123;sp:0x7f5918d8f5d8, fp:0x7f5918d8f600&#125; stack=[0x7f5918594148,0x7f5918d93d48)</span><br><span class="line">00007f5918d8f4d8:  000000c000000900  01000000000003e8 </span><br><span class="line">00007f5918d8f4e8:  0000000000000004  000000000000001f </span><br><span class="line">00007f5918d8f4f8:  000000000045cf1f &lt;runtime.GoroutineProfile.func2+47&gt;  0000000000000000 </span><br><span class="line">00007f5918d8f508:  0000000000000080  00000000013f1515 </span><br><span class="line">00007f5918d8f518:  00007f5918d8f560  000000000045d961 &lt;runtime.fatalthrow.func1+97&gt; </span><br><span class="line">00007f5918d8f528:  000000c000000900  0000000000430dd4 &lt;runtime.throw+116&gt; </span><br><span class="line">00007f5918d8f538:  00007f5918d8f5a8  0000000000000001 </span><br><span class="line">00007f5918d8f548:  00007f5918d8f5a8  0000000000430dd4 &lt;runtime.throw+116&gt; </span><br><span class="line">00007f5918d8f558:  000000c000000900  00007f5918d8f598 </span><br><span class="line">00007f5918d8f568:  0000000000430fa9 &lt;runtime.fatalthrow+89&gt;  00007f5918d8f578 </span><br><span class="line">00007f5918d8f578:  000000000045d900 &lt;runtime.fatalthrow.func1+0&gt;  000000c000000900 </span><br><span class="line">00007f5918d8f588:  0000000000430dd4 &lt;runtime.throw+116&gt;  00007f5918d8f5a8 </span><br><span class="line">00007f5918d8f598:  00007f5918d8f5c8  0000000000430dd4 &lt;runtime.throw+116&gt; </span><br><span class="line">00007f5918d8f5a8:  00007f5918d8f5b0  000000000045d870 &lt;runtime.throw.func1+0&gt; </span><br><span class="line">00007f5918d8f5b8:  00000000013dce54  0000000000000012 </span><br><span class="line">00007f5918d8f5c8:  00007f5918d8f608  000000000040beba &lt;runtime.lfnodeValidate+170&gt; </span><br><span class="line">00007f5918d8f5d8: &lt;00000000013dce54  0000000000000012 </span><br><span class="line">00007f5918d8f5e8:  00000000004317a0 &lt;runtime.recordForPanic+304&gt;  000000000274594b </span><br><span class="line">00007f5918d8f5f8: !0000000000000000 &gt;0000000000000000 </span><br><span class="line">00007f5918d8f608:  00007f5918d8f648  0000000000000000 </span><br><span class="line">00007f5918d8f618:  00007f5918d8f650  00000000004317a0 &lt;runtime.recordForPanic+304&gt; </span><br><span class="line">00007f5918d8f628:  000000000274594b  0000000000000004 </span><br><span class="line">00007f5918d8f638:  00007f5918d8f670  00000000004317a0 &lt;runtime.recordForPanic+304&gt; </span><br><span class="line">00007f5918d8f648:  00007f5918d8f668  000000000043183d &lt;runtime.printlock+109&gt; </span><br><span class="line">00007f5918d8f658:  00000000027445b0  000000c000074380 </span><br><span class="line">00007f5918d8f668:  00007f5918d8f698  000000000045d8a6 &lt;runtime.throw.func1+54&gt; </span><br><span class="line">00007f5918d8f678:  0000000000431967 &lt;runtime.gwrite+167&gt;  0000000000000002 </span><br><span class="line">00007f5918d8f688:  000000000000002a  00000000014054d6 </span><br><span class="line">00007f5918d8f698:  00007f5918d8f6c8  0000000000430dad &lt;runtime.throw+77&gt; </span><br><span class="line">00007f5918d8f6a8:  00007f5918d8f6b0  000000000045d870 &lt;runtime.throw.func1+0&gt; </span><br><span class="line">00007f5918d8f6b8:  00000000014054d6  000000000000002a </span><br><span class="line">00007f5918d8f6c8:  00007f5918d8f6f8  0000000000446a60 &lt;runtime.sigpanic+1152&gt; </span><br><span class="line">00007f5918d8f6d8:  00000000014054d6  000000000000002a </span><br><span class="line">00007f5918d8f6e8:  00000000013c9053  0000000000000001 </span><br><span class="line">00007f5918d8f6f8:  00007f5918d8f720 </span><br><span class="line">runtime.lfnodeValidate(0x0)</span><br><span class="line">        /usr/local/go/src/runtime/lfstack.go:65 +0xaa</span><br><span class="line"></span><br><span class="line">goroutine 1 [chan receive]:</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/containerd/go-runc.(*defaultMonitor).Wait(0x2744360, 0xc0002738c0, 0xc00035e4e0, 0x0, 0x0, 0x3d54454b434f)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/containerd/go-runc/monitor.go:74 +0x50</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/containerd/go-runc.cmdOutput(0xc0002738c0, 0xc0002ee301, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/containerd/go-runc/runc.go:709 +0x14e</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/containerd/go-runc.(*Runc).runOrError(0xc00055ec80, 0xc0002738c0, 0xc0002cee40, 0xc00004ee40)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/containerd/go-runc/runc.go:689 +0x186</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/containerd/go-runc.(*Runc).Delete(0xc00055ec80, 0x1bcd9e0, 0xc0002cee40, 0xc000016796, 0x40, 0xc00062cd77, 0x40, 0xc00055ec80)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/containerd/go-runc/runc.go:302 +0x16a</span><br><span class="line">github.com/containerd/containerd/runtime/v1/linux.(*Runtime).terminate(0xc000240960, 0x1bcd9e0, 0xc0002cee40, 0xc0002cea80, 0xc00010d5d1, 0x4, 0xc000016796, 0x40, 0x1, 0x1)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/runtime/v1/linux/runtime.go:473 +0xfc</span><br><span class="line">github.com/containerd/containerd/runtime/v1/linux.(*Runtime).cleanupAfterDeadShim(0xc000240960, 0x1bcd9e0, 0xc0002cee40, 0xc0002cea80, 0xc00010d5d1, 0x4, 0xc000016796, 0x40, 0x1b938c0, 0xc00003caf0)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/runtime/v1/linux/runtime.go:432 +0x386</span><br><span class="line">github.com/containerd/containerd/runtime/v1/linux.(*Runtime).loadTasks(0xc000240960, 0x1bcd960, 0xc000040098, 0xc00010d5d1, 0x4, 0x0, 0x0, 0xc00062e610, 0x439d71, 0xc000052500)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/runtime/v1/linux/runtime.go:362 +0xa28</span><br><span class="line">github.com/containerd/containerd/runtime/v1/linux.(*Runtime).restoreTasks(0xc000240960, 0x1bcd960, 0xc000040098, 0x1a9f5c0, 0xc000532540, 0x0, 0x0, 0xc00062e898)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/runtime/v1/linux/runtime.go:298 +0x368</span><br><span class="line">github.com/containerd/containerd/runtime/v1/linux.New(0xc00034ea80, 0xc000332c60, 0x2, 0x2, 0x1968f20)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/runtime/v1/linux/runtime.go:125 +0x3db</span><br><span class="line">github.com/containerd/containerd/plugin.(*Registration).Init(0xc00009a1e0, 0xc00034ea80, 0x18c2f40)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/plugin/plugin.go:110 +0x3a</span><br><span class="line">github.com/containerd/containerd/services/server.New(0x1bcd960, 0xc000040098, 0xc00055c480, 0x1, 0x1, 0xc0002079b0)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/services/server/server.go:167 +0xcaa</span><br><span class="line">github.com/containerd/containerd/cmd/containerd/command.App.func1(0xc000558580, 0x0, 0xc000162880)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/cmd/containerd/command/main.go:177 +0x7fa</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/urfave/cli.HandleAction(0x1937d80, 0x1b71550, 0xc000558580, 0xc000558580, 0x0)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/urfave/cli/app.go:523 +0xc0</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/urfave/cli.(*App).Run(0xc000536700, 0xc00003c050, 0x5, 0x5, 0x0, 0x0)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/urfave/cli/app.go:285 +0x5e1</span><br><span class="line">main.main()</span><br><span class="line">        github.com/containerd/containerd/cmd/containerd/main.go:33 +0x51</span><br><span class="line"></span><br><span class="line">goroutine 6 [syscall]:</span><br><span class="line">os/signal.signal_recv(0x0)</span><br><span class="line">        /usr/local/go/src/runtime/sigqueue.go:147 +0x9e</span><br><span class="line">os/signal.loop()</span><br><span class="line">        /usr/local/go/src/os/signal/signal_unix.go:23 +0x24</span><br><span class="line">created by os/signal.init.0</span><br><span class="line">        /usr/local/go/src/os/signal/signal_unix.go:29 +0x43</span><br><span class="line"></span><br><span class="line">goroutine 7 [chan receive]:</span><br><span class="line">github.com/containerd/containerd/vendor/k8s.io/klog.(*loggingT).flushDaemon(0x2721260)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/k8s.io/klog/klog.go:1010 +0x8d</span><br><span class="line">created by github.com/containerd/containerd/vendor/k8s.io/klog.init.0</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/k8s.io/klog/klog.go:411 +0xd8</span><br><span class="line"></span><br><span class="line">goroutine 43 [select]:</span><br><span class="line">github.com/containerd/containerd/cmd/containerd/command.handleSignals.func1(0xc000551380, 0xc000551320, 0x1bcd960, 0xc000040098, 0xc00054c300)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/cmd/containerd/command/main_unix.go:44 +0xf2</span><br><span class="line">created by github.com/containerd/containerd/cmd/containerd/command.handleSignals</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/cmd/containerd/command/main_unix.go:41 +0x8b</span><br><span class="line"></span><br><span class="line">goroutine 11 [select]:</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/docker/go-events.(*Broadcaster).run(0xc00003c0f0)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/docker/go-events/broadcast.go:117 +0x1b3</span><br><span class="line">created by github.com/containerd/containerd/vendor/github.com/docker/go-events.NewBroadcaster</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/docker/go-events/broadcast.go:39 +0x1b0</span><br><span class="line"></span><br><span class="line">goroutine 114 [runnable]:</span><br><span class="line">os/exec.(*Cmd).Start.func2(0xc0002738c0)</span><br><span class="line">        /usr/local/go/src/os/exec/exec.go:448 +0xc6</span><br><span class="line">created by os/exec.(*Cmd).Start</span><br><span class="line">        /usr/local/go/src/os/exec/exec.go:447 +0x6d2</span><br><span class="line"></span><br><span class="line">goroutine 47 [select]:</span><br><span class="line">github.com/containerd/containerd/gc/scheduler.(*gcScheduler).run(0xc0002408a0, 0x1bcd960, 0xc000040098)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:268 +0x1ce</span><br><span class="line">created by github.com/containerd/containerd/gc/scheduler.init.0.func1</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:132 +0x429</span><br><span class="line"></span><br><span class="line">goroutine 115 [runnable]:</span><br><span class="line">os/exec.(*Cmd).Wait(0xc0002738c0, 0x0, 0x0)</span><br><span class="line">        /usr/local/go/src/os/exec/exec.go:514 +0x127</span><br><span class="line">github.com/containerd/containerd/vendor/github.com/containerd/go-runc.(*defaultMonitor).Start.func1(0xc0002738c0, 0xc00035e4e0)</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/containerd/go-runc/monitor.go:55 +0x31</span><br><span class="line">created by github.com/containerd/containerd/vendor/github.com/containerd/go-runc.(*defaultMonitor).Start</span><br><span class="line">        /tmp/tmp.0JSku0IZFM/src/github.com/containerd/containerd/vendor/github.com/containerd/go-runc/monitor.go:53 +0xa7</span><br></pre></td></tr></table></figure><p>上面只是偶尔的报错，偶尔也会报错 <code>segment fault</code>，同时根目录也有 coredump 生成：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ ll /</span><br><span class="line">总用量 145912</span><br><span class="line">lrwxrwxrwx.   1 root root         7 10月 13 2021 bin -&gt; usr/bin</span><br><span class="line">dr-xr-xr-x.   5 root root      4096 4月  11 14:55 boot</span><br><span class="line">-rw-------    1 root root 230711296 4月  15 13:59 core.28650</span><br><span class="line">-rw-------    1 root root 187424768 4月  15 13:46 core.3977</span><br><span class="line">drwxr-xr-x   12 xxx  xxx        156 10月 29 11:32 data</span><br><span class="line">drwxr-xr-x   19 root root      3180 4月  15 12:38 dev</span><br><span class="line">drwxr-xr-x.  85 root root      8192 4月  15 12:44 etc</span><br><span class="line">drwxr-xr-x.   3 root root        17 10月 28 13:30 home</span><br><span class="line">lrwxrwxrwx.   1 root root         7 10月 13 2021 lib -&gt; usr/lib</span><br><span class="line">lrwxrwxrwx.   1 root root         9 10月 13 2021 lib64 -&gt; usr/lib64</span><br><span class="line">drwxr-xr-x.   3 root root       127 10月 28 13:32 media</span><br><span class="line">drwxr-xr-x.   2 root root         6 4月  11 2018 mnt</span><br><span class="line">drwxr-xr-x.   3 root root        24 10月 28 13:38 opt</span><br><span class="line">dr-xr-xr-x  253 root root         0 4月  15 12:38 proc</span><br><span class="line">dr-xr-x---.   7 root root       258 4月  15 13:13 root</span><br><span class="line">drwxr-xr-x   30 root root       960 4月  15 14:11 run</span><br><span class="line">lrwxrwxrwx.   1 root root         8 10月 13 2021 sbin -&gt; usr/sbin</span><br><span class="line">drwxr-xr-x.   2 root root         6 4月  11 2018 srv</span><br><span class="line">dr-xr-xr-x   13 root root         0 4月  15 13:12 sys</span><br><span class="line">drwxrwxrwt.  14 root root      4096 4月  15 14:28 tmp</span><br><span class="line">drwxr-xr-x.  13 root root       155 10月 13 2021 usr</span><br><span class="line">drwxr-xr-x.  19 root root       267 10月 13 2021 var</span><br></pre></td></tr></table></figure><p>对比了二进制文件，也没损坏，查看了进程，也没有啥安全软件，查看下系统日志，其实在上面排查过程中 ssh 也会偶尔断开，然后 strace 也是没有啥头绪。</p><p>系统日志里过滤一些无用的信息后，发现 bash 也会 <code>segfault</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Apr 15 13:46:32 xxx supervisord: 2022-04-15 13:46:32,958 INFO exited: prometheus_00 (exit status 2; expected)</span><br><span class="line">Apr 15 13:46:33 xxx supervisord: 2022-04-15 13:46:33,962 INFO spawned: &#x27;prometheus_00&#x27; with pid 3751</span><br><span class="line">Apr 15 13:46:35 xxx supervisord: 2022-04-15 13:46:35,379 INFO success: prometheus_00 entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)</span><br><span class="line">Apr 15 13:46:35 xxx dockerd: failed to start daemon: failed to dial &quot;/run/containerd/containerd.sock&quot;: failed to dial &quot;/run/containerd/containerd.sock&quot;: context deadline exceeded</span><br><span class="line">Apr 15 13:46:35 xxx systemd: docker.service: main process exited, code=exited, status=1/FAILURE</span><br><span class="line">Apr 15 13:46:35 xxx systemd: Unit docker.service entered failed state.</span><br><span class="line">Apr 15 13:46:35 xxx systemd: docker.service failed.</span><br><span class="line">Apr 15 13:46:40 xxx systemd: docker.service holdoff time over, scheduling restart.</span><br><span class="line">Apr 15 13:46:40 xxx systemd: Starting Docker Application Container Engine...</span><br><span class="line">Apr 15 13:46:40 xxx systemd: Started Docker Application Container Engine.</span><br><span class="line">Apr 15 13:46:40 xxx systemd: docker.service: main process exited, code=killed, status=11/SEGV</span><br><span class="line">Apr 15 13:46:40 xxx systemd: Unit docker.service entered failed state.</span><br><span class="line">Apr 15 13:46:40 xxx systemd: docker.service failed.</span><br><span class="line">Apr 15 13:46:42 xxx kernel: bash[4028]: segfault at fa8 ip 0000000000440c58 sp 00007fff8528e830 error 4 in bash[400000+de000]</span><br></pre></td></tr></table></figure><p>单独看看 <code>segfault</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ grep kernal /var/log/messages | grep -i segfault</span><br><span class="line"></span><br><span class="line">Apr 15 11:17:31 xxx kernel: celery[13267]: segfault at 6de410 ip 000000000041b720 sp 00007ffccf1813c0 error 4 in python3.7[400000+293000]</span><br><span class="line">Apr 15 11:20:35 xxx kernel: redis-server[21118]: segfault at 5dd030 ip 00000000005dd030 sp 00007ffc05cbe440 error 14 in redis-server[6dd000+1000]</span><br><span class="line">Apr 15 11:20:35 xxx kernel: supervisord[20844]: segfault at 29 ip 000000000045c030 sp 00007ffd48aa29d0 error 4 in python3.7[400000+293000]</span><br><span class="line">Apr 15 11:20:51 xxx kernel: gunicorn[23241]: segfault at 8 ip 00000000004272c8 sp 00007ffc0c4f34e0 error 4 in python3.7[400000+293000]</span><br><span class="line">Apr 15 11:20:56 xxx kernel: celery[23193]: segfault at 10 ip 0000000000454a29 sp 00007ffd17acce78 error 4 in python3.7[400000+293000]</span><br><span class="line">Apr 15 11:20:56 xxx kernel: python[23196]: segfault at 154c0ab0 ip 0000000000420eb0 sp 00007fff43b9fe08 error 6 in python3.7[400000+293000]</span><br><span class="line">Apr 15 12:38:34 xxx kernel: init.ipv6-globa[1154]: segfault at 68 ip 000000000044f62f sp 00007ffc2cf307a8 error 6 in bash[400000+de000]</span><br><span class="line">Apr 15 12:59:25 xxx kernel: bash[11112]: segfault at 18 ip 0000000000449dc9 sp 00007ffffbc2dac8 error 4 in bash[400000+de000]</span><br><span class="line">Apr 15 13:00:58 xxx kernel: bash[14117]: segfault at ffffffff8d48ffff ip 0000000000440c36 sp 00007ffe9be29640 error 7 in bash[400000+de000]</span><br><span class="line">Apr 15 13:01:19 xxx kernel: strace[14852]: segfault at 0 ip           (null) sp 00007ffc836a2be8 error 14 in strace[400000+f7000]</span><br><span class="line">Apr 15 13:10:44 xxx kernel: grep[2587]: segfault at a0d ip 000000000040c43f sp 00007ffcda056be8 error 4 in grep[400000+25000]</span><br><span class="line">Apr 15 13:14:06 xxx kernel: strace[9484]: segfault at ffffffff89489abc ip 00000000004336c2 sp 00007ffcb217a758 error 7 in strace[400000+f7000]</span><br><span class="line">Apr 15 13:27:57 xxx kernel: grep[32713]: segfault at 0 ip           (null) sp 00007ffebf181d10 error 14 in grep[400000+25000]</span><br><span class="line">Apr 15 13:30:22 xxx kernel: bash[5138]: segfault at 108 ip 000000000040cc32 sp 00007ffd55de2728 error 4 in bash[400000+de000]</span><br><span class="line">Apr 15 13:35:56 xxx kernel: strace[15846]: segfault at 0 ip           (null) sp 00007ffef0a931e0 error 14 in strace[400000+f7000]</span><br><span class="line">Apr 15 13:36:27 xxx kernel: bash[16818]: segfault at 33173b0 ip 000000000046d584 sp 00007ffdb71ff5c8 error 6 in bash[400000+de000]</span><br><span class="line">Apr 15 13:37:04 xxx kernel: bash[17993]: segfault at 46a0 ip 0000000000440c58 sp 00007fffd76cf1e0 error 4 in bash[400000+de000]</span><br><span class="line">Apr 15 13:39:49 xxx kernel: bash[23124]: segfault at 5aa0 ip 0000000000440c58 sp 00007fff98271ac0 error 4 in bash[400000+de000]</span><br><span class="line">Apr 15 13:46:42 xxx kernel: bash[4028]: segfault at fa8 ip 0000000000440c58 sp 00007fff8528e830 error 4 in bash[400000+de000]</span><br><span class="line">Apr 15 13:59:04 xxx kernel: redis_exporter-[21520]: segfault at 43b8ba ip 000000000043b892 sp 000000c000057f48 error 7 in redis_exporter-1.23.1.linux-x86_64[400000+41f000]</span><br><span class="line">Apr 15 14:12:12 xxx kernel: grepconf.sh[20714]: segfault at 0 ip           (null) sp 00007ffe3009c158 error 14 in bash[400000+de000]</span><br><span class="line">Apr 15 14:14:43 xxx kernel: bash[25463]: segfault at 313f4023 ip 00000000313f4023 sp 00007ffd25075948 error 14 in ISO8859-1.so[7f6335587000+2000]</span><br><span class="line">Apr 15 14:22:32 xxx kernel: bash[7390]: segfault at 18 ip 0000000000449dc9 sp 00007ffc765d0558 error 4 in bash[400000+de000]</span><br><span class="line">Apr 15 14:38:21 xxx kernel: prometheus[26806]: segfault at 440eb5 ip 0000000000421391 sp 000000c0002f1f38 error 7 in prometheus[400000+20ec000]</span><br><span class="line">Apr 15 14:38:21 xxx kernel: prometheus[26801]: segfault at bffffffff8 ip 0000000000440ea3 sp 000000c000000000 error 6 in prometheus[400000+20ec000]</span><br></pre></td></tr></table></figure><p>看了下内存容量也正常，很多东西都会触发 segmenft fault，但是最常见的就是内存越界，但是根据系统日志看并不存在内存越界（毕竟这么多进程都 segfault，不可能这么多进程代码写的有问题），使用 <code>rpm -V glibc</code> 也没看到 so 文件被修改，初步怀疑客户的宿主机内存有问题，让客户迁移下这台机器。</p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>2022&#x2F;04&#x2F;22 反馈迁移后一切都正常了。</p><h2 id="coredump-的配置参考"><a href="#coredump-的配置参考" class="headerlink" title="coredump 的配置参考"></a>coredump 的配置参考</h2><p>参考文章 <a href="https://www.cnblogs.com/arnoldlu/p/11160510.html">coredump配置、产生、分析以及分析示例</a> coredump 的一些配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/kernel/core_pattern </span><br><span class="line">core</span><br></pre></td></tr></table></figure><p>临时修改</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;core-%e-%p-%t-%s&quot; &gt; /proc/sys/kernel/core_pattern </span><br></pre></td></tr></table></figure><p>参数说明：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%% - 单个%字符</span><br><span class="line">%p - 添加pid</span><br><span class="line">%u - 添加当前uid</span><br><span class="line">%g - 添加当前gid</span><br><span class="line">%s - 添加导致产生core的信号</span><br><span class="line">%t - 添加core文件生成时的unix时间</span><br><span class="line">%h - 添加主机名</span><br><span class="line">%e - 添加程序文件名 </span><br></pre></td></tr></table></figure><p>sysctl 固化</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kernel.core_pattern=core-%e-%p-%t-%s</span><br><span class="line">kernel.core_uses_pid=1</span><br></pre></td></tr></table></figure><p><code>limit.d/*.conf</code> 配置 coredump 文件限制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* soft core 1024</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;客户环境 docker 和 containerd 启动时不时 segment fault 的一次处理过程。&lt;/p&gt;</summary>
    
    
    
    
    <category term="linux" scheme="http://zhangguanzhang.github.io/tags/linux/"/>
    
    <category term="segfault" scheme="http://zhangguanzhang.github.io/tags/segfault/"/>
    
  </entry>
  
  <entry>
    <title>ecs 中毒的一次处理过程</title>
    <link href="http://zhangguanzhang.github.io/2022/04/21/ecs-xmrig/"/>
    <id>http://zhangguanzhang.github.io/2022/04/21/ecs-xmrig/</id>
    <published>2022-04-21T19:17:30.000Z</published>
    <updated>2022-04-21T19:17:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>一次客户 ecs 中毒的处理过程，可以给读者参考下中毒的处理过程。</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>客户机器中毒了，pm 找我来让处理下，记录下，给其他人做个处理过程的参考。</p><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><p>机器是 centos ，先利用 <code>rpm -V &lt;pkg_name&gt;</code> 确认基础的排查命令没被修改过：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rpm -qf `<span class="built_in">which</span> ps`</span><br><span class="line">procps-ng-3.3.10-23.el7.x86_64</span><br><span class="line">$ rpm -V procps-ng </span><br><span class="line">$ rpm -qf `<span class="built_in">which</span> top`</span><br><span class="line">procps-ng-3.3.10-23.el7.x86_64</span><br><span class="line"><span class="comment"># 看sshd 的 so被修改了没，配置文件也可以看下</span></span><br><span class="line">$ rpm -v `<span class="built_in">which</span> sshd`</span><br><span class="line">S.5....T.  c /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p>top 看到异常 cpu 的进程占用 cpu 很高：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ top</span><br><span class="line">top - 19:44:29 up 34 days,  5:08,  4 users,  load average: 612.03, 617.15, 482.75</span><br><span class="line">Tasks: 2014 total,  66 running, 1946 sleeping,   0 stopped,   2 zombie</span><br><span class="line">%Cpu(s): 96.6 us,  3.1 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st</span><br><span class="line">KiB Mem : 13186040+total,  2722452 free, 48820448 used, 80317504 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used. 78946784 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                                                </span><br><span class="line"> 1206 root      20   0 5251748   2.3g   3584 S  2956  1.8 465:37.77 ld-linux-x86-64</span><br></pre></td></tr></table></figure><p>给它 <code>STOP</code> 信号不让 cpu 切换到它，而不是直接 kill 掉它：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -STOP 1206</span><br></pre></td></tr></table></figure><p>查看来源和清理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ll /proc/1206/exe</span><br><span class="line">lrwxrwxrwx 1 root root 0 Apr 21 19:44 /proc/1206/exe -&gt; /dev/shm/.x/stak/ld-linux-x86-64.so.2</span><br></pre></td></tr></table></figure><h3 id="清理定时任务"><a href="#清理定时任务" class="headerlink" title="清理定时任务"></a>清理定时任务</h3><p>排查定时任务，发现有内容，清理掉， crond 的子目录也看下，文件内容和多了的子文件也处理下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ crontab -l</span><br><span class="line">* * * * * /dev/shm/.x/upd &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">@reboot /dev/shm/.x/upd &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line">$ find /etc/cron.* -type f </span><br><span class="line">/etc/cron.d/0hourly</span><br><span class="line">/etc/cron.d/sysstat</span><br><span class="line">/etc/cron.daily/logrotate</span><br><span class="line">/etc/cron.daily/man-db.cron</span><br><span class="line">/etc/cron.deny</span><br><span class="line">/etc/cron.hourly/0anacron</span><br><span class="line"></span><br><span class="line"># 查看目录是否有其他用户的 crontab 文件</span><br><span class="line">$ ls -l /var/spool/cron/</span><br></pre></td></tr></table></figure><p>查看下进程树，是否有父进程拉起 <code>1206</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">$ pstree -sp 1206</span><br><span class="line">systemd(1)───ld-linux-x86-64(1206)─┬─&#123;ld-linux-x86-64&#125;(1209)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(1211)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(1216)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(1217)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(1218)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6436)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6437)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6439)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6440)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6441)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6443)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6471)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6472)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6476)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6484)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6489)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6495)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6501)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6504)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6505)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6508)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6509)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6511)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6523)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6527)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6529)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6531)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6535)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6547)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6554)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6563)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6567)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6568)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6569)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6572)</span><br><span class="line">                                   ├─&#123;ld-linux-x86-64&#125;(6579)</span><br><span class="line">                                   └─&#123;ld-linux-x86-64&#125;(6580)</span><br></pre></td></tr></table></figure><p>发现并没有，查看下进程的 <code>cmdline</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ xargs -0 &lt; /proc/1206/cmdline</span><br><span class="line">xmrig   </span><br><span class="line">    --library-path stak stak/xmrig -o 185.82.200.52:443 -k</span><br></pre></td></tr></table></figure><h3 id="检查系统的-so-和开机启动项"><a href="#检查系统的-so-和开机启动项" class="headerlink" title="检查系统的 so 和开机启动项"></a>检查系统的 so 和开机启动项</h3><p>搜了下这个 ip 是外国的，查看下 ld 的 so 导入配置文件，看看是否有被加入额外的 so 导入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ rpm -qf /etc/ld.so.conf</span><br><span class="line">glibc-2.17-260.el7.x86_64</span><br><span class="line"># glibc 也提供了很多基础的 so，这步同时也可以看出来</span><br><span class="line"># 基础的 so 有被替换不</span><br><span class="line">$ rpm -V glibc</span><br></pre></td></tr></table></figure><p>同理查看下 systemd 的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ rpm -V systemd</span><br><span class="line">.M.......  c /etc/machine-id</span><br><span class="line">SM5....T.  c /etc/rc.d/rc.local # 这个文件也记得查下</span><br><span class="line">S.5....T.  c /etc/systemd/system.conf</span><br><span class="line">.M.......  g /etc/udev/hwdb.bin</span><br><span class="line">.M.......  g /var/lib/systemd/random-seed</span><br><span class="line">.M....G..  g /var/log/journal</span><br><span class="line">.M....G..  g /var/log/wtmp</span><br><span class="line">.M....G..  g /var/run/utmp</span><br><span class="line"># 查看下有没有被添加 systemd 的开机启动任务，异常的 timer</span><br><span class="line">$ systemctl list-units</span><br></pre></td></tr></table></figure><h3 id="清理进程相关"><a href="#清理进程相关" class="headerlink" title="清理进程相关"></a>清理进程相关</h3><p>我们环境是 k8s 和 docker 的，以下情况都不会发生：</p><ul><li>etcd 没证书，</li><li>kubelet 的 http 可写，</li><li>docker 开网络端口不 tls</li><li>redis 无密码</li></ul><p>看了下我们配置的部署配置文件，初步怀疑是一个有 sudo 的弱密码用户被爆破导致的中毒，查看了具有 sudo 权限和 root 的 <code>~/.ssh/authorized_keys</code> 也没被添加别人的公钥（有的话记得清理下），开始删除挖矿进程的目录：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /dev/shm/.x/</span><br><span class="line">kill -9 1206</span><br></pre></td></tr></table></figure><h3 id="排查网络"><a href="#排查网络" class="headerlink" title="排查网络"></a>排查网络</h3><p>看看是否还有其他后台进程上报或者下载的，看了下 udp 的正常，tcp监听的端口也没莫名其妙的端口，所以提取所有活跃的 tcp 连接 ip 看看有异常的 IP 没：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -ant |&amp; grep -Po &#x27;(\d&#123;1,3&#125;\.)&#123;3&#125;\d&#123;1,3&#125;&#x27; | sort | grep -v 10.187.0 | uniq -c</span><br><span class="line">     49 0.0.0.0</span><br><span class="line">      2 119.82.135.65</span><br><span class="line">    111 127.0.0.1</span><br><span class="line">      4 169.254.169.254</span><br><span class="line">   2271 192.168.0.235</span><br><span class="line">     13 2xx.1xx.15.161</span><br><span class="line">      1 3x.1xx.2x.7</span><br><span class="line">     27 4x.x.1xx.x3</span><br><span class="line">      1 xx.1xx.6x.x54</span><br><span class="line"></span><br><span class="line">$ netstat -ant | grep 119.82.135.65</span><br><span class="line">tcp        0   1281 192.168.0.235:22        119.82.135.65:38525     LAST_ACK   </span><br><span class="line">tcp        0      1 192.168.0.235:22        119.82.135.65:54598     LAST_ACK   </span><br><span class="line">$ lsof -nPi :38525</span><br><span class="line">$ lsof -nPi :54598</span><br><span class="line">$ </span><br></pre></td></tr></table></figure><p>看了下只有不断被外国 IP 暴力 ssh 的 IP，其余几个 IP 是我和客户那边的人员 IP。让客户改密码后再观察下。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>只是列举了大概的排查范围，有其他的自己独特的排查范围也可以尝试下。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一次客户 ecs 中毒的处理过程，可以给读者参考下中毒的处理过程。&lt;/p&gt;</summary>
    
    
    
    
    <category term="ecs" scheme="http://zhangguanzhang.github.io/tags/ecs/"/>
    
    <category term="xmrig" scheme="http://zhangguanzhang.github.io/tags/xmrig/"/>
    
  </entry>
  
  <entry>
    <title>无数据的 tcp 链接被 SDN/防火墙 干掉的一种处理办法</title>
    <link href="http://zhangguanzhang.github.io/2022/04/11/sdn-tcp-keepalive/"/>
    <id>http://zhangguanzhang.github.io/2022/04/11/sdn-tcp-keepalive/</id>
    <published>2022-04-11T11:28:30.000Z</published>
    <updated>2022-04-11T11:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>客户的环境下，业务运行在 dmz 区，mysql 在非 dmz 区，业务连 mysql 的空闲 tcp 连接 240s 后会被 SDN 干掉，本文实际介绍一种不动业务(代码)，利用代理的解决办法</p><span id="more"></span><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>客户提供的环境是 ACS 虚拟化平台，我们业务部署在他们的 dmz 区，mysql 他们提供的，在非 dmz 区，部署后有个问题就是页面经常 504，504 后刷新下就好了，最后排查到是业务连 mysql 的 tcp 连接没有数据传输超过 240s 后会被 SDN 干掉。</p><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>业务的产品挺多的，业务的 db 连接池探活就立刻反馈产品，让下个版本加进去避免这种问题，但是客户现场看看是否有不动业务的解决办法，后面大致看了下 tcp 的 keepalive 可能解决。</p><h3 id="为啥需要-tcp-的-keepalive"><a href="#为啥需要-tcp-的-keepalive" class="headerlink" title="为啥需要 tcp 的 keepalive"></a>为啥需要 tcp 的 keepalive</h3><p>不是 vrrp 和 lvs 的 keepalived 那个，其实这个问题现象和客户的 SDN 关系不大（我意思是说没必要去要求客户调整 SDN 的配置啥的），常见的园区 NAT 网络环境下也有类似问题：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 假如内网本机访问公网的 1.1.1.1:80 </span><br><span class="line"></span><br><span class="line"># client 端会随机分配一个 client port 用于和目标 ip 建立 tcp 连接</span><br><span class="line">CLIENT 192.168.1.2:47914 </span><br><span class="line">    ||</span><br><span class="line">    ||</span><br><span class="line">    ||</span><br><span class="line">    \/</span><br><span class="line">GW/FW SNAT # 网关或者边界有公网 ip 的防火墙做 SNAT</span><br><span class="line">    ||</span><br><span class="line">    ||</span><br><span class="line">    ||</span><br><span class="line">    \/</span><br><span class="line">REAL SERVER 1.1.1.1:80</span><br></pre></td></tr></table></figure><p>假如边界防火墙的公网 IP 为 <code>61.183.112.202</code>，在客户端访问的时候会有个管理 nat 条目的表：</p><table><thead><tr><th>内网IP</th><th>内网IP的端口</th><th>本身的端口</th><th>目的主机IP</th><th>目的主机端口</th></tr></thead><tbody><tr><td>192.168.1.2</td><td>47914</td><td>52617</td><td>1.1.1.1</td><td>80</td></tr></tbody></table><p>远端的 <code>1.1.1.1:80</code> 看到的 client tcp 信息是 <code>61.183.112.202:52617</code>，边界的公网防火墙由于园区内设备太多，这个端口转换表由于端口数量有限（0~65535），对于过期的记录，需要删除掉。大体过程如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 下面字符画推荐 pc 端浏览，否则会错位</span><br><span class="line"></span><br><span class="line">      client端       中间设备         服务端</span><br><span class="line">      └──┬──┘        └──┬──┘        └──┬──┘</span><br><span class="line">         │    ┌────┐    │              │</span><br><span class="line">         ├────┼data┼───►│    ┌────┐    │</span><br><span class="line">         │    └────┘    ├────┼data┼───►│</span><br><span class="line">         │              │    └────┘    │</span><br><span class="line">         │              │    ┌────┐    │</span><br><span class="line">         │    ┌────┐    │◄───┼ACK ┼────┤</span><br><span class="line">         │◄───┼ACK ┼────┤    └────┘    │</span><br><span class="line">         │    └────┘   /│              │</span><br><span class="line">         │            │ │              │</span><br><span class="line">         │            │ │              │</span><br><span class="line">         │    no data&lt;  │              │</span><br><span class="line">         │            │ │              │</span><br><span class="line">         │            │ │              │</span><br><span class="line">         │             \│              │</span><br><span class="line">         │              │长时间无数据交互│</span><br><span class="line">         │              │设备删掉表条目 │</span><br><span class="line">         │    ┌────┐    │              │</span><br><span class="line">client   ├────┼data┼───►│无连接信息     │</span><br><span class="line">发送数据  │    └────┘    │直接发送RST    │</span><br><span class="line">         │    ┌────┐    │ 或丢弃        │</span><br><span class="line">应用异常  │◄───┼RST ┼────┤              │</span><br><span class="line">         │    └────┘    │              │</span><br></pre></td></tr></table></figure><p>现场的 SDN 就是 240s 后干掉这个 tcp 连接，解决办法就是 TCP 这层的 keepalive 机制维持长连接，让网关的 nat 条目 ttl 保活。</p><h3 id="tcp-keepalive-介绍"><a href="#tcp-keepalive-介绍" class="headerlink" title="tcp keepalive 介绍"></a>tcp keepalive 介绍</h3><p>相关的内核参数有三个：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -a |&amp; grep tcp.keepalive_</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 75</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 9</span><br><span class="line">net.ipv4.tcp_keepalive_time = 7200</span><br></pre></td></tr></table></figure><p>启用 tcp keepalive 的一端，在没有数据传输的时候，会有定时器（也有人翻译称为计数器）工作，到了 <code>tcp_keepalive_time</code> 秒还没有数据传输，就发一次 TCP 探测包。每隔 <code>tcp_keepalive_intvl</code> 发一次，如果首次对端响应 keepalive 报文，后面就不发送了，如果没响应也就是一直 <code>tcp_keepalive_probes</code> 次发送都没响应后，就会认为对方挂了。</p><blockquote><ul><li>TCP 探测包是一个纯 ACK 包（<a href="https://tools.ietf.org/html/rfc1122#section-4.2.3.6">RFC1122#TCP Keep-Alives</a> 规范建议：不应该包含任何数据，但也可以包含1个无意义的字节，比如0x0），其 Seq号 与上一个包是重复的，所以其实探测保活报文不在窗口控制范围内。</li></ul></blockquote><p>我们调整了业务机器上的这三个参数，但是还是依旧的问题，发现必须在应用层创建 socket 的时候设置 <code>SO_KEEPALIVE</code> 套接字选项才能生效。例如 c 语言：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conn.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, True)</span><br><span class="line">conn.setsockopt(socket.SOL_TCP, socket.TCP_KEEPIDLE, <span class="number">20</span>) # 覆盖tcp_keepalive_time</span><br><span class="line">conn.setsockopt(socket.SOL_TCP, socket.TCP_KEEPCNT, <span class="number">5</span>)  # 覆盖tcp_keepalive_probes</span><br><span class="line">conn.setsockopt(socket.SOL_TCP, socket.TCP_KEEPINTVL, <span class="number">10</span>) # 覆盖tcp_keepalive_intvl</span><br></pre></td></tr></table></figure><p>其他语言，例如 golang 的话可以看这个文章 <a href="https://zhuanlan.zhihu.com/p/69337371">知乎: golang 程序开启 tcp keepalive</a></p><h3 id="nginx-tcp-代理思路"><a href="#nginx-tcp-代理思路" class="headerlink" title="nginx tcp 代理思路"></a>nginx tcp 代理思路</h3><p>和领导讨论后说用 nginx 做代理试下，根据 nginx 官方文档的 <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#listen">listen 字段</a> 的 <code>[so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]]</code> 看到 nginx 可以开启 <code>so_keepalive</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    # http://nginx.org/en/docs/http/ngx_http_core_module.html#listen</span><br><span class="line">    #listen 0.0.0.0:3306 so_keepalive=on;</span><br><span class="line">    # 如果上面这样就使用 内核参数的值，也可以自定义，也就是下面这样对应三个参数</span><br><span class="line">    listen 0.0.0.0:3306 so_keepalive=60s:20:10;</span><br><span class="line">    proxy_pass xxxx:3306;</span><br><span class="line">    </span><br><span class="line">    #建立连接时间</span><br><span class="line">    proxy_connect_timeout 5s;</span><br><span class="line">    #保持连接时间</span><br><span class="line">    proxy_timeout 3600s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试了下发现还是不行，本地搭建个环境试试。</p><h3 id="本地环境实战"><a href="#本地环境实战" class="headerlink" title="本地环境实战"></a>本地环境实战</h3><p>机器信息：</p><table><thead><tr><th>IP</th><th>role</th></tr></thead><tbody><tr><td>192.168.2.111</td><td>mysql</td></tr><tr><td>192.168.2.112</td><td>nginx</td></tr></tbody></table><p><code>192.168.2.111</code> 上利用 docker 起个 mysql:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">mysql:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql:5.7</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">mysql</span> </span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">3306</span><span class="string">:3306</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./mysql:/var/lib/mysql</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MYSQL_DATABASE:</span> <span class="string">zgz</span></span><br><span class="line">      <span class="attr">MYSQL_USER:</span> <span class="string">zgz</span></span><br><span class="line">      <span class="attr">MYSQL_PASSWORD:</span> <span class="string">zhangguanzhang</span></span><br><span class="line">      <span class="attr">MYSQL_ROOT_PASSWORD:</span> <span class="string">zhangguanzhang</span></span><br><span class="line">    <span class="attr">logging:</span></span><br><span class="line">      <span class="attr">driver:</span> <span class="string">json-file</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="attr">max-size:</span> <span class="string">20k</span></span><br><span class="line">        <span class="attr">max-file:</span> <span class="string">&#x27;3&#x27;</span></span><br></pre></td></tr></table></figure><p><code>192.168.2.112</code> 上利用 docker 起个 nginx 做代理:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3.4&#x27;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nginx:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:alpine</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">proxy</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">proxy</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./nginx.conf:/etc/nginx/nginx.conf</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./conf.d/:/etc/nginx/conf.d/</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./stream.d/:/etc/nginx/stream.d/</span></span><br><span class="line">    <span class="attr">network_mode:</span> <span class="string">&quot;host&quot;</span></span><br><span class="line">    <span class="attr">logging:</span></span><br><span class="line">      <span class="attr">driver:</span> <span class="string">json-file</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="attr">max-file:</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line">        <span class="attr">max-size:</span> <span class="string">100m</span></span><br></pre></td></tr></table></figure><p><code>nginx.conf</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">user  nginx;</span><br><span class="line">worker_processes  auto;</span><br><span class="line"></span><br><span class="line">error_log  /var/log/nginx/error.log notice;</span><br><span class="line">pid        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       /etc/nginx/mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;</span><br><span class="line">                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;</span><br><span class="line">                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line">    #tcp_nopush     on;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    #gzip  on;</span><br><span class="line"></span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">&#125;</span><br><span class="line">stream &#123;</span><br><span class="line">    include /etc/nginx/stream.d/*.conf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>stream.d/test.conf</code> :</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    # http://nginx.org/en/docs/http/ngx_http_core_module.html#listen</span><br><span class="line">    listen 0.0.0.0:3307 so_keepalive=60s:20:9;</span><br><span class="line">    proxy_pass 192.168.2.111:3306;</span><br><span class="line">    </span><br><span class="line">    #建立连接时间</span><br><span class="line">    proxy_connect_timeout 2s;</span><br><span class="line">    #保持连接时间</span><br><span class="line">    #proxy_timeout 3600s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>起来后用 mysql的镜像起 mysql 客户端:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -ti --net host mysql:5.7 bash</span><br><span class="line">mysql -u root -p -h 192.168.2.112 -P 3307</span><br></pre></td></tr></table></figure><p>抓包 <code>port 3306 and host 192.168.2.111</code> 没有看到 keepalive 的包，然后突然意识到 <code>listen</code> 的 <code>so_keepalive</code> 是 nginx 作为 server 端去探测 client 端的，而不是 proxy 的，搜了下搜到 <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_socket_keepalive">proxy_socket_keepalive 字段</a>，<code>stream.d/test.conf</code> 配置 server 段里加下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy_socket_keepalive on;</span><br></pre></td></tr></table></figure><p>然后发现这个开了后探测时间和间隔是按照的内核参数，调整了下内核参数，后续记得自行持久化到 <code>/etc/sysctl.d/xxx.conf</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -a |&amp; grep tcp.keepalive_</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 6</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 5</span><br><span class="line">net.ipv4.tcp_keepalive_time = 60</span><br></pre></td></tr></table></figure><p>mysql 客户端连接上后抓包看到：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">18:53:35.347954 IP 192.168.2.112.34492 &gt; 192.168.2.111.3306: Flags [.], ack 2504, win 501, options [nop,nop,TS val 3972839466 ecr 4160374696], length 0</span><br><span class="line">18:53:35.348410 IP 192.168.2.111.3306 &gt; 192.168.2.112.34492: Flags [.], ack 618, win 243, options [nop,nop,TS val 4160435115 ecr 3972779047], length 0</span><br><span class="line">18:54:36.787902 IP 192.168.2.112.34492 &gt; 192.168.2.111.3306: Flags [.], ack 2504, win 501, options [nop,nop,TS val 3972900906 ecr 4160435115], length 0</span><br><span class="line">18:54:36.788318 IP 192.168.2.111.3306 &gt; 192.168.2.112.34492: Flags [.], ack 618, win 243, options [nop,nop,TS val 4160496556 ecr 3972779047], length 0</span><br><span class="line">18:55:38.227896 IP 192.168.2.112.34492 &gt; 192.168.2.111.3306: Flags [.], ack 2504, win 501, options [nop,nop,TS val 3972962346 ecr 4160496556], length 0</span><br><span class="line">18:55:38.228185 IP 192.168.2.111.3306 &gt; 192.168.2.112.34492: Flags [.], ack 618, win 243, options [nop,nop,TS val 4160557996 ecr 3972779047], length 0</span><br><span class="line">18:56:39.667973 IP 192.168.2.112.34492 &gt; 192.168.2.111.3306: Flags [.], ack 2504, win 501, options [nop,nop,TS val 3973023786 ecr 4160557996], length 0</span><br><span class="line">18:56:39.668303 IP 192.168.2.111.3306 &gt; 192.168.2.112.34492: Flags [.], ack 618, win 243, options [nop,nop,TS val 4160619436 ecr 3972779047], length 0</span><br></pre></td></tr></table></figure><p>因为客户端 mysql 连接上后没执行任何 sql，然后 nginx 每隔 net.ipv4.tcp_keepalive_time 的 60s 发送保活报文，mysql server 端也回复了（就不进行5次间隔6s的后续探活了），所以结果就如上图抓包所示，60s 发一次保活的报文。</p><p>后面让客户调整了下业务机器上的这三个内核参数解决了该问题。</p><h3 id="wireshark-抓包"><a href="#wireshark-抓包" class="headerlink" title="wireshark 抓包"></a>wireshark 抓包</h3><p>实际如果是在 mysql client 建立连接后去抓包导入 wireshark ，心跳包会被识别成 <code>TCP Dup ACK</code>，只有抓完整的报文 wireshark 才会识别为 <code>TCP Keep-Alive ACK</code>。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>应用层在套接字开启 <code>SO_KEEPALIVE</code> 才可以使用 keepalive 能力。</li><li>基本只有 c 语言才有函数能不走内核参数，来自定义自己的 keepalive 三个值。也就是说大多数应用层开启 keepalive 后，还需要调整运行的机器的这三个内核参数。</li><li>在 IM 开发经验里，客户端去使用 keepalive 才是最正确的。</li><li>redis server 有配置开启 keepalive</li><li>kafka 官方默认开启了 keepalive，见 <a href="https://issues.apache.org/jira/browse/KAFKA-2096">Enable keepalive socket option for broker</a> 和 <a href="https://github.com/apache/kafka/blob/2.1.1/core/src/main/scala/kafka/network/SocketServer.scala#L465">官方源码里的 socketChannel.socket().setKeepAlive(true) </a></li></ol><p>其实本次 nginx 做代理，在思想上挺像 sidecar 的理念的。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.jianshu.com/p/e3791f975d7b">简书: TCP keepalive 详解</a></li><li><a href="http://www.52im.net/thread-3506-1-1.html">即时通讯网: 彻底搞懂TCP协议层的KeepAlive保活机制</a></li><li><a href="https://zhuanlan.zhihu.com/p/69337371">知乎: golang 程序开启 tcp keepalive</a></li><li><a href="http://nginx.org/en/docs/http/ngx_http_core_module.html#listen">nginx doc: listen 字段</a></li><li><a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_socket_keepalive">nginx doc: proxy_socket_keepalive 字段</a></li><li><a href="https://plantegg.github.io/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--wireshark-dup-ack-issue/">博客: TCP-wireshark-dup-ack-issue</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;客户的环境下，业务运行在 dmz 区，mysql 在非 dmz 区，业务连 mysql 的空闲 tcp 连接 240s 后会被 SDN 干掉，本文实际介绍一种不动业务(代码)，利用代理的解决办法&lt;/p&gt;</summary>
    
    
    
    
    <category term="linux" scheme="http://zhangguanzhang.github.io/tags/linux/"/>
    
    <category term="tcp" scheme="http://zhangguanzhang.github.io/tags/tcp/"/>
    
    <category term="keepalive" scheme="http://zhangguanzhang.github.io/tags/keepalive/"/>
    
  </entry>
  
</feed>
